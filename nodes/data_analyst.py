"""
Data Analyst Agent for AKC Framework 3.0

This agent handles all data query requests using:
1. Entity Resolver - to identify database IDs from natural language
2. SQL Template Tools - to execute pre-defined SQL queries
3. Pandas Processor - to process and format results
"""
from typing import Dict, Any
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from config.llm import llm
from schemas.state import AgentState
from tools.entity_resolver import resolve_entity
from tools.campaign_template_tool import (
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template
)
from tools.performance_tools import query_performance_metrics
from tools.data_processing_tool import pandas_processor
import json

# Available tools for Data Analyst
TOOLS = [
    resolve_entity,
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template,
    query_performance_metrics,
    pandas_processor
]

# Bind tools to LLM
llm_with_tools = llm.bind_tools(TOOLS)

ANALYST_SYSTEM_PROMPT = """ä½ æ˜¯ AKC æ™ºèƒ½åŠ©æ‰‹çš„æ•¸æ“šåˆ†æå¸« (Data Analyst)ã€‚

**ç•¶å‰æ—¥æœŸ:** {current_date}

**ä½ çš„æ ¸å¿ƒåŸå‰‡:**
- **çµ•å°ä¸è¦è‡ªå·±å¯« SQL**ã€‚æ‰€æœ‰ SQL é‚è¼¯å·²ç¶“åœ¨ Python å·¥å…·ä¸­å®šç¾©å¥½ã€‚
- ä½ çš„å·¥ä½œæ˜¯ã€Œå¡«ç©ºã€è€Œéã€Œå¯«ç¨‹å¼ã€ã€‚

**å·¥ä½œæµç¨‹:**

1. **å¯¦é«”è§£æ (Entity Resolution) - ä¸‰éšæ®µæµç¨‹**
   - **æª¢æŸ¥ Context**: å…ˆæŸ¥çœ‹ System Prompt ä¸­æ˜¯å¦å·²æœ‰ã€Œå·²ç¢ºèªçš„å¯¦é«”ã€ã€‚è‹¥æœ‰ä¸”ç¬¦åˆç•¶å‰éœ€æ±‚ï¼Œ**è«‹ç›´æ¥ä½¿ç”¨è©² IDï¼Œä¸è¦å†æ¬¡å‘¼å« resolve_entity**ã€‚
   - ä½¿ç”¨ `resolve_entity(keyword="...")` é€²è¡Œå¯¦é«”æŸ¥è©¢
   - **åƒæ•¸è¨­å®šåŸå‰‡**:
     - **é è¨­æƒ…æ³**: **ä¸è¦** è¨­å®š `target_types` (ä¿æŒé è¨­å€¼ None)ã€‚é€™æœƒåŒæ™‚æœå°‹ Campaign, Client, Agency, Brand ç­‰æ‰€æœ‰è¡¨æ ¼ã€‚
     - **ä¾‹å¤–æƒ…æ³**: åªæœ‰ç•¶ä½¿ç”¨è€…æ˜ç¢ºæŒ‡å®šé¡å‹æ™‚ï¼ˆä¾‹å¦‚ï¼šã€ŒæŸ¥è©¢**ä»£ç†å•†**äºæ€åšã€ã€ã€Œ**å®¢æˆ¶**æ‚ éŠå¡ã€ï¼‰ï¼Œæ‰è¨­å®š `target_types=["agency"]` æˆ– `target_types=["client"]`ã€‚
     - **åŸå› **: è‹¥ä½¿ç”¨è€…åªèªªã€Œæ‚ éŠå¡ã€ï¼Œå®ƒå¯èƒ½æ˜¯å®¢æˆ¶åä¹Ÿå¯èƒ½æ˜¯æ´»å‹•åã€‚å¿…é ˆæœå°‹å…¨éƒ¨ï¼Œè‹¥ç¸½çµæœåªæœ‰ 1 ç­†æ‰æœƒè‡ªå‹•åŒ¹é…ã€‚
   - å·¥å…·æœƒè‡ªå‹•åŸ·è¡Œï¼šLIKE æŸ¥è©¢ â†’ ä½¿ç”¨è€…ç¢ºèª â†’ RAG å‘é‡æœå°‹

   **è™•ç†ä¸åŒçš„è¿”å›ç‹€æ…‹:**

   a) `status: "exact_match"` - æ‰¾åˆ°å”¯ä¸€åŒ¹é…
      - ç›´æ¥ä½¿ç”¨è¿”å›çš„å¯¦é«” ID ç¹¼çºŒæŸ¥è©¢
      - ä¾‹å¦‚: `{{"status": "exact_match", "data": {{"id": 123, "name": "æ‚ éŠå¡", "type": "client"}}}}`

   b) `status: "needs_confirmation"` - æ‰¾åˆ°å¤šå€‹åŒ¹é…
      - å‘ä½¿ç”¨è€…å±•ç¤ºé¸é …æ¸…å–®
      - ä½¿ç”¨ Markdown æ ¼å¼åŒ–é¸é …ï¼ˆç·¨è™Ÿã€åç¨±ã€é¡å‹ï¼‰
      - è¦æ±‚ä½¿ç”¨è€…å›è¦†ç·¨è™Ÿæˆ–åç¨±
      - **åœæ­¢ç•¶å‰åˆ†æï¼Œç­‰å¾…ä½¿ç”¨è€…é¸æ“‡**
      - ç•¶ä½¿ç”¨è€…å›è¦†å¾Œï¼Œä½¿ç”¨ `resolve_entity(keyword="...", selected_id=X, selected_type="...")` ç¢ºèªé¸æ“‡

   c) `status: "rag_results"` - LIKE æŸ¥è©¢ç„¡çµæœï¼Œä½† RAG æ‰¾åˆ°ç›¸ä¼¼å¯¦é«”
      - å‘ä½¿ç”¨è€…å±•ç¤º RAG å»ºè­°çš„å¯¦é«”
      - è©¢å•æ˜¯å¦ä½¿ç”¨é€™äº›å»ºè­°

   d) `status: "not_found"` - å®Œå…¨æ‰¾ä¸åˆ°
      - å‘ŠçŸ¥ä½¿ç”¨è€…ç„¡æ³•æ‰¾åˆ°è©²å¯¦é«”
      - å»ºè­°ä½¿ç”¨è€…æª¢æŸ¥æ‹¼å¯«æˆ–æä¾›æ›´å¤šè³‡è¨Š

   **è™•ç†ä½¿ç”¨è€…å›è¦†é¸æ“‡ (é‡è¦å„ªåŒ–)**:
   - è‹¥ä¸Šä¸€æ­¥æ˜¯ä½ è©¢å•ä½¿ç”¨è€…ã€Œè«‹é¸æ“‡...ã€ï¼Œè€Œä½¿ç”¨è€…å›è¦†äº†é¸æ“‡ï¼š
   - **è¦å‰‡ 1: å®Œå…¨åç¨±åŒ¹é… (Exact Name Match)**
     - è‹¥ä½¿ç”¨è€…è¼¸å…¥çš„åç¨±èˆ‡é¸é …ä¸­çš„æŸå€‹åç¨±**å®Œå…¨ç›¸åŒ**ï¼ˆä¾‹å¦‚é¸é …æœ‰ "(æš«åœä½¿ç”¨) A" å’Œ "A"ï¼Œä½¿ç”¨è€…å›è¦† "A"ï¼‰ï¼Œ**è«‹ç›´æ¥è¦–ç‚ºé¸æ“‡äº† "A"**ã€‚
     - **ç‰¹æ®Šç‹€æ³**: è‹¥æœ‰å¤šå€‹é¸é …åç¨±ä¸€æ¨¡ä¸€æ¨£ (ä¾‹å¦‚å…©å€‹éƒ½å« "å°ç£å¦®ç¶­é›…è‚¡ä»½æœ‰é™å…¬å¸")ï¼š
       - **ä¸è¦å†æ¬¡è©¢å•**ï¼é€™æœƒè®“ä½¿ç”¨è€…å›°æƒ‘ã€‚
       - **è‡ªå‹•é¸æ“‡é‚è¼¯**: å„ªå…ˆé¸æ“‡çœ‹èµ·ä¾†æ˜¯ã€Œå•Ÿç”¨ä¸­ã€çš„é‚£å€‹ (ä¾‹å¦‚æ’é™¤æœ‰ "(æš«åœä½¿ç”¨)" æ¨™è¨˜çš„)ã€‚è‹¥ç„¡æ³•åˆ¤æ–·ï¼Œé¸æ“‡ ID è¼ƒå¤§çš„é‚£å€‹ (é€šå¸¸æ˜¯è¼ƒæ–°çš„è³‡æ–™)ã€‚
       - ç›´æ¥ä½¿ç”¨è©² ID å‘¼å« `resolve_entity(..., selected_id=...)`ã€‚

   - **è¦å‰‡ 2: ç·¨è™Ÿé¸æ“‡**
     - è‹¥ä½¿ç”¨è€…å›è¦†æ•¸å­— (å¦‚ "1")ï¼Œå°æ‡‰é¸é …æ¸…å–®çš„ç·¨è™Ÿã€‚

   - **è¦å‰‡ 3: éƒ¨åˆ†åŒ¹é…**
     - è‹¥ä½¿ç”¨è€…å›è¦†éƒ¨åˆ†åç¨±ï¼Œå˜—è©¦æ‰¾åˆ°æœ€æ¥è¿‘çš„åŒ¹é…é …ã€‚

   - **ç¦æ­¢äº‹é …**: åš´ç¦åœ¨ä½¿ç”¨è€…å·²ç¶“æ˜ç¢ºå›è¦†åç¨±ï¼ˆä¸”è©²åç¨±åœ¨é¸é …ä¸­å­˜åœ¨ï¼‰çš„æƒ…æ³ä¸‹ï¼Œå†æ¬¡è·³å‡ºä¸€æ¨£çš„é¸é …è¦æ±‚ç¢ºèªã€‚

2. **é¸æ“‡æ­£ç¢ºçš„ SQL å·¥å…· (æ–°ç‰ˆæ¨¡çµ„åŒ– Templates)**

   **åŸºç¤æŸ¥è©¢å·¥å…·**:
   - `query_campaign_basic`: æŸ¥è©¢æ´»å‹•åŸºæœ¬è³‡è¨Šï¼ˆå®¢æˆ¶ã€æ´»å‹•åç¨±ã€æ—¥æœŸã€é ç®—ï¼‰
     - é©ç”¨ï¼šå–å¾— campaign IDsã€åŸºæœ¬æ¦‚è¦½
     - åƒæ•¸ï¼šclient_names, campaign_ids, start_date, end_date

   **é ç®—ç›¸é—œå·¥å…·**:
   - `query_investment_budget`: æŸ¥è©¢ã€Œé€²å–®/æŠ•è³‡ã€é‡‘é¡ï¼ˆæ ¼å¼å±¤ç´šæ˜ç´°ï¼‰
     - é©ç”¨ï¼šã€Œé ç®—ã€ã€ã€Œé€²å–®ã€ã€ã€ŒæŠ•è³‡é‡‘é¡ã€ç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names, campaign_ids, start_date, end_date

   - `query_execution_budget`: æŸ¥è©¢ã€ŒåŸ·è¡Œ/èªåˆ—ã€é‡‘é¡ï¼ˆåŸ·è¡Œå–®å±¤ç´šæ˜ç´°ï¼‰
     - é©ç”¨ï¼šã€ŒåŸ·è¡Œã€ã€ã€Œèªåˆ—ã€ã€ã€Œå¯¦éš›èŠ±è²»ã€ç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names, campaign_ids, start_date, end_date

   - `query_budget_details`: æŸ¥è©¢é ç®—æ‘˜è¦ï¼ˆæ•´åˆæŠ•è³‡èˆ‡åŸ·è¡Œé‡‘é¡ï¼‰
     - é©ç”¨ï¼šã€Œé ç®—ç¼ºå£ã€ã€ã€Œé ç®—å°æ¯”ã€åˆ†æ
     - âš ï¸ å¿…é ˆæä¾› campaign_idsï¼ˆéœ€å…ˆå‘¼å« query_campaign_basicï¼‰

   **æ ¼å¼èˆ‡å—çœ¾å·¥å…·**:
   - `query_ad_formats`: æŸ¥è©¢å»£å‘Šæ ¼å¼æ˜ç´°
     - é©ç”¨ï¼šã€Œæ ¼å¼ã€ã€ã€Œå»£å‘Šå½¢å¼ã€ã€ã€Œç§’æ•¸ã€ã€ã€Œå¹³å°ã€ç›¸é—œå•é¡Œ
     - âš ï¸ å¿…é ˆæä¾› campaign_ids

   - `query_targeting_segments`: æŸ¥è©¢æ•¸æ“šé–å®š/å—çœ¾æ¨™ç±¤
     - é©ç”¨ï¼šã€Œæ•¸æ“šé–å®šã€ã€ã€Œå—çœ¾ã€ã€ã€ŒTAã€ã€ã€Œæ¨™ç±¤ã€ç›¸é—œå•é¡Œ
     - âš ï¸ å¿…é ˆæä¾› campaign_ids

   **æˆæ•ˆæ•¸æ“šå·¥å…·**:
   - `query_performance_metrics`: æŸ¥è©¢ ClickHouse æˆæ•ˆæ•¸æ“šï¼ˆCTR, VTR, ER, Impressions, Clicksï¼‰
     - é©ç”¨ï¼šæ‰€æœ‰æˆæ•ˆç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names æˆ– cmp_ids, dimension ('format' or 'campaign')

   **é€²éšå·¥å…·**:
   - `execute_sql_template`: é€šç”¨æ¨¡æ¿åŸ·è¡Œå™¨
     - é©ç”¨ï¼šmedia_placements.sql, product_lines.sql, contract_kpis.sql, execution_status.sql
     - åªåœ¨ä¸Šè¿°å°ˆç”¨å·¥å…·ä¸é©ç”¨æ™‚æ‰ä½¿ç”¨

3. **åˆ¤æ–·æ—¥æœŸç¯„åœ (é‡è¦)**
   - **é è¨­ç¯„åœ**:
     - **Start Date**: {current_year}-01-01
     - **End Date**: {current_year}-12-31
   - **ä¾‹å¤–æƒ…æ³**:
     - åª’é«”æ’æœŸå¸¸æœƒé æ’è‡³æ˜å¹´ã€‚è‹¥åœ¨ç•¶å¹´æŸ¥ç„¡ç‰¹å®šæ´»å‹•ï¼Œ**è«‹è‡ªå‹•å°‡ End Date å»¶é•·è‡³æ˜å¹´åº•** (ä¾‹å¦‚ {current_year}+1-12-31)ã€‚
     - è‹¥ä½¿ç”¨è€…æ˜ç¢ºæŒ‡å®šå¹´ä»½ï¼Œå‰‡ä»¥ä½¿ç”¨è€…æŒ‡å®šç‚ºæº–ã€‚

4. **è³‡æ–™è™•ç† (CRITICAL!)**
   - SQL å·¥å…·å›å‚³åŸå§‹æ•¸æ“šï¼Œå¯èƒ½åŒ…å« NULL æˆ–é‡è¤‡çš„ entity_name
   - **ä½ å¿…é ˆ ALWAYS ä½¿ç”¨ `pandas_processor` è™•ç†æ•¸æ“šï¼**
   - **é‡è¦**ï¼šèª¿ç”¨ pandas_processor æ™‚ï¼Œ**ä¸è¦å‚³ `data` åƒæ•¸**ï¼Œç³»çµ±æœƒè‡ªå‹•æ³¨å…¥å®Œæ•´æ•¸æ“š

   **âš ï¸ CRITICAL - ç†è§£æ•¸æ“šç‹€æ…‹ï¼**
   - **è²¡å‹™å·¥å…·** (investment_budget, execution_budget) â†’ è¿”å›**åŸå§‹æ˜ç´°æ•¸æ“š**ï¼ˆå¯èƒ½æœ‰å¤šè¡Œï¼‰
   - **æˆæ•ˆå·¥å…·** (query_performance_metrics) â†’ è¿”å›**å·²åŒ¯ç¸½æ•¸æ“š**ï¼ˆå·²æŒ‰ dimension åˆ†çµ„ï¼‰

   **è™•ç†è²¡å‹™æ•¸æ“šï¼ˆåŸå§‹æ˜ç´°ï¼‰**ï¼š
   - ä½¿ç”¨ `operation="groupby_sum"` åˆ†çµ„åŠ ç¸½
   - **åƒæ•¸è¦å‰‡**ï¼š
     - `groupby_col`: åˆ†çµ„æ¬„ä½ï¼ˆå¦‚ "format_name"ï¼‰
     - `sum_col`: **æ”¯æ´å¤šæ¬„ä½**ï¼ˆé€—è™Ÿåˆ†éš”å­—ä¸²ï¼Œå¦‚ "amount,budget,clicks"ï¼‰
   - **ç¤ºä¾‹**ï¼š
     ```python
     pandas_processor(
         operation="groupby_sum",
         groupby_col="format_name",
         sum_col="investment_amount,investment_gift",
         ascending=False
     )
     ```

   **åˆä½µæ•¸æ“šç­–ç•¥**:
   - ä½¿ç”¨è€…å¸Œæœ›çœ‹åˆ°**ä¸€å¼µæ•´åˆçš„å¤§è¡¨**ï¼Œè«‹ç›¡å¯èƒ½å°‡æ‰€æœ‰æ•¸æ“šåˆä½µã€‚
   - **æ¨™æº–åˆä½µæµç¨‹**:
     1. å…ˆå‘¼å« `query_campaign_basic` å–å¾— campaign_ids
     2. ä½¿ç”¨é€™äº› IDs å‘¼å«å…¶ä»–å·¥å…·ï¼ˆad_formats, targeting_segments, budget_details ç­‰ï¼‰
     3. ä½¿ç”¨ `pandas_processor(operation="merge", merge_on="campaign_id")` åˆä½µçµæœ

5. **æœ€çµ‚å›æ‡‰**
   - ä»¥ Markdown æ ¼å¼å‘ˆç¾åˆ†æçµæœ
   - è‹¥æˆåŠŸåˆä½µï¼Œå±•ç¤ºä¸€å¼µå¤§è¡¨ã€‚
   - è‹¥åˆ†é–‹å±•ç¤ºï¼Œè«‹èªªæ˜åŸå› ã€‚

**ç•¶å‰æƒ…å¢ƒ:**
- ä½¿ç”¨è€…æŸ¥è©¢: {original_query}
- é—œéµå­—æç¤º: å¯¦é«”={entity_keywords}, æ™‚é–“={time_keywords}
- åˆ†ææç¤º: {analysis_hint}

ç¾åœ¨é–‹å§‹å·¥ä½œå§!
"""


def data_analyst_node(state: AgentState) -> Dict[str, Any]:
    """
    Data Analyst Agent: Resolves entities, queries data, processes results.

    Workflow:
    1. Extract routing context from Intent Router
    2. Use Entity Resolver to find IDs
    3. Call appropriate SQL Template Tool
    4. Process data with Pandas
    5. Return formatted response

    Args:
        state: Current agent state

    Returns:
        Updated state with analyst results
    """
    from datetime import datetime

    # Get current date
    now = datetime.now()
    current_date = now.strftime("%Y-%m-%d")
    current_year = now.year

    # Extract context from state
    routing_context = state.get("routing_context", {})
    original_query = routing_context.get("original_query", "")
    entity_keywords = routing_context.get("entity_keywords", [])
    time_keywords = routing_context.get("time_keywords", [])
    analysis_hint = routing_context.get("analysis_hint")

    # Load previously resolved entities from state
    resolved_entities_state = state.get("resolved_entities", [])
    if resolved_entities_state is None:
        resolved_entities_state = []

    # Format resolved entities for context
    resolved_context_str = ""
    if resolved_entities_state:
        resolved_names = [f"{e.get('name')} (ID: {e.get('id')})" for e in resolved_entities_state]
        resolved_context_str = f"\n**å·²ç¢ºèªçš„å¯¦é«” (ç„¡éœ€å†æ¬¡è©¢å•):** {', '.join(resolved_names)}"
        print(f"DEBUG [DataAnalyst] Loaded resolved entities: {resolved_names}")

    # Handle multimodal content format (Gemini API may return list)
    if isinstance(original_query, list):
        text_parts = []
        for part in original_query:
            if isinstance(part, dict) and part.get("type") == "text":
                text_parts.append(part.get("text", ""))
            elif isinstance(part, str):
                text_parts.append(part)
        original_query = " ".join(text_parts).strip()
        print(f"DEBUG [DataAnalyst] Converted multimodal query to string")

    # Ensure original_query is string
    if not isinstance(original_query, str):
        original_query = str(original_query)

    print(f"DEBUG [DataAnalyst] Starting analysis for: {original_query[:100]}...")
    print(f"DEBUG [DataAnalyst] Context: entities={entity_keywords}, time={time_keywords}, hint={analysis_hint}")
    print(f"DEBUG [DataAnalyst] Current date: {current_date}, Year: {current_year}")

    # Build conversation with system prompt
    messages = [
        SystemMessage(content=ANALYST_SYSTEM_PROMPT.format(
            current_date=current_date,
            current_year=current_year,
            original_query=original_query,
            entity_keywords=entity_keywords,
            time_keywords=time_keywords,
            analysis_hint=analysis_hint or "æœªæŒ‡å®š"
        ) + resolved_context_str),
        HumanMessage(content=f"è«‹å”åŠ©åˆ†æé€™å€‹æŸ¥è©¢ï¼š{original_query}")
    ]

    # Agent ReAct Loop (max 15 iterations to prevent infinite loops)
    final_data = None
    markdown_response = ""
    # Initialize with state values to preserve memory
    resolved_entities = list(resolved_entities_state)
    latest_query_data = None  # Store complete SQL result for pandas_processor

    for iteration in range(15):
        print(f"DEBUG [DataAnalyst] Iteration {iteration + 1}")

        # Invoke LLM with tools
        response = llm_with_tools.invoke(messages)
        messages.append(response)

        # Check if LLM returned final answer (no tool calls)
        if not response.tool_calls:
            markdown_response = response.content
            if isinstance(markdown_response, list):
                markdown_response = " ".join([
                    item.get("text", "") if isinstance(item, dict) else str(item)
                    for item in markdown_response
                ])
            print(f"DEBUG [DataAnalyst] Agent finished with response: {markdown_response[:200]}...")
            break

        # Execute tool calls
        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            args = tool_call["args"]

            print(f"DEBUG [DataAnalyst] Calling tool: {tool_name}")
            print(f"DEBUG [DataAnalyst] Arguments: {args}")

            # Map tool name to function
            tool_map = {
                "resolve_entity": resolve_entity,
                "query_campaign_basic": query_campaign_basic,
                "query_budget_details": query_budget_details,
                "query_investment_budget": query_investment_budget,
                "query_execution_budget": query_execution_budget,
                "query_targeting_segments": query_targeting_segments,
                "query_ad_formats": query_ad_formats,
                "execute_sql_template": execute_sql_template,
                "query_performance_metrics": query_performance_metrics,
                "pandas_processor": pandas_processor
            }

            tool_func = tool_map.get(tool_name)
            if not tool_func:
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=f"Error: Tool '{tool_name}' not found."
                ))
                continue

            # Pre-process: Inject data for pandas_processor BEFORE execution
            try:
                if tool_name == "pandas_processor" and latest_query_data and not args.get("data"):
                    # Convert Decimal before injecting
                    from decimal import Decimal
                    def _safe_convert(obj):
                        if isinstance(obj, dict):
                            return {k: _safe_convert(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [_safe_convert(item) for item in obj]
                        elif isinstance(obj, Decimal):
                            return float(obj)
                        else:
                            return obj

                    args["data"] = _safe_convert(latest_query_data)
                    print(f"DEBUG [DataAnalyst] Injected {len(latest_query_data)} complete records into pandas_processor")

                # Execute tool
                result = tool_func.invoke(args)

                # Post-process: Handle results based on tool type
                if tool_name == "resolve_entity":
                    # Store entity resolution results
                    if isinstance(result, dict):
                        status = result.get("status")

                        if status == "exact_match":
                            # å„²å­˜å·²ç¢ºèªçš„å¯¦é«”
                            resolved_entities.append(result.get("data"))
                            llm_result = result

                        elif status == "needs_confirmation":
                            # æ ¼å¼åŒ–å¤šé¸é …å±•ç¤º
                            candidates = result.get("data", [])
                            formatted_options = []
                            for idx, candidate in enumerate(candidates, 1):
                                formatted_options.append(
                                    f"{idx}. {candidate['name']} ({candidate['type']}) - ä¾†è‡ª {candidate['table']}.{candidate['column']}"
                                )

                            llm_result = {
                                "status": "needs_confirmation",
                                "message": result.get("message"),
                                "instruction": "âš ï¸ æ‰¾åˆ°å¤šå€‹åŒ¹é…é …ï¼Œè«‹å‘ä½¿ç”¨è€…å±•ç¤ºä»¥ä¸‹é¸é …ä¸¦è¦æ±‚å…¶é¸æ“‡ï¼š",
                                "options": formatted_options,
                                "candidates_data": candidates,  # ä¿ç•™å®Œæ•´æ•¸æ“šä¾›å¾ŒçºŒä½¿ç”¨
                                "note": "ç•¶ä½¿ç”¨è€…å›è¦†å¾Œï¼Œä½¿ç”¨ resolve_entity(keyword='...', selected_id=X, selected_type='Y') ç¢ºèªé¸æ“‡"
                            }

                        elif status == "rag_results":
                            # æ ¼å¼åŒ– RAG çµæœå±•ç¤º
                            rag_data = result.get("data", [])
                            formatted_rag = []
                            for idx, item in enumerate(rag_data, 1):
                                formatted_rag.append(
                                    f"{idx}. {item.get('value')} (ç›¸ä¼¼åº¦: {item.get('score', 0):.2f}) - ä¾†æº: {item.get('table')}.{item.get('source')}"
                                )

                            llm_result = {
                                "status": "rag_results",
                                "message": result.get("message"),
                                "instruction": "ğŸ” LIKE æŸ¥è©¢ç„¡çµæœï¼Œä½† RAG æ‰¾åˆ°ä»¥ä¸‹ç›¸ä¼¼å¯¦é«”ï¼š",
                                "rag_suggestions": formatted_rag,
                                "note": "è«‹å‘ä½¿ç”¨è€…ç¢ºèªæ˜¯å¦ä½¿ç”¨é€™äº›å»ºè­°ï¼Œæˆ–è¦æ±‚å…¶æä¾›æ›´æº–ç¢ºçš„åç¨±"
                            }

                        else:
                            # not_found æˆ–å…¶ä»–ç‹€æ…‹
                            llm_result = result
                    else:
                        llm_result = result

                elif ("query_" in tool_name and tool_name != "query_performance_metrics") or tool_name == "execute_sql_template":
                    # Store SQL query results (from MySQL templates)
                    if isinstance(result, dict):
                        final_data = result
                        latest_query_data = result.get("data", [])  # Store complete data

                        # Convert Decimal to float for JSON serialization
                        from decimal import Decimal
                        def _convert_decimals(obj):
                            if isinstance(obj, dict):
                                return {k: _convert_decimals(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [_convert_decimals(item) for item in obj]
                            elif isinstance(obj, Decimal):
                                return float(obj)
                            else:
                                return obj

                        sample_preview = _convert_decimals(result.get("data", [])[:5])

                        # Return simplified version to LLM with metadata
                        llm_result = {
                            "status": result.get("status"),
                            "count": result.get("count"),
                            "message": f"âœ… æŸ¥è©¢æˆåŠŸï¼å…± {result.get('count')} ç­†æ•¸æ“šå·²æº–å‚™å¥½ã€‚",
                            "instruction": "å®Œæ•´æ•¸æ“š ({0} ç­†) å·²è¼‰å…¥ï¼Œè«‹ä½¿ç”¨ pandas_processor è™•ç†".format(result.get("count")),
                            "columns": list(result.get("data", [{}])[0].keys()) if result.get("data") else [],
                            "sample_preview": sample_preview,
                            "generated_sql": result.get("generated_sql", "")  # ğŸ” é¡¯ç¤ºåŸ·è¡Œçš„ SQL
                        }
                    else:
                        llm_result = result

                elif tool_name == "query_performance_metrics":
                    # Store performance query results (from ClickHouse)
                    if isinstance(result, dict):
                        final_data = result
                        latest_query_data = result.get("data", [])

                        from decimal import Decimal
                        def _convert_decimals(obj):
                            if isinstance(obj, dict):
                                return {k: _convert_decimals(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [_convert_decimals(item) for item in obj]
                            elif isinstance(obj, Decimal):
                                return float(obj)
                            else:
                                return obj

                        sample_preview = _convert_decimals(result.get("data", [])[:5])

                        llm_result = {
                            "status": result.get("status"),
                            "count": result.get("count"),
                            "message": f"âœ… æˆæ•ˆæŸ¥è©¢æˆåŠŸï¼å…± {result.get('count')} ç­†æ•¸æ“šå·²æº–å‚™å¥½ã€‚",
                            "instruction": "å®Œæ•´æ•¸æ“š ({0} ç­†) å·²è¼‰å…¥ï¼Œè«‹ä½¿ç”¨ pandas_processor è™•ç†".format(result.get("count")),
                            "columns": list(result.get("data", [{}])[0].keys()) if result.get("data") else [],
                            "sample_preview": sample_preview
                        }
                    else:
                        llm_result = result

                elif tool_name == "pandas_processor":
                    # Store processed data
                    if isinstance(result, dict) and result.get("status") == "success":
                        final_data = result
                    llm_result = result

                else:
                    llm_result = result

                # Convert result to JSON-safe format (handle Decimal, datetime, etc.)
                def convert_to_json_safe(obj):
                    """Convert non-JSON-serializable objects to JSON-safe types"""
                    from decimal import Decimal
                    from datetime import datetime, date
                    import math

                    if isinstance(obj, float):
                        if math.isnan(obj) or math.isinf(obj):
                            return None
                        return obj
                    elif isinstance(obj, dict):
                        return {k: convert_to_json_safe(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_to_json_safe(item) for item in obj]
                    elif isinstance(obj, Decimal):
                        return float(obj)
                    elif isinstance(obj, (datetime, date)):
                        return obj.isoformat()
                    else:
                        return obj

                llm_result = convert_to_json_safe(llm_result)

                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=json.dumps(llm_result, ensure_ascii=False, indent=2)
                ))

            except Exception as e:
                error_msg = f"Tool execution error: {str(e)}"
                print(f"ERROR [DataAnalyst] {error_msg}")
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=error_msg
                ))

    # Return updated state
    return {
        "analyst_data": final_data,
        "resolved_entities": resolved_entities,
        "final_response": markdown_response,
        "messages": [AIMessage(content=markdown_response)],
        "next": "END"
    }
