"""
AKC Framework 3.0 - Data Analyst Agent (V2)
Implemented using langchain.agents.create_agent
"""
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional

from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import wrap_tool_call, dynamic_prompt, ModelRequest
from langchain.messages import SystemMessage, ToolMessage, AIMessage, HumanMessage
from langchain_core.messages import BaseMessage

from config.llm import llm
from agent.state import AgentState as ProjectAgentState
from tools.entity_resolver import resolve_entity
from tools.campaign_template_tool import (
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template,
    query_industry_format_budget,
    query_media_placements
)
from tools.performance_tools import (
    query_format_benchmark,
    query_unified_performance,
    query_unified_dimensions
)

# Setup logging
logger = logging.getLogger("akc.analyst")
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# Tools for Retrieval
RETRIEVER_TOOLS = [
    resolve_entity,
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template,
    query_industry_format_budget,
    query_media_placements,
    query_format_benchmark,
    query_unified_performance,
    query_unified_dimensions
]

RETRIEVER_SYSTEM_PROMPT = """ä½ æ˜¯ AKC æ™ºèƒ½åŠ©æ‰‹çš„æ•¸æ“šæª¢ç´¢å°ˆå®¶ (Data Retriever)ã€‚

**ç•¶å‰æ—¥æœŸ**: {current_date}

**ç³»çµ±æŒ‡å®šæŸ¥è©¢ç¯„åœ (Strict Constraints)**:
- **é–‹å§‹æ—¥æœŸ**: {start_date}
- **çµæŸæ—¥æœŸ**: {end_date}
- **å¼·åˆ¶åŸ·è¡Œ**: å³ä½¿ä½¿ç”¨è€…çš„å•é¡Œä¸­çœ‹èµ·ä¾†æœ‰æ—¥æœŸ (ä¾‹å¦‚ "2023å¹´")ï¼Œè‹¥ä¸Šæ–¹æŒ‡å®šäº†æ—¥æœŸç¯„åœï¼Œ**è«‹å‹™å¿…ä½¿ç”¨ç³»çµ±æŒ‡å®šæ—¥æœŸ**ã€‚

**æ ¸å¿ƒä»»å‹™**: æ ¹æ“šä½¿ç”¨è€…éœ€æ±‚ï¼Œé¸æ“‡æ­£ç¢ºçš„è³‡æ–™æº (MySQL æˆ– ClickHouse) ç²å–æ•¸æ“šã€‚

---

### ðŸš¦ é›™è»Œåˆ†æµç­–ç•¥ (Dual-Track Strategy)

**åŽŸå‰‡ï¼šæˆæ•ˆèˆ‡æŽ¢ç´¢èµ° ClickHouse (å¿«)ï¼ŒéŒ¢èˆ‡è¨­å®šèµ° MySQL (æº–)ã€‚**

#### 1. æˆæ•ˆèˆ‡ç¶­åº¦æŽ¢ç´¢ (Performance & Discovery) â†’ ðŸš€ ä½¿ç”¨ ClickHouse
ç•¶æŸ¥è©¢æ¶‰åŠï¼š**é»žæ“Šã€æ›å…‰ã€CTRã€ç”¢å“ç·š (Product Line)ã€æ ¼å¼æ¸…å–®ã€ç‰ˆä½æ¸…å–®**ã€‚
- **å·¥å…·**:
  - `query_unified_performance`: **(ä¸»è¦å·¥å…·)** æŸ¥æˆæ•ˆ (Impressions, Clicks, CTR)ã€‚
  - `query_unified_dimensions`: æŸ¥æ¸…å–® (æœ‰å“ªäº›ç”¢å“ç·šï¼Ÿæœ‰å“ªäº›æ ¼å¼ï¼Ÿ)ã€‚
- **å„ªå‹¢**: é€Ÿåº¦å¿«ï¼Œæ”¯æ´ç”¢å“ç·šç¶­åº¦ã€‚

#### 2. é‡‘é¡èˆ‡è¨­å®š (Budget & Setup) â†’ ðŸ’° ä½¿ç”¨ MySQL
ç•¶æŸ¥è©¢æ¶‰åŠï¼š**é ç®—ã€é‡‘é¡ (Cost/Investment)ã€å—çœ¾ (Targeting)ã€åˆç´„ç‹€æ…‹**ã€‚
- **å·¥å…·**:
  - `query_budget_details`: æŸ¥æ´»å‹•ç¸½é ç®—ã€‚
  - `query_industry_format_budget`: æŸ¥ç”¢æ¥­/å®¢æˆ¶çš„é ç®—ä½”æ¯” (Share)ã€‚
  - `query_targeting_segments`: æŸ¥å—çœ¾è¨­å®šã€‚
  - `query_investment_budget`: æŸ¥è©³ç´°é€²å–®é‡‘é¡ã€‚
- **é™åˆ¶**: ä¸æ”¯æ´ç”¢å“ç·šç¶­åº¦ã€‚

#### 3. æ··åˆéœ€æ±‚ (Hybrid) â†’ ðŸ”— é›™é‚ŠæŸ¥è©¢ + Pandas åˆä½µ
ç•¶ä½¿ç”¨è€…åŒæ™‚å•ã€Œæˆæ•ˆã€èˆ‡ã€Œé ç®—ã€æ™‚ (ä¾‹å¦‚ï¼šå„ç”¢å“ç·šçš„ CPCï¼Ÿ)ã€‚
- **åŸ·è¡Œæ­¥é©Ÿ**:
  1. å‘¼å« `query_unified_performance` å–å¾—æˆæ•ˆ (å« `plaid` æˆ– `cmpid`)ã€‚
  2. å‘¼å« `query_media_placements` (æˆ–ç›¸é—œå·¥å…·) å–å¾—é ç®— (å« `placement_id` æˆ– `campaign_id`)ã€‚
  3. **(é—œéµ)**: åœæ­¢å·¥å…·å‘¼å«ï¼Œè®“ Reporter ä½¿ç”¨ Pandas å°‡å…©ä»½æ•¸æ“šä¾æ“š ID (`plaid` = `placement_id`) åˆä½µè¨ˆç®—ã€‚

---

### ðŸ› ï¸ å·¥å…·é¸æ“‡æŒ‡å— (SOP)

**æƒ…å¢ƒ A: ã€Œå…¨ç«™ã€æˆ–ã€Œç”¢æ¥­ã€å±¤ç´šåˆ†æž**

1. **å•ã€Œé ç®—ä½”æ¯”ã€æˆ–ã€Œé‡‘é¡æŽ’åã€**:
   - âš¡ï¸ **ç›´æŽ¥ä½¿ç”¨** `query_industry_format_budget(dimension='industry'|'client')`ã€‚
   - ä¸è¦æŸ¥ Campaign Listï¼Œä¹Ÿä¸è¦æŸ¥ ClickHouseã€‚

2. **å•ã€Œæˆæ•ˆ (CTR/VTR)ã€æˆ–ã€Œç”¢å“ç·šè¡¨ç¾ã€**:
   - âš¡ï¸ **ç›´æŽ¥ä½¿ç”¨** `query_unified_performance(group_by=['product_line']...)`ã€‚
   - è‹¥éœ€ç‰¹å®šç”¢æ¥­ï¼Œå‚³å…¥ `one_categories=['Automotive']` (éœ€å…ˆç¢ºèªæ­£ç¢ºåç¨±æˆ– ID)ã€‚

3. **å•ã€Œæœ‰å“ªäº›...ã€ (æŽ¢ç´¢æ¸…å–®)**:
   - âš¡ï¸ **ç›´æŽ¥ä½¿ç”¨** `query_unified_dimensions(dimensions=['product_line'])`ã€‚

**æƒ…å¢ƒ B: ã€Œç‰¹å®šå®¢æˆ¶/å¯¦é«”ã€åˆ†æž (ä¾‹å¦‚: Nike)**

1. **Step 1: å¯¦é«”è§£æž (å¿…é ˆ)**
   - ä½¿ç”¨ `resolve_entity(keyword='Nike')` å–å¾— `client_id`ã€‚

2. **Step 2: å–å¾— Campaign IDs (é—œéµ)**
   - âš ï¸ **ClickHouse å­—å…¸å¯èƒ½æœƒæœ‰å»¶é²æˆ–ç¼ºæ¼ï¼Œè«‹å‹™å¿…å…ˆå¾ž MySQL ç²å–ç²¾ç¢º IDã€‚**
   - **å„ªå…ˆä½¿ç”¨** `query_campaign_basic(client_ids=[id], start_date=..., end_date=...)`ã€‚
   - é€™æœƒå›žå‚³è©²å®¢æˆ¶åœ¨æŒ‡å®šæœŸé–“å…§çš„æ‰€æœ‰ `campaign_id`ã€‚

3. **Step 3: æ ¹æ“šéœ€æ±‚åˆ†æµ**
   - **æŸ¥æˆæ•ˆ/æ ¼å¼/ç”¢å“ç·š**:
     - `query_unified_performance(cmpids=[...], group_by=['ad_format_type', 'product_line'])`ã€‚
     - **æ³¨æ„**: è«‹å°‡ Step 2 æ‹¿åˆ°çš„ `campaign_id` åˆ—è¡¨å‚³å…¥ `cmpids` åƒæ•¸ã€‚é€™æ˜¯æœ€æ¨™æº–çš„åšæ³•ã€‚
   
   - **æŸ¥ç´°éƒ¨ç‰ˆä½ (Deep Dive) æˆ– æ•¸æ“šéŽ–å®šæˆæ•ˆ**:
     - è‹¥ç”¨æˆ¶å•åˆ°ã€Œç‰ˆä½è¡¨ç¾ã€æˆ–ã€Œæ•¸æ“šéŽ–å®šæˆæ•ˆã€(Targeting Performance)ï¼š
       1. å‘¼å« `query_media_placements(campaign_ids=[...])` å–å¾— `plaids` èˆ‡ `placement_id`ã€‚
       2. å‘¼å« `query_unified_performance`ï¼š
          - **å¿…é ˆåŒ…å«** `group_by=['ad_format_type', 'plaid']` (é—œéµï¼šä¿ç•™ plaid ä»¥ä¾¿èˆ‡ Targeting å°æŽ¥)ã€‚
          - å‚³å…¥ `plaids=[...]` é€²è¡ŒéŽæ¿¾ã€‚
       3. å‘¼å« `query_targeting_segments(campaign_ids=[...])`ã€‚

   - **æŸ¥é ç®—/èŠ±è²»**:
     - `query_investment_budget(client_ids=[id])` (çœ‹é€²å–®) æˆ– `query_execution_budget` (çœ‹åŸ·è¡Œ)ã€‚
   - **æŸ¥å—çœ¾/è¨­å®š**:
     - ç›´æŽ¥ `query_targeting_segments(campaign_ids=[...])`ã€‚

**æƒ…å¢ƒ C: æ··åˆè¨ˆç®— (ä¾‹å¦‚: Nike çš„ç”¢å“ç·š CPC)**
   1. `query_campaign_basic` (æ‹¿ cmpids)
   2. `query_unified_performance(cmpids=[...])` (æ‹¿ Clicks)
   3. `query_investment_budget(client_ids=[id])` (æ‹¿ Budget)
   4. **çµæŸå·¥å…·å‘¼å«**ã€‚ (Reporter æœƒè™•ç† `Budget / Clicks`)

---

**âš ï¸ ID ä½¿ç”¨éµå¾‹**:
- ClickHouse å·¥å…·çš„ ID åƒæ•¸ç‚º: `client_ids`, `product_line_ids`, `plaids` (å°æ‡‰ MySQL placement_id), `cmpids` (å°æ‡‰ MySQL campaign_id)ã€‚
- åªè¦ `resolve_entity` æ‹¿åˆ° IDï¼Œå°±å¿…é ˆå„ªå…ˆå‚³å…¥ ID åƒæ•¸ï¼Œä¸è¦å‚³ Nameã€‚

**çµæŸæ¢ä»¶**:
-ç•¶å¿…è¦çš„ã€Œæˆæ•ˆé¢ã€èˆ‡ã€Œé‡‘é¡é¢ã€æ•¸æ“šéƒ½æ‹¿åˆ°å¾Œï¼Œè«‹åœæ­¢ã€‚
"""

@dynamic_prompt
def retriever_dynamic_prompt(request: ModelRequest) -> str:
    """Injects current date, date range, and resolved entities into the system prompt."""
    now = datetime.now()
    current_date = now.strftime("%Y-%m-%d")
    
    # Get date range from routing_context
    routing_context = request.state.get("routing_context", {})
    start_date = routing_context.get("start_date") or "2021-01-01"
    end_date = routing_context.get("end_date") or current_date
    
    base_prompt = RETRIEVER_SYSTEM_PROMPT.format(
        current_date=current_date,
        start_date=start_date,
        end_date=end_date
    )
    
    # In-context learning for resolved entities
    resolved_entities = request.state.get("resolved_entities", [])
    if resolved_entities:
        context_lines = []
        for e in resolved_entities:
            e_type = e.get('type', 'unknown')
            e_id = e.get('id')
            e_name = e.get('name')
            context_lines.append(f"- {e_type.upper()} ID: {e_id} (åç¨±: {e_name})")

        entity_context = "\n\nå·²ç¢ºèªçš„å¯¦é«”è³‡è¨Šï¼š\n" + "\n".join(context_lines)
        return base_prompt + entity_context
    
    return base_prompt

@wrap_tool_call
def retriever_tool_middleware(request: Any, handler):
    """
    Middleware to handle:
    1. Data storage in state['data_store']
    2. Custom guidance for Entity Resolution and Campaign queries
    3. Debug logging
    4. Force Date Override
    """
    tool_call = request.tool_call
    tool_name = tool_call["name"]
    args = tool_call["args"]
    state = request.state
    
    # Initialize state fields if needed
    if "data_store" not in state or state["data_store"] is None:
        state["data_store"] = {}
    if "debug_logs" not in state or state["debug_logs"] is None:
        state["debug_logs"] = []
    if "resolved_entities" not in state or state["resolved_entities"] is None:
        state["resolved_entities"] = []

    # [NEW] Force Date Override
    if state.get("routing_context"):
        logger.info(f"Routing Context: {state.get('routing_context')}")
        system_start = state["routing_context"].get("start_date")
        system_end = state["routing_context"].get("end_date")
        
        if system_start and "start_date" in args:
            if args["start_date"] != system_start:
                logger.warning(f"Force overriding start_date: {args['start_date']} -> {system_start}")
                args["start_date"] = system_start
                
        if system_end and "end_date" in args:
            if args["end_date"] != system_end:
                logger.warning(f"Force overriding end_date: {args['end_date']} -> {system_end}")
                args["end_date"] = system_end

    try:
        # Execute tool
        result = handler(request)
        
        # Extract raw data from result
        raw_result = None
        if isinstance(result, ToolMessage):
            content = result.content
            try:
                raw_result = json.loads(content)
            except:
                try:
                    import ast
                    import re
                    cleaned = re.sub(r"Decimal\('([^']+)'\)", r"\1", content)
                    cleaned = re.sub(r"datetime\.date\((\d+), (\d+), (\d+)\)", r"'\1-\2-\3'", cleaned)
                    cleaned = re.sub(r"datetime\.datetime\((\d+), (\d+), (\d+),? ?(\d+)?,? ?(\d+)?,? ?(\d+)?\)", 
                                     lambda m: "'" + m.group(1) + "-" + m.group(2) + "-" + m.group(3) + "'", cleaned)
                    
                    raw_result = ast.literal_eval(cleaned)
                except Exception as parse_e:
                    logger.debug(f"Failed to parse content for {tool_name}: {parse_e}")
        elif isinstance(result, dict):
            raw_result = result
            
        if raw_result and isinstance(raw_result, dict):
            # 1. Logic to store data (with Deduplication)
            if "data" in raw_result:
                data = raw_result.get("data")
                if data and isinstance(data, list) and len(data) > 0:
                    if tool_name not in state["data_store"]:
                        state["data_store"][tool_name] = []

                    # Deduplicate
                    try:
                        existing_data_str = {json.dumps(row, sort_keys=True, default=str) for row in state["data_store"][tool_name]}
                        new_rows = []
                        for row in data:
                            row_str = json.dumps(row, sort_keys=True, default=str)
                            if row_str not in existing_data_str:
                                new_rows.append(row)
                                existing_data_str.add(row_str)

                        if new_rows:
                            state["data_store"][tool_name].extend(new_rows)
                            logger.info(f"Stored {len(new_rows)} rows in data_store for {tool_name}")
                    except Exception as e:
                        logger.error(f"Deduplication failed: {e}")
                        state["data_store"][tool_name].extend(data)
            
            # 2. Handle Entity Resolution specifically for state update
            if tool_name == "resolve_entity":
                status = raw_result.get("status")
                if status in ["exact_match", "merged_match"]:
                    entity = raw_result.get("data")
                    if isinstance(entity, list):
                        state["resolved_entities"].extend(entity)
                    else:
                        state["resolved_entities"].append(entity)
                logger.info(f"Updated resolved_entities: {len(state['resolved_entities'])}")

            # 3. Add guidance and convert to valid JSON
            def json_default(obj):
                import decimal
                import datetime
                if isinstance(obj, decimal.Decimal):
                    return float(obj)
                if isinstance(obj, (datetime.date, datetime.datetime)):
                    return obj.isoformat()
                return str(obj)

            content = json.dumps(raw_result, ensure_ascii=False, default=json_default)
            
            if tool_name == "query_campaign_basic" and raw_result.get("data"):
                campaign_ids = [row.get('campaign_id') for row in raw_result.get("data", []) if row.get('campaign_id')]
                if campaign_ids:
                    content += f"\n\nâœ… å·²å–å¾— {len(campaign_ids)} å€‹æ´»å‹•çš„åŸºæœ¬è³‡æ–™ã€‚\nðŸ‘‰ ä¸‹ä¸€æ­¥: è«‹æŸ¥è©¢æˆæ•ˆ/é ç®—ç­‰æ•¸æ“šã€‚"
            
            return ToolMessage(tool_call_id=tool_call["id"], content=content)

        return result
    except Exception as e:
        logger.error(f"Tool error: {e}")
        return ToolMessage(tool_call_id=tool_call["id"], content=json.dumps({"error": str(e)}))

# Create the agent
retriever_agent = create_agent(
    model=llm,
    tools=RETRIEVER_TOOLS,
    middleware=[retriever_dynamic_prompt, retriever_tool_middleware],
    state_schema=ProjectAgentState
)

def _check_performance_tools_needed(state: ProjectAgentState, result: Dict[str, Any]) -> Dict[str, bool]:
    original_query = state.get("routing_context", {}).get("original_query", "").lower()
    format_keywords = ["æ ¼å¼", "format", "banner", "å½±éŸ³", "å»£å‘Šå½¢å¼"]
    has_format = any(kw in original_query for kw in format_keywords)
    performance_keywords = ["ctr", "vtr", "er", "é»žæ“ŠçŽ‡", "è§€çœ‹çŽ‡", "äº’å‹•çŽ‡", "æˆæ•ˆ", "æŽ’å", "å¹³å‡"]
    has_performance = any(kw in original_query for kw in performance_keywords)
    client_keywords = ["å®¢æˆ¶", "client", "å»£å‘Šä¸»", "å“ç‰Œ"]
    has_client = any(kw in original_query for kw in client_keywords)
    data_store = result.get("data_store", {})
    has_benchmark = "query_format_benchmark" in data_store
    has_performance_tool = "query_unified_performance" in data_store
    has_campaign_basic = "query_campaign_basic" in data_store
    needs = {
        "needs_benchmark": False,
        "needs_performance": False,
        "needs_campaign_basic": False
    }
    if has_format and has_performance:
        if has_client:
            needs["needs_performance"] = not has_performance_tool
            needs["needs_campaign_basic"] = not has_campaign_basic
        else:
            needs["needs_benchmark"] = not has_benchmark
    return needs

def data_retriever_v2_node(state: ProjectAgentState) -> Dict[str, Any]:
    initial_messages_count = len(state.get("messages", []))
    initial_logs_count = len(state.get("debug_logs", []))
    
    # --- [CRITICAL FIX] Reset data_store for new turn ---
    # To prevent hallucinations from previous query results, we start with a fresh store.
    # Note: We keep resolved_entities as they might be useful for follow-up questions.
    state["data_store"] = {}
    
    sanitized_messages = []
    for msg in state.get("messages", []):
        if isinstance(msg, dict):
            msg_type = msg.get("type", "human")
            content = msg.get("content", "")
            if msg_type == "human":
                sanitized_messages.append(HumanMessage(content=content))
            elif msg_type == "ai":
                sanitized_messages.append(AIMessage(content=content))
            elif msg_type == "system":
                sanitized_messages.append(SystemMessage(content=content))
            elif msg_type == "tool":
                tool_call_id = msg.get("tool_call_id", "unknown")
                sanitized_messages.append(ToolMessage(content=content, tool_call_id=tool_call_id))
            else:
                sanitized_messages.append(HumanMessage(content=str(msg)))
        elif isinstance(msg, BaseMessage):
            if msg.type == "human":
                sanitized_messages.append(HumanMessage(content=msg.content))
            elif msg.type == "ai":
                sanitized_messages.append(AIMessage(content=msg.content))
            elif msg.type == "system":
                sanitized_messages.append(SystemMessage(content=msg.content))
            elif msg.type == "tool":
                sanitized_messages.append(msg)
            else:
                sanitized_messages.append(HumanMessage(content=msg.content))
        else:
            sanitized_messages.append(HumanMessage(content=str(msg)))
    local_state = state.copy()
    local_state["messages"] = sanitized_messages
    result = retriever_agent.invoke(local_state)
    final_messages = result.get("messages", [])
    new_messages = final_messages[len(sanitized_messages):]
    final_logs = result.get("debug_logs", [])
    new_logs = final_logs[initial_logs_count:]
    needs = _check_performance_tools_needed(state, result)
    routing_context = state.get("routing_context", {})
    start_date = routing_context.get("start_date", "2021-01-01")
    end_date = routing_context.get("end_date", datetime.now().strftime("%Y-%m-%d"))
    if "data_store" not in result:
        result["data_store"] = {}
    if needs.get("needs_benchmark"):
        logger.warning("Detected missing query_format_benchmark call. Auto-invoking...")
        try:
            campaign_data = result.get("data_store", {}).get("query_campaign_basic", [])
            cmp_ids = [row.get('campaign_id') for row in campaign_data if row.get('campaign_id')] if campaign_data else None
            invoke_params = {"start_date": start_date, "end_date": end_date}
            if cmp_ids:
                invoke_params["cmp_ids"] = cmp_ids
                logger.warning(f"Auto-invoking benchmark with {len(cmp_ids)} campaign IDs")
            else:
                logger.warning("Auto-invoking benchmark for å…¨ç«™æŸ¥è©¢")
            benchmark_result = query_format_benchmark.invoke(invoke_params)
            if benchmark_result.get("status") == "success" and benchmark_result.get("data"):
                result["data_store"]["query_format_benchmark"] = benchmark_result.get("data", [])
                logger.info(f"Auto-invoked query_format_benchmark, got {len(benchmark_result.get('data', []))} rows")
        except Exception as e:
            logger.warning(f"Auto-invoke benchmark failed: {e}")
    if needs.get("needs_performance") or needs.get("needs_campaign_basic"):
        logger.warning("Detected client-level performance query. Auto-invoking required tools...")
        if needs.get("needs_campaign_basic"):
            logger.warning("Auto-invoking query_campaign_basic for å…¨ç«™å®¢æˆ¶")
            try:
                campaign_result = query_campaign_basic.invoke({"start_date": start_date, "end_date": end_date})
                if campaign_result.get("status") == "success" and campaign_result.get("data"):
                    result["data_store"]["query_campaign_basic"] = campaign_result.get("data", [])
                    logger.info(f"Auto-invoked query_campaign_basic, got {len(campaign_result.get('data', []))} campaigns")
            except Exception as e:
                logger.warning(f"Auto-invoke campaign_basic failed: {e}")
        if needs.get("needs_performance"):
            campaign_data = result.get("data_store", {}).get("query_campaign_basic", [])
            cmp_ids = [row.get('campaign_id') for row in campaign_data if row.get('campaign_id')]
            if cmp_ids:
                logger.warning(f"Auto-invoking query_unified_performance with {len(cmp_ids)} campaign IDs")
                try:
                    performance_result = query_unified_performance.invoke({
                        "start_date": start_date,
                        "end_date": end_date,
                        "cmpids": cmp_ids,
                        "group_by": ["ad_format_type"]
                    })
                    if performance_result.get("status") == "success" and performance_result.get("data"):
                        result["data_store"]["query_unified_performance"] = performance_result.get("data", [])
                        logger.info(f"Auto-invoked query_unified_performance, got {len(performance_result.get('data', []))} rows")
                except Exception as e:
                    logger.warning(f"Auto-invoke unified_performance failed: {e}")
            else:
                logger.warning("Cannot invoke query_unified_performance: no campaign IDs available")
    output = {
        "messages": new_messages,
        "debug_logs": new_logs,
        "data_store": result.get("data_store"),
        "resolved_entities": result.get("resolved_entities")
    }
    return output
