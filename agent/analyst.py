"""
Data Analyst Agent for AKC Framework 3.0

This agent handles all data query requests using:
1. Entity Resolver - to identify database IDs from natural language
2. SQL Template Tools - to execute pre-defined SQL queries
3. Pandas Processor - to process and format results
"""
from typing import Dict, Any
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from config.llm import llm
from agent.state import AgentState
from tools.entity_resolver import resolve_entity
from tools.campaign_template_tool import (
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template,
    query_industry_format_budget
)
from tools.performance_tools import query_performance_metrics, query_format_benchmark
from tools.data_processing_tool import pandas_processor
import json

# Available tools for Data Analyst
TOOLS = [
    resolve_entity,
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template,
    query_industry_format_budget,
    query_performance_metrics,
    query_format_benchmark,
    pandas_processor
]

# Bind tools to LLM
llm_with_tools = llm.bind_tools(TOOLS)

ANALYST_SYSTEM_PROMPT = """ä½ æ˜¯ AKC æ™ºèƒ½åŠ©æ‰‹çš„æ•¸æ“šåˆ†æå¸« (Data Analyst)ã€‚

**ç•¶å‰æ—¥æœŸ:** {current_date}

**ä½ çš„æ ¸å¿ƒåŸå‰‡:**
- **çµ•å°ä¸è¦è‡ªå·±å¯« SQL**ã€‚æ‰€æœ‰ SQL é‚è¼¯å·²ç¶“åœ¨ Python å·¥å…·ä¸­å®šç¾©å¥½ã€‚
- ä½ çš„å·¥ä½œæ˜¯ã€Œå¡«ç©ºã€è€Œéã€Œå¯«ç¨‹å¼ã€ã€‚

**å·¥ä½œæµç¨‹:**

1. **å¯¦é«”è§£æ (Entity Resolution) - ä¸‰éšæ®µæµç¨‹**
   - **æª¢æŸ¥ Context**: å…ˆæŸ¥çœ‹ System Prompt ä¸­æ˜¯å¦å·²æœ‰ã€Œå·²ç¢ºèªçš„å¯¦é«”ã€ã€‚è‹¥æœ‰ä¸”ç¬¦åˆç•¶å‰éœ€æ±‚ï¼Œ**è«‹ç›´æ¥ä½¿ç”¨è©² IDï¼Œä¸è¦å†æ¬¡å‘¼å« resolve_entity**ã€‚
   - ä½¿ç”¨ `resolve_entity(keyword="...")` é€²è¡Œå¯¦é«”æŸ¥è©¢
   - **åƒæ•¸è¨­å®šåŸå‰‡**:
     - **é è¨­æƒ…æ³**: **ä¸è¦** è¨­å®š `target_types` (ä¿æŒé è¨­å€¼ None)ã€‚é€™æœƒåŒæ™‚æœå°‹ Campaign, Client, Agency, Brand ç­‰æ‰€æœ‰è¡¨æ ¼ã€‚
     - **ä¾‹å¤–æƒ…æ³**: åªæœ‰ç•¶ä½¿ç”¨è€…æ˜ç¢ºæŒ‡å®šé¡å‹æ™‚ï¼ˆä¾‹å¦‚ï¼šã€ŒæŸ¥è©¢**ä»£ç†å•†**äºæ€åšã€ã€ã€Œ**å®¢æˆ¶**æ‚ éŠå¡ã€ã€ã€Œ**ç”¢æ¥­**ç¾å¦ã€ï¼‰ï¼Œæ‰è¨­å®š `target_types=["agency"]` æˆ– `target_types=["industry"]`ã€‚
     - **åŸå› **: è‹¥ä½¿ç”¨è€…åªèªªã€Œæ‚ éŠå¡ã€ï¼Œå®ƒå¯èƒ½æ˜¯å®¢æˆ¶åä¹Ÿå¯èƒ½æ˜¯æ´»å‹•åã€‚å¿…é ˆæœå°‹å…¨éƒ¨ï¼Œè‹¥ç¸½çµæœåªæœ‰ 1 ç­†æ‰æœƒè‡ªå‹•åŒ¹é…ã€‚
   - å·¥å…·æœƒè‡ªå‹•åŸ·è¡Œï¼šLIKE æŸ¥è©¢ â†’ ä½¿ç”¨è€…ç¢ºèª â†’ RAG å‘é‡æœå°‹

   **è™•ç†ä¸åŒçš„è¿”å›ç‹€æ…‹:**

   a) `status: "exact_match"` - æ‰¾åˆ°å”¯ä¸€åŒ¹é…
      - ç›´æ¥ä½¿ç”¨è¿”å›çš„å¯¦é«” ID ç¹¼çºŒæŸ¥è©¢
      - ä¾‹å¦‚: `{{"status": "exact_match", "data": {{"id": 123, "name": "æ‚ éŠå¡", "type": "client"}}}}`
      - è‹¥ type ç‚º `industry` æˆ– `sub_industry`ï¼Œè«‹å°‡ ID åˆ†åˆ¥æ”¾å…¥ `industry_ids` æˆ– `sub_industry_ids` åƒæ•¸ä¸­ã€‚

   b) `status: "needs_confirmation"` - æ‰¾åˆ°å¤šå€‹åŒ¹é…
      - å‘ä½¿ç”¨è€…å±•ç¤ºé¸é …æ¸…å–®
      - ä½¿ç”¨ Markdown æ ¼å¼åŒ–é¸é …ï¼ˆç·¨è™Ÿã€åç¨±ã€é¡å‹ï¼‰
      - è¦æ±‚ä½¿ç”¨è€…å›è¦†ç·¨è™Ÿæˆ–åç¨±
      - **åœæ­¢ç•¶å‰åˆ†æï¼Œç­‰å¾…ä½¿ç”¨è€…é¸æ“‡**
      - ç•¶ä½¿ç”¨è€…å›è¦†å¾Œï¼Œä½¿ç”¨ `resolve_entity(keyword="...", selected_id=X, selected_type="...")` ç¢ºèªé¸æ“‡

   c) `status: "rag_results"` - LIKE æŸ¥è©¢ç„¡çµæœï¼Œä½† RAG æ‰¾åˆ°ç›¸ä¼¼å¯¦é«”
      - å‘ä½¿ç”¨è€…å±•ç¤º RAG å»ºè­°çš„å¯¦é«”
      - è©¢å•æ˜¯å¦ä½¿ç”¨é€™äº›å»ºè­°

   d) `status: "not_found"` - å®Œå…¨æ‰¾ä¸åˆ°
      - å‘ŠçŸ¥ä½¿ç”¨è€…ç„¡æ³•æ‰¾åˆ°è©²å¯¦é«”
      - å»ºè­°ä½¿ç”¨è€…æª¢æŸ¥æ‹¼å¯«æˆ–æä¾›æ›´å¤šè³‡è¨Š

   **è™•ç†ä½¿ç”¨è€…å›è¦†é¸æ“‡ (é‡è¦å„ªåŒ–)**:
   - è‹¥ä¸Šä¸€æ­¥æ˜¯ä½ è©¢å•ä½¿ç”¨è€…ã€Œè«‹é¸æ“‡...ã€ï¼Œè€Œä½¿ç”¨è€…å›è¦†äº†é¸æ“‡ï¼š
   - **è¦å‰‡ 1: å®Œå…¨åç¨±åŒ¹é… (Exact Name Match)**
     - è‹¥ä½¿ç”¨è€…è¼¸å…¥çš„åç¨±èˆ‡é¸é …ä¸­çš„æŸå€‹åç¨±**å®Œå…¨ç›¸åŒ**ï¼ˆä¾‹å¦‚é¸é …æœ‰ "(æš«åœä½¿ç”¨) A" å’Œ "A"ï¼Œä½¿ç”¨è€…å›è¦† "A"ï¼‰ï¼Œ**è«‹ç›´æ¥è¦–ç‚ºé¸æ“‡äº† "A"**ã€‚
     - **ç‰¹æ®Šç‹€æ³**: è‹¥æœ‰å¤šå€‹é¸é …åç¨±ä¸€æ¨¡ä¸€æ¨£ (ä¾‹å¦‚å…©å€‹éƒ½å« "å°ç£å¦®ç¶­é›…è‚¡ä»½æœ‰é™å…¬å¸")ï¼š
       - **ä¸è¦å†æ¬¡è©¢å•**ï¼é€™æœƒè®“ä½¿ç”¨è€…å›°æƒ‘ã€‚
       - **è‡ªå‹•é¸æ“‡é‚è¼¯**: å„ªå…ˆé¸æ“‡çœ‹èµ·ä¾†æ˜¯ã€Œå•Ÿç”¨ä¸­ã€çš„é‚£å€‹ (ä¾‹å¦‚æ’é™¤æœ‰ "(æš«åœä½¿ç”¨)" æ¨™è¨˜çš„)ã€‚è‹¥ç„¡æ³•åˆ¤æ–·ï¼Œé¸æ“‡ ID è¼ƒå¤§çš„é‚£å€‹ (é€šå¸¸æ˜¯è¼ƒæ–°çš„è³‡æ–™)ã€‚
       - ç›´æ¥ä½¿ç”¨è©² ID å‘¼å« `resolve_entity(..., selected_id=...)`ã€‚

   **å¤šé‡å¯¦é«”è™•ç† (Batch Processing)**:
   - è‹¥ `entity_keywords` åŒ…å«å¤šå€‹é—œéµå­— (ä¾‹å¦‚ "å°åŒ—, äºæ€åš, è–æ´‹ç§‘æŠ€")ï¼š
     - è«‹å‹™å¿…å° **æ¯ä¸€å€‹** é—œéµå­—éƒ½å‘¼å« `resolve_entity`ã€‚
     - **ä¸è¦** åœ¨ç¬¬ä¸€å€‹é—œéµå­—éœ€è¦ç¢ºèªæ™‚å°±ç›´æ¥åœæ­¢ï¼Œè«‹å…ˆè§£æå®Œæ‰€æœ‰é—œéµå­—ã€‚
     - è‹¥æœ‰å¤šå€‹å¯¦é«”éœ€è¦ç¢ºèªï¼Œè«‹åœ¨åŒä¸€æ¬¡å›æ‡‰ä¸­åˆ—å‡ºæ‰€æœ‰çš„ç¢ºèªé¸é …ã€‚
     - è‹¥éƒ¨åˆ†å¯¦é«”å·²ç¢ºèª (Exact Match)ï¼Œéƒ¨åˆ†éœ€è¦ç¢ºèªï¼Œè«‹æš«å­˜å·²ç¢ºèªçš„ IDsï¼Œä¸¦é‡å°æ¨¡ç³Šçš„é …ç›®æå•ã€‚

   **ç”¢æ¥­åˆ¥æŸ¥è©¢å„ªåŒ– (Industry Aggregation Rule)**:
   - ç•¶ä½¿ç”¨è€…æŸ¥è©¢ç”¢æ¥­ (å¦‚ã€Œç¾å¦ã€ã€ã€ŒéŠæˆ²ã€) æ™‚ï¼š
     - è‹¥ `resolve_entity` å›å‚³å¤šå€‹ç›¸é—œçš„ç”¢æ¥­é¡åˆ¥ (åŒ…å« `industry` èˆ‡ `sub_industry`)ï¼Œä¸”åç¨±èˆ‡é—œéµå­—é«˜åº¦ç›¸é—œã€‚
     - **ä¸è¦è¦æ±‚ä½¿ç”¨è€…é€ä¸€ç¢ºèª**ã€‚
     - è«‹**ä¸»å‹•åˆä½µæ‰€æœ‰ç›¸é—œ ID** (ä¾‹å¦‚åŒæ™‚å‚³å…¥ `industry_ids=[2]` èˆ‡ `sub_industry_ids=[26, 16]`)ã€‚
     - ç›®çš„ï¼šç¢ºä¿çµ±è¨ˆçµæœæ¶µè“‹è©²ç”¢æ¥­çš„æ‰€æœ‰ç›¸é—œæ¨™ç±¤ï¼Œä¸¦æä¾›æœ€å®Œæ•´çš„ç¸½é ç®—ã€‚

   - **è¦å‰‡ 2: ç·¨è™Ÿé¸æ“‡**
     - è‹¥ä½¿ç”¨è€…å›è¦†æ•¸å­— (å¦‚ "1")ï¼Œå°æ‡‰é¸é …æ¸…å–®çš„ç·¨è™Ÿã€‚

   - **è¦å‰‡ 3: éƒ¨åˆ†åŒ¹é…**
     - è‹¥ä½¿ç”¨è€…å›è¦†éƒ¨åˆ†åç¨±ï¼Œå˜—è©¦æ‰¾åˆ°æœ€æ¥è¿‘çš„åŒ¹é…é …ã€‚

   - **ç¦æ­¢äº‹é …**: åš´ç¦åœ¨ä½¿ç”¨è€…å·²ç¶“æ˜ç¢ºå›è¦†åç¨±ï¼ˆä¸”è©²åç¨±åœ¨é¸é …ä¸­å­˜åœ¨ï¼‰çš„æƒ…æ³ä¸‹ï¼Œå†æ¬¡è·³å‡ºä¸€æ¨£çš„é¸é …è¦æ±‚ç¢ºèªã€‚

   **ç‰¹æ®Šæƒ…å¢ƒï¼šæ’åèˆ‡å…¨å±€æŸ¥è©¢ (Ranking / Global Queries)**
   - è‹¥ System Prompt é¡¯ç¤º `entity_keywords` ç‚ºç©ºï¼Œä¸”å•é¡Œæ¶‰åŠã€Œæ’åã€ã€ã€ŒTop Xã€ã€ã€Œç¸½é¡ã€ï¼š
     - **è·³éå¯¦é«”è§£æ** (Do not call resolve_entity)ã€‚
     - ç›´æ¥ä½¿ç”¨ SQL å·¥å…·é€²è¡Œå»£æ³›æŸ¥è©¢ã€‚
     - **å‹™å¿…æ”¾å¤§ Limit**ï¼šå‘¼å« `query_investment_budget` æˆ– `query_execution_budget` æ™‚ï¼Œè«‹è¨­å®š `limit=5000` ä»¥ç¢ºä¿çµ±è¨ˆçµæœæ¶µè“‹å®Œæ•´å¹´åº¦æ•¸æ“šã€‚
     - **åˆ†çµ„ä¾æ“š**ï¼š
       - å»£å‘Šä¸»æ’åï¼šé‡å° `client_name` é€²è¡Œ `groupby_sum`ã€‚
       - ä»£ç†å•†æ’åï¼šé‡å° `agency_name` é€²è¡Œ `groupby_sum`ã€‚
     - **æ•¸é‡èªªæ˜**ï¼šè‹¥æœ€çµ‚çµæœå°‘æ–¼ä½¿ç”¨è€…è¦æ±‚çš„æ•¸é‡ï¼ˆä¾‹å¦‚æ±‚ Top 20 ä½†åªåˆ—å‡º 5 å€‹ï¼‰ï¼Œè«‹åœ¨å›æ‡‰ä¸­èªªæ˜ã€Œè©²æœŸé–“åƒ…æœ‰ 5 ç­†ç¬¦åˆæ¢ä»¶çš„è³‡æ–™ã€ã€‚

   **æ¯æœˆ/é€±æœŸæ€§åˆ†æ (Monthly / Period Analysis)**:
   - è‹¥ä½¿ç”¨è€…è¦æ±‚ã€Œæ¯æœˆã€ã€ã€Œæ¯å­£ã€ã€ã€Œå¹´åº¦è¶¨å‹¢ã€ï¼š
     1. å‘¼å« SQL å·¥å…·ï¼ˆå¦‚ `query_investment_budget`ï¼‰ï¼Œç¢ºä¿è³‡æ–™ä¸­åŒ…å«æ—¥æœŸæ¬„ä½ã€‚
     2. å‘¼å« `pandas_processor(operation="add_time_period", date_col="...", period="month")` ç”Ÿæˆ `period` æ¬„ä½ã€‚
     3. å†æ¬¡å‘¼å« `pandas_processor(operation="groupby_sum", groupby_col="period, agency_name", ...)` é€²è¡ŒåŒ¯ç¸½ã€‚
     4. çµ•å°ä¸è¦å› ç‚ºåŸå§‹æ•¸æ“šä¸­æ²’æœ‰ "month" æ¬„ä½å°±ç›´æ¥èªªç„¡æ³•å½™æ•´ï¼Œä½ å¿…é ˆä¸»å‹•ä½¿ç”¨å·¥å…·ç”Ÿæˆå®ƒã€‚

4. **è³‡æ–™è™•ç† (CRITICAL!)**

   **åŸºç¤æŸ¥è©¢å·¥å…·**:
   - `query_campaign_basic`: æŸ¥è©¢æ´»å‹•åŸºæœ¬è³‡è¨Šï¼ˆå®¢æˆ¶ã€æ´»å‹•åç¨±ã€æ—¥æœŸã€é ç®—ï¼‰
     - é©ç”¨ï¼šå–å¾— campaign IDsã€åŸºæœ¬æ¦‚è¦½
     - åƒæ•¸ï¼šclient_names, client_ids, industry_ids, sub_industry_ids, campaign_ids, start_date, end_date
     - **é‡è¦**: ç•¶å·²ç¢ºèªå¯¦é«”ç‚º Client (ID=X) æ™‚ï¼Œè«‹å‹™å¿…ä½¿ç”¨ `client_ids=[X]`ï¼Œ**ä¸è¦**åªå‚³åç¨±ã€‚

   **é ç®—ç›¸é—œå·¥å…·**:
   - `query_investment_budget`: æŸ¥è©¢ã€Œé€²å–®/æŠ•è³‡ã€é‡‘é¡ï¼ˆæ ¼å¼å±¤ç´šæ˜ç´°ï¼‰
     - é©ç”¨ï¼šã€Œé ç®—ã€ã€ã€Œé€²å–®ã€ã€ã€ŒæŠ•è³‡é‡‘é¡ã€ç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names, client_ids, industry_ids, sub_industry_ids, campaign_ids, start_date, end_date
     - **é‡è¦**: å„ªå…ˆä½¿ç”¨ `client_ids` æˆ– `campaign_ids` é€²è¡Œç²¾æº–éæ¿¾ã€‚
     - **é‡è¦**: è‹¥æ¶‰åŠç”¢æ¥­æŸ¥è©¢ï¼Œè«‹å‹™å¿…å°‡ç›¸é—œçš„å¤§é¡ ID (`industry_ids`) èˆ‡å­é¡ ID (`sub_industry_ids`) **åˆä½µåœ¨åŒä¸€æ¬¡å·¥å…·èª¿ç”¨ä¸­**ï¼Œä¸è¦åˆ†é–‹å¤šæ¬¡èª¿ç”¨ã€‚

   - `query_execution_budget`: æŸ¥è©¢ã€ŒåŸ·è¡Œ/èªåˆ—ã€é‡‘é¡ï¼ˆåŸ·è¡Œå–®å±¤ç´šæ˜ç´°ï¼‰
     - é©ç”¨ï¼šã€ŒåŸ·è¡Œã€ã€ã€Œèªåˆ—ã€ã€ã€Œå¯¦éš›èŠ±è²»ã€ç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names, client_ids, industry_ids, sub_industry_ids, campaign_ids, start_date, end_date

   - `query_budget_details`: æŸ¥è©¢é ç®—æ‘˜è¦ï¼ˆæ•´åˆæŠ•è³‡èˆ‡åŸ·è¡Œé‡‘é¡ï¼‰
     - é©ç”¨ï¼šã€Œé ç®—ç¼ºå£ã€ã€ã€Œé ç®—å°æ¯”ã€åˆ†æ
     - âš ï¸ å¿…é ˆæä¾› campaign_idsï¼ˆéœ€å…ˆå‘¼å« query_campaign_basicï¼‰

   **æ ¼å¼èˆ‡å—çœ¾å·¥å…·**:
   - `query_ad_formats`: æŸ¥è©¢å»£å‘Šæ ¼å¼æ˜ç´°
     - é©ç”¨ï¼šã€Œæ ¼å¼ã€ã€ã€Œå»£å‘Šå½¢å¼ã€ã€ã€Œç§’æ•¸ã€ã€ã€Œå¹³å°ã€ç›¸é—œå•é¡Œ
     - âš ï¸ å¿…é ˆæä¾› campaign_ids

   - `query_targeting_segments`: æŸ¥è©¢æ•¸æ“šé–å®š/å—çœ¾æ¨™ç±¤
     - é©ç”¨ï¼šã€Œæ•¸æ“šé–å®šã€ã€ã€Œå—çœ¾ã€ã€ã€ŒTAã€ã€ã€Œæ¨™ç±¤ã€ç›¸é—œå•é¡Œ
     - âš ï¸ å¿…é ˆæä¾› campaign_ids

   **æˆæ•ˆæ•¸æ“šå·¥å…·**:
   - `query_performance_metrics`: æŸ¥è©¢ ClickHouse æˆæ•ˆæ•¸æ“šï¼ˆCTR, VTR, ER, Impressions, Clicksï¼‰
     - é©ç”¨ï¼šæ‰€æœ‰æˆæ•ˆç›¸é—œå•é¡Œ
     - **åƒæ•¸å¼·åˆ¶è¦å‰‡ (Mandatory Params)**:
       - **å¿…é ˆä¸”åªèƒ½ä½¿ç”¨ `cmp_ids`**ã€‚
       - **åš´ç¦** ä½¿ç”¨ `client_names` æŸ¥è©¢æˆæ•ˆï¼Œå› ç‚º ClickHouse ä¸­çš„åç¨±åŒ¹é…æ¥µä¸ç©©å®šã€‚
       - **æ¨™æº–æµç¨‹**:
         1. æ°¸é å…ˆå‘¼å« `query_campaign_basic` å–å¾—è©²å®¢æˆ¶æˆ–æ´»å‹•çš„ Campaign IDsã€‚
         2. å°‡å–å¾—çš„ `campaign_id` åˆ—è¡¨å‚³å…¥ `query_performance_metrics(cmp_ids=[...])`ã€‚
     - **ç¶­åº¦èªªæ˜**:
       - `dimension='format'`: æŒ‰å»£å‘Šæ ¼å¼åˆ†çµ„ï¼ˆé è¨­ï¼‰ã€‚
       - `dimension='campaign'`: æŒ‰æ´»å‹•åˆ†çµ„ã€‚

   **çµ±è¨ˆèˆ‡åŸºæº–å·¥å…· (Statistical & Benchmark Tools)**:
   - `query_industry_format_budget`: å¤šç¶­åº¦é ç®—åˆ†ä½ˆçµ±è¨ˆ
     - **åƒæ•¸ dimension (é‡è¦)**:
       - `dimension='industry'` (é è¨­): å¤§é¡ç”¢æ¥­ã€‚
       - `dimension='sub_industry'`: å­é¡ç”¢æ¥­ (æ¨è–¦ç”¨æ–¼è©³ç´°ç”¢æ¥­åˆ†æ)ã€‚
       - `dimension='client'`: å®¢æˆ¶ã€‚
       - `dimension='agency'`: ä»£ç†å•†ã€‚
     - **åƒæ•¸ split_by_format**:
       - `True`: é¡¯ç¤ºæ ¼å¼ç´°ç¯€ (é è¨­)ã€‚
       - `False`: åƒ…é¡¯ç¤ºç¸½è¨ˆã€‚
     - **åƒæ•¸ primary_view**:
       - `'dimension'` (é è¨­): ç¬¬ä¸€æ¬„ç‚ºç”¢æ¥­/å®¢æˆ¶ã€‚
       - `'format'`: ç¬¬ä¸€æ¬„ç‚ºæ ¼å¼ã€‚
     - **ä½¿ç”¨ç¯„ä¾‹**:
       - æŸ¥ã€ŒBanner æŠ•åˆ°å“ªäº›ç”¢æ¥­ã€: `dimension='industry'`, `format_ids=[BannerID]`, `primary_view='format'`
       - æŸ¥ã€ŒBanner çš„å‰åå¤§å®¢æˆ¶ã€: `dimension='client'`, `format_ids=[BannerID]`, `primary_view='format'`
       - æŸ¥ã€Œæ‰€æœ‰æ ¼å¼æŠ•æ”¾åˆ°çš„ç”¢æ¥­ã€: `dimension='industry'`, `primary_view='format'`
       - æŸ¥ã€Œæ±½è»Šç”¢æ¥­çš„æ ¼å¼åˆ†ä½ˆã€: `dimension='industry'`, `industry_ids=[AutoID]`, `primary_view='dimension'`

   - `query_format_benchmark`: æŸ¥è©¢æ ¼å¼æˆæ•ˆåŸºæº–èˆ‡æ’å
     - é©ç”¨ï¼šæŸ¥è©¢ã€Œæ‰€æœ‰æ ¼å¼çš„ CTR æ’åã€ã€ã€Œæ±½è»Šç”¢æ¥­çš„å¹³å‡ VTRã€ç­‰**åŸºæº–å‹**å•é¡Œã€‚
     - **å„ªé»**ï¼šç›´æ¥å›å‚³ CTR/VTR å¹³å‡å€¼èˆ‡æ’åï¼Œç„¡éœ€è™•ç†å€‹åˆ¥ Campaignã€‚
     - åƒæ•¸ï¼šcmp_ids (é¸å¡«ï¼Œç”¨æ–¼ç”¢æ¥­ç¯©é¸), format_ids (é¸å¡«)ã€‚

   **é€²éšå·¥å…·**:
   - `execute_sql_template`: é€šç”¨æ¨¡æ¿åŸ·è¡Œå™¨
     - é©ç”¨ï¼šmedia_placements.sql, product_lines.sql, contract_kpis.sql, execution_status.sql
     - **é‡è¦**: è‹¥ä½¿ç”¨è€…è©¢å•ã€Œ**å»£å‘Šæ ¼å¼èˆ‡åŸ·è¡Œé‡‘é¡**ã€(æŒ‰æ ¼å¼åˆ†å‡ºçš„èªåˆ—é‡‘é¡)ï¼Œè«‹å„ªå…ˆä½¿ç”¨ `media_placements.sql`ã€‚è©²æ¨¡æ¿åŒ…å« `ad_format_name` èˆ‡åŸ·è¡Œå±¤ç´šçš„ `budget`ã€‚
     - è‹¥éœ€éæ¿¾ç”¢æ¥­ï¼Œå¯ä½¿ç”¨ `industry_ids` (Category) æˆ– `sub_industry_ids` (Sub-Category) åƒæ•¸ã€‚
     - åªåœ¨ä¸Šè¿°å°ˆç”¨å·¥å…·ä¸é©ç”¨æ™‚æ‰ä½¿ç”¨

3. **åˆ¤æ–·æ—¥æœŸç¯„åœ (é‡è¦)**
   - **ç³»çµ±å·²æŒ‡å®šæŸ¥è©¢ç¯„åœ**:
     - **Start Date**: {start_date}
     - **End Date**: {end_date}
   - **è«‹å‹™å¿…å°‡æ­¤æ—¥æœŸç¯„åœæ‡‰ç”¨æ–¼æ‰€æœ‰æŸ¥è©¢å·¥å…·çš„ `start_date` èˆ‡ `end_date` åƒæ•¸ã€‚**
   - **æœ€çµ‚å›æ‡‰è¦æ±‚**:
     - åœ¨å›ç­”é–‹é ­æˆ–çµå°¾ï¼Œå¿…é ˆæ˜ç¢ºèªªæ˜ï¼šã€Œ**æœ¬æ•¸æ“šæ¶µè“‹ç¯„åœ: {start_date} è‡³ {end_date}**ã€ã€‚

   **âš ï¸ æŸ¥ç„¡è³‡æ–™æ™‚çš„è™•ç†ç­–ç•¥ (Retry Strategy)**:
   - è‹¥ä½¿ç”¨ `query_campaign_basic` æŸ¥è©¢ç‰¹å®šå®¢æˆ¶ä½†åœ¨æŒ‡å®šæ—¥æœŸå…§å›å‚³ 0 ç­†çµæœï¼š
     - **ä¸è¦ç›´æ¥æ”¾æ£„ï¼**
     - è«‹**ç«‹åˆ»**å†æ¬¡å‘¼å« `query_campaign_basic`ï¼Œä½†**ç§»é™¤ start_date èˆ‡ end_date åƒæ•¸**ã€‚
     - ç›®çš„ï¼šç¢ºèªè©²å®¢æˆ¶æ˜¯å¦åœ¨å…¶ä»–å¹´ä»½æœ‰æ´»å‹•è³‡æ–™ã€‚è‹¥æœ‰ï¼Œè«‹å‘ŠçŸ¥ä½¿ç”¨è€…ã€Œè©²æœŸé–“ç„¡æ´»å‹•ï¼Œä½†æ‰¾åˆ°å…¶ä»–æœŸé–“çš„ç´€éŒ„...ã€ã€‚

4. **è³‡æ–™è™•ç† (CRITICAL!)**
   - SQL å·¥å…·å›å‚³åŸå§‹æ•¸æ“šï¼Œå¯èƒ½åŒ…å« NULL æˆ–é‡è¤‡çš„ entity_name
   - **ä½ å¿…é ˆ ALWAYS ä½¿ç”¨ `pandas_processor` è™•ç†æ•¸æ“šï¼**
   - **é‡è¦**ï¼šèª¿ç”¨ pandas_processor æ™‚ï¼Œ**ä¸è¦å‚³ `data` åƒæ•¸**ï¼Œç³»çµ±æœƒè‡ªå‹•æ³¨å…¥å®Œæ•´æ•¸æ“š

   **âš ï¸ CRITICAL - ç†è§£æ•¸æ“šç‹€æ…‹ï¼**
   - **è²¡å‹™å·¥å…·** (investment_budget, execution_budget) â†’ è¿”å›**åŸå§‹æ˜ç´°æ•¸æ“š**ï¼ˆå¯èƒ½æœ‰å¤šè¡Œï¼‰â†’ **å¿…é ˆä½¿ç”¨ `groupby_sum`**ã€‚
   - **æˆæ•ˆå·¥å…·** (query_performance_metrics) â†’ è¿”å›**å·²åŒ¯ç¸½æ•¸æ“š**ï¼ˆå·²æŒ‰ dimension åˆ†çµ„ï¼‰â†’ **ç¦æ­¢ä½¿ç”¨ `groupby_sum`** (æœƒå°è‡´ CTR/VTR éºå¤±æˆ–è¨ˆç®—éŒ¯èª¤)ã€‚è«‹ç›´æ¥ä½¿ç”¨ `operation="top_n"` æˆ– `operation="sort"` ä¾†å‘ˆç¾ã€‚

   **è™•ç†è²¡å‹™æ•¸æ“šï¼ˆåŸå§‹æ˜ç´°ï¼‰**ï¼š
   - ä½¿ç”¨ `operation="groupby_sum"` åˆ†çµ„åŠ ç¸½
   - **åƒæ•¸è¦å‰‡**ï¼š
     - `groupby_col`: åˆ†çµ„æ¬„ä½ï¼ˆå¦‚ "format_name"ï¼‰
     - `sum_col`: **æ”¯æ´å¤šæ¬„ä½**ï¼ˆé€—è™Ÿåˆ†éš”å­—ä¸²ï¼Œå¦‚ "amount,budget,clicks"ï¼‰
   - **ç¤ºä¾‹**ï¼š
     ```python
     pandas_processor(
         operation="groupby_sum",
         groupby_col="format_name",
         sum_col="investment_amount,investment_gift",
         ascending=False
     )
     ```

      **åˆä½µæ•¸æ“šç­–ç•¥ (Merge Strategy - è£½ä½œå–®ä¸€å¤§è¡¨)**:
      - **æ ¸å¿ƒåŸå‰‡**: ç„¡è«–ä½¿ç”¨è€…å•äº†å¤šå°‘å€‹ç¶­åº¦ï¼Œæœ€çµ‚**åªèƒ½è¼¸å‡ºä¸€å¼µæ•´åˆè¡¨æ ¼**ã€‚
      - **Step 1: æ±ºå®šä¸»è¡¨ (Anchor Table)**
        - è‹¥æŸ¥è©¢åŒ…å«ã€ŒæŠ•è³‡é‡‘é¡ã€ã€ã€Œé ç®—åˆ†é…ã€â†’ ä¸»è¡¨ç‚º `query_investment_budget` (Format Level)ã€‚
        - è‹¥æŸ¥è©¢åŒ…å«ã€Œå»£å‘Šæ ¼å¼ã€ã€ã€Œæˆæ•ˆã€â†’ ä¸»è¡¨ç‚º `query_ad_formats` æˆ– `query_performance_metrics` (Format Level)ã€‚
      - **Step 2: æº–å‚™å±¬æ€§è³‡æ–™ (Attributes)**
        - **Segments**: å‘¼å« `query_targeting_segments`ï¼Œä¸¦**å‹™å¿…**å…ˆç”¨ `groupby_concat` å£“å¹³æˆ "One Row per Campaign" (concat_col="segment_name")ã€‚
      - **Step 3: åŸ·è¡Œåˆä½µ (Left Join Sequence) - é€™æ˜¯æœ€é—œéµçš„ä¸€æ­¥ï¼**
        - ä½ ä¸èƒ½åˆ†é–‹å±•ç¤ºã€ŒæŠ•è³‡é‡‘é¡è¡¨ã€å’Œã€Œæˆæ•ˆè¡¨ã€ã€‚ä½ å¿…é ˆä½¿ç”¨ `pandas_processor(operation="merge", ...)` å°‡å®ƒå€‘åˆè€Œç‚ºä¸€ã€‚
        - **é †åº**:
          1. å–å¾—ä¸»è¡¨è³‡æ–™ (ä¾‹å¦‚ Investment)ã€‚
          2. å–å¾—å‰¯è¡¨è³‡æ–™ (ä¾‹å¦‚ Performance)ã€‚
          3. å‘¼å« `pandas_processor(operation="merge", data=ä¸»è¡¨, merge_data=å‰¯è¡¨, merge_on="format_type_id" or "campaign_id", ...)`ã€‚
          4. è‹¥é‚„æœ‰ Segmentsï¼Œå†å°‡çµæœèˆ‡ Segments è¡¨é€²è¡Œ Mergeã€‚
      
      **Mandatory Output Step (å¼·åˆ¶è¼¸å‡ºæ­¥é©Ÿ)**:
      - **ç•¶ä½ å·²ç¶“å‘¼å«äº†ä»»ä½• `query_*` å·¥å…·ä¸¦ç²å¾—è³‡æ–™å¾Œï¼Œä½ å¿…é ˆä¸”åªèƒ½åšä¸€ä»¶äº‹ï¼š**
      - **å‘¼å« `pandas_processor` è¼¸å‡ºæœ€çµ‚åˆä½µè¡¨**ã€‚
      - **é‡è¦åƒæ•¸**: 
        - å‹™å¿…ä½¿ç”¨ `select_columns` æŒ‡å®šä½¿ç”¨è€…æ„Ÿèˆˆè¶£çš„æ‰€æœ‰æ¬„ä½ (ä¾‹å¦‚ `['å»£å‘Šæ ¼å¼', 'æŠ•è³‡é‡‘é¡', 'CTR', 'æ•¸æ“šé–å®š']`)ã€‚
        - å¦‚æœä½ æ²’æŒ‡å®š `select_columns`ï¼Œæˆ–è€…ä½ çš„è¡¨ä¸­ç¼ºå°‘äº†æŸäº›æ¬„ä½ (å› ç‚ºæ²’ Merge)ï¼Œä½¿ç”¨è€…æœƒè¦ºå¾—ä½ æ²’å›ç­”å®Œæ•´ã€‚
      - **ç¦æ­¢**ï¼šç¦æ­¢åœ¨ç²å¾— SQL è³‡æ–™å¾Œï¼Œä¸ç¶“é pandas_processor ç›´æ¥å›ç­”ã€Œè³‡æ–™å¦‚ä¸‹...ã€ã€‚
      - **ç¦æ­¢**ï¼šç¦æ­¢åˆ†é–‹è¼¸å‡ºå¤šå¼µå°è¡¨ï¼Œå¿…é ˆ Merge æˆä¸€å¼µå¤§è¡¨ã€‚
5. **æœ€çµ‚å›æ‡‰ (Critical)**
   - è«‹æä¾›ç°¡æ½”çš„æ•¸æ“šæ´å¯Ÿï¼Œä¸¦ä»¥ã€Œè©³ç´°æ•¸æ“šå¦‚ä¸‹è¡¨ï¼šã€ä½œç‚ºçµå°¾ã€‚
   - **ä¸è¦** è¼¸å‡ºè¡¨æ ¼å…§å®¹ã€‚

**ç•¶å‰æƒ…å¢ƒ:**
- ä½¿ç”¨è€…æŸ¥è©¢: {original_query}
- é—œéµå­—æç¤º: å¯¦é«”={entity_keywords}, æ™‚é–“={time_keywords}
- åˆ†ææç¤º: {analysis_hint}

ç¾åœ¨é–‹å§‹å·¥ä½œå§!
"""


def data_analyst_node(state: AgentState) -> Dict[str, Any]:
    """
    Data Analyst Agent: Resolves entities, queries data, processes results.

    Workflow:
    1. Extract routing context from Intent Router
    2. Use Entity Resolver to find IDs
    3. Call appropriate SQL Template Tool
    4. Process data with Pandas
    5. Return formatted response

    Args:
        state: Current agent state

    Returns:
        Updated state with analyst results
    """
    from datetime import datetime

    # Get current date
    now = datetime.now()
    current_date = now.strftime("%Y-%m-%d")
    current_year = now.year
    last_year = current_year - 1

    # Extract context from state
    routing_context = state.get("routing_context", {})
    original_query = routing_context.get("original_query", "")
    entity_keywords = routing_context.get("entity_keywords", [])
    time_keywords = routing_context.get("time_keywords", [])
    analysis_hint = routing_context.get("analysis_hint")
    
    # [NEW] Extract dates
    start_date = routing_context.get("start_date")
    end_date = routing_context.get("end_date")
    
    # Fallback default dates if not provided (safety net)
    if not start_date or not end_date:
        start_date = f"{last_year}-01-01"
        end_date = f"{current_year}-12-31"
        print(f"DEBUG [DataAnalyst] Using fallback dates: {start_date} ~ {end_date}")
    else:
        print(f"DEBUG [DataAnalyst] Using router provided dates: {start_date} ~ {end_date}")

    # Load previously resolved entities from state
    resolved_entities_state = state.get("resolved_entities", [])
    if resolved_entities_state is None:
        resolved_entities_state = []

    # Format resolved entities for context
    resolved_context_str = ""
    if resolved_entities_state:
        resolved_names = [f"{e.get('name')} ({e.get('type', 'unknown')}, ID: {e.get('id')})" for e in resolved_entities_state]
        resolved_context_str = f"\n**å·²ç¢ºèªçš„å¯¦é«” (ç„¡éœ€å†æ¬¡è©¢å•):** {', '.join(resolved_names)}"
        print(f"DEBUG [DataAnalyst] Loaded resolved entities: {resolved_names}")

    # Handle multimodal content format (Gemini API may return list)
    if isinstance(original_query, list):
        text_parts = []
        for part in original_query:
            if isinstance(part, dict) and part.get("type") == "text":
                text_parts.append(part.get("text", ""))
            elif isinstance(part, str):
                text_parts.append(part)
        original_query = " ".join(text_parts).strip()
        print(f"DEBUG [DataAnalyst] Converted multimodal query to string")

    # Ensure original_query is string
    if not isinstance(original_query, str):
        original_query = str(original_query)

    # Initialize logs
    execution_logs = []

    print(f"DEBUG [DataAnalyst] Starting analysis for: {original_query[:100]}...")
    print(f"DEBUG [DataAnalyst] Context: entities={entity_keywords}, time={time_keywords}, hint={analysis_hint}")
    print(f"DEBUG [DataAnalyst] Current date: {current_date}, Year: {current_year}")

    # Log initial context
    execution_logs.append({
        "step": "start",
        "timestamp": datetime.now().isoformat(),
        "query": original_query,
        "context": {
            "entity_keywords": entity_keywords,
            "time_keywords": time_keywords,
            "start_date": start_date,
            "end_date": end_date,
            "analysis_hint": analysis_hint,
            "resolved_entities_count": len(resolved_entities_state)
        }
    })

    # Build conversation with system prompt
    messages = [
        SystemMessage(content=ANALYST_SYSTEM_PROMPT.format(
            current_date=current_date,
            current_year=current_year,
            last_year=last_year,
            original_query=original_query,
            entity_keywords=entity_keywords,
            time_keywords=time_keywords,
            start_date=start_date, # [NEW]
            end_date=end_date,     # [NEW]
            analysis_hint=analysis_hint or "æœªæŒ‡å®š"
        ) + resolved_context_str),
        HumanMessage(content=f"è«‹å”åŠ©åˆ†æé€™å€‹æŸ¥è©¢ï¼š{original_query}")
    ]

    # Agent ReAct Loop (max 15 iterations to prevent infinite loops)
    final_data = None
    markdown_response = ""
    cached_markdown_table = ""  # Store the perfect table from tool
    has_reminded_to_process = False # Failsafe flag

    # Initialize with state values to preserve memory
    resolved_entities = list(resolved_entities_state)
    latest_query_data = None  # Store complete SQL result for pandas_processor

    for iteration in range(15):
        print(f"DEBUG [DataAnalyst] Iteration {iteration + 1}")

        # Invoke LLM with tools
        response = llm_with_tools.invoke(messages)
        
        # Check if LLM returned final answer (no tool calls)
        if not response.tool_calls:
            # --- [NEW] Failsafe: Check if LLM tried to finish without processing data ---
            has_query = any("query_" in str(log.get("tool")) or log.get("tool") == "execute_sql_template" 
                            for log in execution_logs if log.get("step") == "tool_call")
            has_processor = any(log.get("tool") == "pandas_processor" 
                                for log in execution_logs if log.get("step") == "tool_call")
            
            if has_query and not has_processor and not has_reminded_to_process:
                print("DEBUG [DataAnalyst] LLM tried to finish without pandas_processor. Injecting reminder.")
                has_reminded_to_process = True
                messages.append(AIMessage(content="æˆ‘å·²ç¶“æ”¶é›†å®Œè³‡æ–™ï¼Œç¾åœ¨æº–å‚™é€²è¡Œæ•´åˆã€‚")) # è®“æ­·å²è¨˜éŒ„é€£è²«
                messages.append(HumanMessage(content="[ç³»çµ±æç¤º] ä½ å·²ç¶“æŸ¥è©¢äº†æ•¸æ“šï¼Œä½†å°šæœªå‘¼å« pandas_processor é€²è¡Œè³‡æ–™åˆä½µèˆ‡è¡¨æ ¼ç”¢å‡ºã€‚è«‹å‹™å¿…ä½¿ç”¨ pandas_processor(operation='merge', ...) æ•´åˆæ‰€æœ‰ç¶­åº¦ï¼ˆåŒ…å«æŠ•è³‡é‡‘é¡ã€æˆæ•ˆã€å—çœ¾ç­‰ï¼‰ä¸¦è¼¸å‡ºæœ€çµ‚è¡¨æ ¼ã€‚ç¦æ­¢ç›´æ¥çµæŸï¼"))
                continue # Re-run loop with the reminder
            
            messages.append(response)
            markdown_response = response.content
            if isinstance(markdown_response, list):
                markdown_response = " ".join([
                    item.get("text", "") if isinstance(item, dict) else str(item)
                    for item in markdown_response
                ])
            print(f"DEBUG [DataAnalyst] Agent finished with response: {markdown_response[:200]}...")
            
            execution_logs.append({
                "step": "finish",
                "timestamp": datetime.now().isoformat(),
                "response_preview": markdown_response[:200]
            })
            break

        messages.append(response)
        # Execute tool calls
        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            args = tool_call["args"]

            print(f"DEBUG [DataAnalyst] Calling tool: {tool_name}")
            print(f"DEBUG [DataAnalyst] Arguments: {args}")

            # Log tool call
            execution_logs.append({
                "step": "tool_call",
                "timestamp": datetime.now().isoformat(),
                "iteration": iteration + 1,
                "tool": tool_name,
                "args": args
            })

            # Map tool name to function
            tool_map = {
                "resolve_entity": resolve_entity,
                "query_campaign_basic": query_campaign_basic,
                "query_budget_details": query_budget_details,
                "query_investment_budget": query_investment_budget,
                "query_execution_budget": query_execution_budget,
                "query_targeting_segments": query_targeting_segments,
                "query_ad_formats": query_ad_formats,
                "execute_sql_template": execute_sql_template,
                "query_industry_format_budget": query_industry_format_budget,
                "query_performance_metrics": query_performance_metrics,
                "query_format_benchmark": query_format_benchmark,
                "pandas_processor": pandas_processor
            }

            tool_func = tool_map.get(tool_name)
            if not tool_func:
                error_msg = f"Error: Tool '{tool_name}' not found."
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=error_msg
                ))
                execution_logs.append({
                    "step": "tool_error",
                    "timestamp": datetime.now().isoformat(),
                    "tool": tool_name,
                    "error": error_msg
                })
                continue

            # Pre-process: Inject data for pandas_processor BEFORE execution
            try:
                if tool_name == "pandas_processor" and latest_query_data and not args.get("data"):
                    # Convert Decimal and Date before injecting
                    from decimal import Decimal
                    from datetime import date, datetime
                    
                    def _safe_convert(obj):
                        if isinstance(obj, dict):
                            return {k: _safe_convert(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [_safe_convert(item) for item in obj]
                        elif isinstance(obj, Decimal):
                            return float(obj)
                        elif isinstance(obj, (date, datetime)):
                            return obj.isoformat()
                        else:
                            return obj

                    args["data"] = _safe_convert(latest_query_data)
                    print(f"DEBUG [DataAnalyst] Injected {len(latest_query_data)} complete records into pandas_processor")
                    
                    execution_logs.append({
                        "step": "data_injection",
                        "timestamp": datetime.now().isoformat(),
                        "tool": tool_name,
                        "data_count": len(latest_query_data)
                    })

                # Execute tool
                result = tool_func.invoke(args)
                
                # Log tool result (summary)
                log_entry = {
                    "step": "tool_result",
                    "timestamp": datetime.now().isoformat(),
                    "tool": tool_name,
                    "status": "unknown"
                }

                # Post-process: Handle results based on tool type
                if tool_name == "resolve_entity":
                    # Store entity resolution results
                    if isinstance(result, dict):
                        status = result.get("status")
                        log_entry["status"] = status

                        if status == "exact_match":
                            # å„²å­˜å·²ç¢ºèªçš„å¯¦é«”
                            resolved_entities.append(result.get("data"))
                            llm_result = result
                            log_entry["details"] = f"Resolved: {result.get('data', {}).get('name')}"

                        elif status == "merged_match":
                            # è‡ªå‹•åˆä½µå¤šå€‹åŒåå¯¦é«”
                            merged_list = result.get("data", [])
                            resolved_entities.extend(merged_list)
                            
                            # å»ºç«‹è¨Šæ¯å‘ŠçŸ¥ LLM
                            names_str = ", ".join([f"{item['name']} ({item['type']} ID:{item['id']})" for item in merged_list])
                            llm_result = {
                                "status": "success", 
                                "message": f"âœ… å·²è‡ªå‹•åˆä½µ {len(merged_list)} å€‹åŒåå¯¦é«”: {names_str}",
                                "data": merged_list,
                                "instruction": "è«‹æ ¹æ“šä¸Šè¿° ID åˆ†åˆ¥å‘¼å«å°æ‡‰çš„å·¥å…· (ä¾‹å¦‚ query_campaign_basic ç”¨ client_ids, query_investment_budget ç”¨ agency_ids)"
                            }
                            log_entry["details"] = f"Merged {len(merged_list)} entities"

                        elif status == "needs_confirmation":
                            # æ ¼å¼åŒ–å¤šé¸é …å±•ç¤º (åˆ†çµ„åŒ–)
                            candidates = result.get("data", [])
                            
                            # æŒ‰é¡å‹åˆ†çµ„
                            grouped = {}
                            for c in candidates:
                                c_type = c.get("type", "other")
                                if c_type not in grouped:
                                    grouped[c_type] = []
                                grouped[c_type].append(c)
                            
                            type_labels = {
                                "client": "ğŸ¢ å®¢æˆ¶ (Clients)",
                                "agency": "ğŸ¢ ä»£ç†å•† (Agencies)",
                                "brand": "ğŸ·ï¸ å“ç‰Œ/ç”¢å“ (Brands)",
                                "campaign": "ğŸ“¢ åŸ·è¡Œæ´»å‹• (Campaigns)",
                                "contract": "ğŸ“„ åˆç´„/æ’æœŸ (Contracts)",
                                "industry": "ğŸ­ ç”¢æ¥­é¡åˆ¥ (Industry)",
                                "sub_industry": "ğŸ­ ç”¢æ¥­å­é¡åˆ¥ (Sub-Industry)",
                                "other": "â“ å…¶ä»–"
                            }
                            
                            formatted_lines = []
                            global_idx = 1
                            
                            # æŒ‰ç…§å„ªå…ˆé †åºé¡¯ç¤ºé¡åˆ¥
                            for t in ["industry", "sub_industry", "client", "agency", "brand", "campaign", "contract", "other"]:
                                if t in grouped:
                                    formatted_lines.append(f"\n**{type_labels.get(t, t)}**")
                                    for item in grouped[t]:
                                        meta_str = ""
                                        if "metadata" in item:
                                            m = item["metadata"]
                                            meta_parts = []
                                            if "year" in m: meta_parts.append(str(m["year"]))
                                            if "status" in m: meta_parts.append(m["status"])
                                            if meta_parts:
                                                meta_str = f" _({', '.join(meta_parts)})_"
                                        
                                        formatted_lines.append(
                                            f"{global_idx}. {item['name']}{meta_str}"
                                        )
                                        # æ›´æ–°å€™é¸äººæ•¸æ“šä¸­çš„ç´¢å¼•ï¼Œä»¥ä¾¿å¾ŒçºŒåŒ¹é…
                                        item["temp_idx"] = global_idx
                                        global_idx += 1

                            llm_result = {
                                "status": "needs_confirmation",
                                "message": result.get("message"),
                                "instruction": "âš ï¸ æ‰¾åˆ°å¤šå€‹åŒ¹é…é …ï¼Œè«‹å‘ä½¿ç”¨è€…å±•ç¤ºä»¥ä¸‹åˆ†çµ„é¸é …ä¸¦è¦æ±‚å…¶é¸æ“‡ï¼š",
                                "formatted_list": "\n".join(formatted_lines),
                                "candidates_data": candidates,
                                "note": "ç•¶ä½¿ç”¨è€…å›è¦†å¾Œï¼Œå„ªå…ˆæ ¹æ“šç·¨è™Ÿæˆ–åç¨±é€²è¡ŒåŒ¹é…"
                            }
                            log_entry["details"] = f"Found {len(candidates)} candidates"

                        elif status == "rag_results":
                            # æ ¼å¼åŒ– RAG çµæœå±•ç¤º
                            rag_data = result.get("data", [])
                            formatted_rag = []
                            for idx, item in enumerate(rag_data, 1):
                                formatted_rag.append(
                                    f"{idx}. {item.get('value')} (ç›¸ä¼¼åº¦: {item.get('score', 0):.2f}) - ä¾†æº: {item.get('table')}.{item.get('source')}"
                                )

                            llm_result = {
                                "status": "rag_results",
                                "message": result.get("message"),
                                "instruction": "ğŸ” LIKE æŸ¥è©¢ç„¡çµæœï¼Œä½† RAG æ‰¾åˆ°ä»¥ä¸‹ç›¸ä¼¼å¯¦é«”ï¼š",
                                "rag_suggestions": formatted_rag,
                                "note": "è«‹å‘ä½¿ç”¨è€…ç¢ºèªæ˜¯å¦ä½¿ç”¨é€™äº›å»ºè­°ï¼Œæˆ–è¦æ±‚å…¶æä¾›æ›´æº–ç¢ºçš„åç¨±"
                            }
                            log_entry["details"] = f"Returned {len(rag_data)} RAG results"

                        else:
                            # not_found æˆ–å…¶ä»–ç‹€æ…‹
                            llm_result = result
                    else:
                        llm_result = result

                elif ("query_" in tool_name and tool_name != "query_performance_metrics") or tool_name == "execute_sql_template":
                    # Store SQL query results (from MySQL templates)
                    if isinstance(result, dict):
                        final_data = result
                        latest_query_data = result.get("data", [])  # Store complete data
                        
                        log_entry["status"] = result.get("status")
                        log_entry["row_count"] = result.get("count")
                        log_entry["generated_sql"] = result.get("generated_sql", "")

                        # Convert Decimal to float for JSON serialization
                        from decimal import Decimal
                        def _convert_decimals(obj):
                            if isinstance(obj, dict):
                                return {k: _convert_decimals(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [_convert_decimals(item) for item in obj]
                            elif isinstance(obj, Decimal):
                                return float(obj)
                            else:
                                return obj

                        sample_preview = _convert_decimals(result.get("data", [])[:5])

                        # Return simplified version to LLM with metadata
                        llm_result = {
                            "status": result.get("status"),
                            "count": result.get("count"),
                            "message": f"âœ… æŸ¥è©¢æˆåŠŸï¼å…± {result.get('count')} ç­†æ•¸æ“šå·²æº–å‚™å¥½ã€‚",
                            "instruction": "å®Œæ•´æ•¸æ“š ({0} ç­†) å·²è¼‰å…¥ï¼Œè«‹ä½¿ç”¨ pandas_processor è™•ç†".format(result.get("count")),
                            "columns": list(result.get("data", [{}])[0].keys()) if result.get("data") else [],
                            "sample_preview": sample_preview,
                            "generated_sql": result.get("generated_sql", "")  # ğŸ” é¡¯ç¤ºåŸ·è¡Œçš„ SQL
                        }
                    else:
                        llm_result = result

                elif tool_name == "query_performance_metrics":
                    # Store performance query results (from ClickHouse)
                    if isinstance(result, dict):
                        final_data = result
                        latest_query_data = result.get("data", [])
                        
                        log_entry["status"] = result.get("status")
                        log_entry["row_count"] = result.get("count")

                        from decimal import Decimal
                        def _convert_decimals(obj):
                            if isinstance(obj, dict):
                                return {k: _convert_decimals(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [_convert_decimals(item) for item in obj]
                            elif isinstance(obj, Decimal):
                                return float(obj)
                            else:
                                return obj

                        sample_preview = _convert_decimals(result.get("data", [])[:5])

                        llm_result = {
                            "status": result.get("status"),
                            "count": result.get("count"),
                            "message": f"âœ… æˆæ•ˆæŸ¥è©¢æˆåŠŸï¼å…± {result.get('count')} ç­†æ•¸æ“šå·²æº–å‚™å¥½ã€‚",
                            "instruction": "å®Œæ•´æ•¸æ“š ({0} ç­†) å·²è¼‰å…¥ï¼Œè«‹ä½¿ç”¨ pandas_processor è™•ç†".format(result.get("count")),
                            "columns": list(result.get("data", [{}])[0].keys()) if result.get("data") else [],
                            "sample_preview": sample_preview
                        }
                    else:
                        llm_result = result

                elif tool_name == "pandas_processor":
                    # Store processed data
                    if isinstance(result, dict) and result.get("status") == "success":
                        final_data = result
                        log_entry["status"] = "success"
                        log_entry["processed_count"] = result.get("count")
                        
                        # Capture the perfect markdown table
                        if "markdown" in result and result["markdown"]:
                            cached_markdown_table = result["markdown"]
                            print(f"DEBUG [DataAnalyst] Cached markdown table ({len(cached_markdown_table)} chars)")

                    llm_result = result

                else:
                    llm_result = result

                # Append log entry
                execution_logs.append(log_entry)

                # Convert result to JSON-safe format (handle Decimal, datetime, etc.)
                def convert_to_json_safe(obj):
                    """Convert non-JSON-serializable objects to JSON-safe types"""
                    from decimal import Decimal
                    from datetime import datetime, date
                    import math

                    if isinstance(obj, float):
                        if math.isnan(obj) or math.isinf(obj):
                            return None
                        return obj
                    elif isinstance(obj, dict):
                        return {k: convert_to_json_safe(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_to_json_safe(item) for item in obj]
                    elif isinstance(obj, Decimal):
                        return float(obj)
                    elif isinstance(obj, (datetime, date)):
                        return obj.isoformat()
                    else:
                        return obj

                llm_result = convert_to_json_safe(llm_result)

                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=json.dumps(llm_result, ensure_ascii=False, indent=2)
                ))

            except Exception as e:
                error_msg = f"Tool execution error: {str(e)}"
                print(f"ERROR [DataAnalyst] {error_msg}")
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=error_msg
                ))
                execution_logs.append({
                    "step": "tool_exception",
                    "timestamp": datetime.now().isoformat(),
                    "tool": tool_name,
                    "error": str(e)
                })

    # Ensure final_data is JSON safe
    if final_data:
        # Re-define or reuse convert_to_json_safe if needed, but since it was inner function, 
        # we need to define it again or rely on the fact that we process final_data if we caught it inside the loop.
        # However, final_data was assigned result BEFORE conversion in the loop:
        # if isinstance(result, dict) and result.get("status") == "success": final_data = result
        # So final_data still has Decimals.
        
        from decimal import Decimal
        from datetime import datetime, date
        import math
        
        def _final_convert(obj):
            if isinstance(obj, float):
                if math.isnan(obj) or math.isinf(obj):
                    return None
                return obj
            elif isinstance(obj, dict):
                return {k: _final_convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [_final_convert(item) for item in obj]
            elif isinstance(obj, Decimal):
                return float(obj)
            elif isinstance(obj, (datetime, date)):
                return obj.isoformat()
            else:
                return obj
                
        final_data = _final_convert(final_data)
        
        # Remove generated_sql from final output (keep it in debug_logs only)
        if isinstance(final_data, dict) and "generated_sql" in final_data:
            del final_data["generated_sql"]

    # --- Programmatic Table Append Logic ---
    # If we have a cached perfect table from pandas_processor, append it to the final response.
    # This prevents the LLM from trying to re-type it and hallucinating format errors.
    if cached_markdown_table:
        # [NEW] Anti-Duplication Filter
        # Check if LLM already hallucinated a table in markdown_response
        import re
        # Look for typical markdown table headers or separators
        table_pattern = re.compile(r'\|.*\|.*[\r\n]+\|[-:| ]+\|', re.MULTILINE)
        
        if markdown_response:
            match = table_pattern.search(markdown_response)
            if match:
                print(f"DEBUG [DataAnalyst] Detected hallucinated table in text response. Truncating...")
                # Keep text before the table
                markdown_response = markdown_response[:match.start()].strip()
                # Ensure we have a nice transition
                if not markdown_response.endswith("ï¼š") and not markdown_response.endswith(":"):
                    markdown_response += "\n\nè©³ç´°æ•¸æ“šå¦‚ä¸‹è¡¨ï¼š"

        # Append the perfect cached table
        markdown_response = (markdown_response or "è©³ç´°æ•¸æ“šå¦‚ä¸‹è¡¨ï¼š") + "\n\n" + cached_markdown_table
        print(f"DEBUG [DataAnalyst] Appended cached table to final response")

    # Return updated state
    return {
        "analyst_data": final_data,
        "resolved_entities": resolved_entities,
        "final_response": markdown_response,
        "messages": [AIMessage(content=markdown_response)],
        "debug_logs": execution_logs,  # [NEW] Return the detailed execution logs
        "next": "END"
    }
