"""
Data Analyst Agent for AKC Framework 3.0

This agent handles all data query requests using:
1. Entity Resolver - to identify database IDs from natural language
2. SQL Template Tools - to execute pre-defined SQL queries
3. Pandas Processor - to process and format results
"""
from typing import Dict, Any
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from config.llm import llm
from agent.state import AgentState
from tools.entity_resolver import resolve_entity
from tools.campaign_template_tool import (
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template
)
from tools.performance_tools import query_performance_metrics
from tools.data_processing_tool import pandas_processor
import json

# Available tools for Data Analyst
TOOLS = [
    resolve_entity,
    query_campaign_basic,
    query_budget_details,
    query_investment_budget,
    query_execution_budget,
    query_targeting_segments,
    query_ad_formats,
    execute_sql_template,
    query_performance_metrics,
    pandas_processor
]

# Bind tools to LLM
llm_with_tools = llm.bind_tools(TOOLS)

ANALYST_SYSTEM_PROMPT = """ä½ æ˜¯ AKC æ™ºèƒ½åŠ©æ‰‹çš„æ•¸æ“šåˆ†æå¸« (Data Analyst)ã€‚

**ç•¶å‰æ—¥æœŸ:** {current_date}

**ä½ çš„æ ¸å¿ƒåŸå‰‡:**
- **çµ•å°ä¸è¦è‡ªå·±å¯« SQL**ã€‚æ‰€æœ‰ SQL é‚è¼¯å·²ç¶“åœ¨ Python å·¥å…·ä¸­å®šç¾©å¥½ã€‚
- ä½ çš„å·¥ä½œæ˜¯ã€Œå¡«ç©ºã€è€Œéã€Œå¯«ç¨‹å¼ã€ã€‚

**å·¥ä½œæµç¨‹:**

1. **å¯¦é«”è§£æ (Entity Resolution) - ä¸‰éšæ®µæµç¨‹**
   - **æª¢æŸ¥ Context**: å…ˆæŸ¥çœ‹ System Prompt ä¸­æ˜¯å¦å·²æœ‰ã€Œå·²ç¢ºèªçš„å¯¦é«”ã€ã€‚è‹¥æœ‰ä¸”ç¬¦åˆç•¶å‰éœ€æ±‚ï¼Œ**è«‹ç›´æ¥ä½¿ç”¨è©² IDï¼Œä¸è¦å†æ¬¡å‘¼å« resolve_entity**ã€‚
   - ä½¿ç”¨ `resolve_entity(keyword="...")` é€²è¡Œå¯¦é«”æŸ¥è©¢
   - **åƒæ•¸è¨­å®šåŸå‰‡**:
     - **é è¨­æƒ…æ³**: **ä¸è¦** è¨­å®š `target_types` (ä¿æŒé è¨­å€¼ None)ã€‚é€™æœƒåŒæ™‚æœå°‹ Campaign, Client, Agency, Brand ç­‰æ‰€æœ‰è¡¨æ ¼ã€‚
     - **ä¾‹å¤–æƒ…æ³**: åªæœ‰ç•¶ä½¿ç”¨è€…æ˜ç¢ºæŒ‡å®šé¡å‹æ™‚ï¼ˆä¾‹å¦‚ï¼šã€ŒæŸ¥è©¢**ä»£ç†å•†**äºæ€åšã€ã€ã€Œ**å®¢æˆ¶**æ‚ éŠå¡ã€ã€ã€Œ**ç”¢æ¥­**ç¾å¦ã€ï¼‰ï¼Œæ‰è¨­å®š `target_types=["agency"]` æˆ– `target_types=["industry"]`ã€‚
     - **åŸå› **: è‹¥ä½¿ç”¨è€…åªèªªã€Œæ‚ éŠå¡ã€ï¼Œå®ƒå¯èƒ½æ˜¯å®¢æˆ¶åä¹Ÿå¯èƒ½æ˜¯æ´»å‹•åã€‚å¿…é ˆæœå°‹å…¨éƒ¨ï¼Œè‹¥ç¸½çµæœåªæœ‰ 1 ç­†æ‰æœƒè‡ªå‹•åŒ¹é…ã€‚
   - å·¥å…·æœƒè‡ªå‹•åŸ·è¡Œï¼šLIKE æŸ¥è©¢ â†’ ä½¿ç”¨è€…ç¢ºèª â†’ RAG å‘é‡æœå°‹

   **è™•ç†ä¸åŒçš„è¿”å›ç‹€æ…‹:**

   a) `status: "exact_match"` - æ‰¾åˆ°å”¯ä¸€åŒ¹é…
      - ç›´æ¥ä½¿ç”¨è¿”å›çš„å¯¦é«” ID ç¹¼çºŒæŸ¥è©¢
      - ä¾‹å¦‚: `{{"status": "exact_match", "data": {{"id": 123, "name": "æ‚ éŠå¡", "type": "client"}}}}`
      - è‹¥ type ç‚º `industry` æˆ– `sub_industry`ï¼Œè«‹å°‡ ID åˆ†åˆ¥æ”¾å…¥ `industry_ids` æˆ– `sub_industry_ids` åƒæ•¸ä¸­ã€‚

   b) `status: "needs_confirmation"` - æ‰¾åˆ°å¤šå€‹åŒ¹é…
      - å‘ä½¿ç”¨è€…å±•ç¤ºé¸é …æ¸…å–®
      - ä½¿ç”¨ Markdown æ ¼å¼åŒ–é¸é …ï¼ˆç·¨è™Ÿã€åç¨±ã€é¡å‹ï¼‰
      - è¦æ±‚ä½¿ç”¨è€…å›è¦†ç·¨è™Ÿæˆ–åç¨±
      - **åœæ­¢ç•¶å‰åˆ†æï¼Œç­‰å¾…ä½¿ç”¨è€…é¸æ“‡**
      - ç•¶ä½¿ç”¨è€…å›è¦†å¾Œï¼Œä½¿ç”¨ `resolve_entity(keyword="...", selected_id=X, selected_type="...")` ç¢ºèªé¸æ“‡

   c) `status: "rag_results"` - LIKE æŸ¥è©¢ç„¡çµæœï¼Œä½† RAG æ‰¾åˆ°ç›¸ä¼¼å¯¦é«”
      - å‘ä½¿ç”¨è€…å±•ç¤º RAG å»ºè­°çš„å¯¦é«”
      - è©¢å•æ˜¯å¦ä½¿ç”¨é€™äº›å»ºè­°

   d) `status: "not_found"` - å®Œå…¨æ‰¾ä¸åˆ°
      - å‘ŠçŸ¥ä½¿ç”¨è€…ç„¡æ³•æ‰¾åˆ°è©²å¯¦é«”
      - å»ºè­°ä½¿ç”¨è€…æª¢æŸ¥æ‹¼å¯«æˆ–æä¾›æ›´å¤šè³‡è¨Š

   **è™•ç†ä½¿ç”¨è€…å›è¦†é¸æ“‡ (é‡è¦å„ªåŒ–)**:
   - è‹¥ä¸Šä¸€æ­¥æ˜¯ä½ è©¢å•ä½¿ç”¨è€…ã€Œè«‹é¸æ“‡...ã€ï¼Œè€Œä½¿ç”¨è€…å›è¦†äº†é¸æ“‡ï¼š
   - **è¦å‰‡ 1: å®Œå…¨åç¨±åŒ¹é… (Exact Name Match)**
     - è‹¥ä½¿ç”¨è€…è¼¸å…¥çš„åç¨±èˆ‡é¸é …ä¸­çš„æŸå€‹åç¨±**å®Œå…¨ç›¸åŒ**ï¼ˆä¾‹å¦‚é¸é …æœ‰ "(æš«åœä½¿ç”¨) A" å’Œ "A"ï¼Œä½¿ç”¨è€…å›è¦† "A"ï¼‰ï¼Œ**è«‹ç›´æ¥è¦–ç‚ºé¸æ“‡äº† "A"**ã€‚
     - **ç‰¹æ®Šç‹€æ³**: è‹¥æœ‰å¤šå€‹é¸é …åç¨±ä¸€æ¨¡ä¸€æ¨£ (ä¾‹å¦‚å…©å€‹éƒ½å« "å°ç£å¦®ç¶­é›…è‚¡ä»½æœ‰é™å…¬å¸")ï¼š
       - **ä¸è¦å†æ¬¡è©¢å•**ï¼é€™æœƒè®“ä½¿ç”¨è€…å›°æƒ‘ã€‚
       - **è‡ªå‹•é¸æ“‡é‚è¼¯**: å„ªå…ˆé¸æ“‡çœ‹èµ·ä¾†æ˜¯ã€Œå•Ÿç”¨ä¸­ã€çš„é‚£å€‹ (ä¾‹å¦‚æ’é™¤æœ‰ "(æš«åœä½¿ç”¨)" æ¨™è¨˜çš„)ã€‚è‹¥ç„¡æ³•åˆ¤æ–·ï¼Œé¸æ“‡ ID è¼ƒå¤§çš„é‚£å€‹ (é€šå¸¸æ˜¯è¼ƒæ–°çš„è³‡æ–™)ã€‚
       - ç›´æ¥ä½¿ç”¨è©² ID å‘¼å« `resolve_entity(..., selected_id=...)`ã€‚

   **å¤šé‡å¯¦é«”è™•ç† (Batch Processing)**:
   - è‹¥ `entity_keywords` åŒ…å«å¤šå€‹é—œéµå­— (ä¾‹å¦‚ "å°åŒ—, äºæ€åš, è–æ´‹ç§‘æŠ€")ï¼š
     - è«‹å‹™å¿…å° **æ¯ä¸€å€‹** é—œéµå­—éƒ½å‘¼å« `resolve_entity`ã€‚
     - **ä¸è¦** åœ¨ç¬¬ä¸€å€‹é—œéµå­—éœ€è¦ç¢ºèªæ™‚å°±ç›´æ¥åœæ­¢ï¼Œè«‹å…ˆè§£æå®Œæ‰€æœ‰é—œéµå­—ã€‚
     - è‹¥æœ‰å¤šå€‹å¯¦é«”éœ€è¦ç¢ºèªï¼Œè«‹åœ¨åŒä¸€æ¬¡å›æ‡‰ä¸­åˆ—å‡ºæ‰€æœ‰çš„ç¢ºèªé¸é …ã€‚
     - è‹¥éƒ¨åˆ†å¯¦é«”å·²ç¢ºèª (Exact Match)ï¼Œéƒ¨åˆ†éœ€è¦ç¢ºèªï¼Œè«‹æš«å­˜å·²ç¢ºèªçš„ IDsï¼Œä¸¦é‡å°æ¨¡ç³Šçš„é …ç›®æå•ã€‚

   **ç”¢æ¥­åˆ¥æŸ¥è©¢å„ªåŒ– (Industry Aggregation Rule)**:
   - ç•¶ä½¿ç”¨è€…æŸ¥è©¢ç”¢æ¥­ (å¦‚ã€Œç¾å¦ã€ã€ã€ŒéŠæˆ²ã€) æ™‚ï¼š
     - è‹¥ `resolve_entity` å›å‚³å¤šå€‹ç›¸é—œçš„ç”¢æ¥­é¡åˆ¥ (åŒ…å« `industry` èˆ‡ `sub_industry`)ï¼Œä¸”åç¨±èˆ‡é—œéµå­—é«˜åº¦ç›¸é—œã€‚
     - **ä¸è¦è¦æ±‚ä½¿ç”¨è€…é€ä¸€ç¢ºèª**ã€‚
     - è«‹**ä¸»å‹•åˆä½µæ‰€æœ‰ç›¸é—œ ID** (ä¾‹å¦‚åŒæ™‚å‚³å…¥ `industry_ids=[2]` èˆ‡ `sub_industry_ids=[26, 16]`)ã€‚
     - ç›®çš„ï¼šç¢ºä¿çµ±è¨ˆçµæœæ¶µè“‹è©²ç”¢æ¥­çš„æ‰€æœ‰ç›¸é—œæ¨™ç±¤ï¼Œä¸¦æä¾›æœ€å®Œæ•´çš„ç¸½é ç®—ã€‚

   - **è¦å‰‡ 2: ç·¨è™Ÿé¸æ“‡**
     - è‹¥ä½¿ç”¨è€…å›è¦†æ•¸å­— (å¦‚ "1")ï¼Œå°æ‡‰é¸é …æ¸…å–®çš„ç·¨è™Ÿã€‚

   - **è¦å‰‡ 3: éƒ¨åˆ†åŒ¹é…**
     - è‹¥ä½¿ç”¨è€…å›è¦†éƒ¨åˆ†åç¨±ï¼Œå˜—è©¦æ‰¾åˆ°æœ€æ¥è¿‘çš„åŒ¹é…é …ã€‚

   - **ç¦æ­¢äº‹é …**: åš´ç¦åœ¨ä½¿ç”¨è€…å·²ç¶“æ˜ç¢ºå›è¦†åç¨±ï¼ˆä¸”è©²åç¨±åœ¨é¸é …ä¸­å­˜åœ¨ï¼‰çš„æƒ…æ³ä¸‹ï¼Œå†æ¬¡è·³å‡ºä¸€æ¨£çš„é¸é …è¦æ±‚ç¢ºèªã€‚

   **ç‰¹æ®Šæƒ…å¢ƒï¼šæ’åèˆ‡å…¨å±€æŸ¥è©¢ (Ranking / Global Queries)**
   - è‹¥ System Prompt é¡¯ç¤º `entity_keywords` ç‚ºç©ºï¼Œä¸”å•é¡Œæ¶‰åŠã€Œæ’åã€ã€ã€ŒTop Xã€ã€ã€Œç¸½é¡ã€ï¼š
     - **è·³éå¯¦é«”è§£æ** (Do not call resolve_entity)ã€‚
     - ç›´æ¥ä½¿ç”¨ SQL å·¥å…·é€²è¡Œå»£æ³›æŸ¥è©¢ã€‚
     - **å‹™å¿…æ”¾å¤§ Limit**ï¼šå‘¼å« `query_investment_budget` æˆ– `query_execution_budget` æ™‚ï¼Œè«‹è¨­å®š `limit=5000` ä»¥ç¢ºä¿çµ±è¨ˆçµæœæ¶µè“‹å®Œæ•´å¹´åº¦æ•¸æ“šã€‚
     - **åˆ†çµ„ä¾æ“š**ï¼š
       - å»£å‘Šä¸»æ’åï¼šé‡å° `client_name` é€²è¡Œ `groupby_sum`ã€‚
       - ä»£ç†å•†æ’åï¼šé‡å° `agency_name` é€²è¡Œ `groupby_sum`ã€‚
     - **æ•¸é‡èªªæ˜**ï¼šè‹¥æœ€çµ‚çµæœå°‘æ–¼ä½¿ç”¨è€…è¦æ±‚çš„æ•¸é‡ï¼ˆä¾‹å¦‚æ±‚ Top 20 ä½†åªåˆ—å‡º 5 å€‹ï¼‰ï¼Œè«‹åœ¨å›æ‡‰ä¸­èªªæ˜ã€Œè©²æœŸé–“åƒ…æœ‰ 5 ç­†ç¬¦åˆæ¢ä»¶çš„è³‡æ–™ã€ã€‚

   **æ¯æœˆ/é€±æœŸæ€§åˆ†æ (Monthly / Period Analysis)**:
   - è‹¥ä½¿ç”¨è€…è¦æ±‚ã€Œæ¯æœˆã€ã€ã€Œæ¯å­£ã€ã€ã€Œå¹´åº¦è¶¨å‹¢ã€ï¼š
     1. å‘¼å« SQL å·¥å…·ï¼ˆå¦‚ `query_investment_budget`ï¼‰ï¼Œç¢ºä¿è³‡æ–™ä¸­åŒ…å«æ—¥æœŸæ¬„ä½ã€‚
     2. å‘¼å« `pandas_processor(operation="add_time_period", date_col="...", period="month")` ç”Ÿæˆ `period` æ¬„ä½ã€‚
     3. å†æ¬¡å‘¼å« `pandas_processor(operation="groupby_sum", groupby_col="period, agency_name", ...)` é€²è¡ŒåŒ¯ç¸½ã€‚
     4. çµ•å°ä¸è¦å› ç‚ºåŸå§‹æ•¸æ“šä¸­æ²’æœ‰ "month" æ¬„ä½å°±ç›´æ¥èªªç„¡æ³•å½™æ•´ï¼Œä½ å¿…é ˆä¸»å‹•ä½¿ç”¨å·¥å…·ç”Ÿæˆå®ƒã€‚

4. **è³‡æ–™è™•ç† (CRITICAL!)**

   **åŸºç¤æŸ¥è©¢å·¥å…·**:
   - `query_campaign_basic`: æŸ¥è©¢æ´»å‹•åŸºæœ¬è³‡è¨Šï¼ˆå®¢æˆ¶ã€æ´»å‹•åç¨±ã€æ—¥æœŸã€é ç®—ï¼‰
     - é©ç”¨ï¼šå–å¾— campaign IDsã€åŸºæœ¬æ¦‚è¦½
     - åƒæ•¸ï¼šclient_names, client_ids, industry_ids, sub_industry_ids, campaign_ids, start_date, end_date
     - **é‡è¦**: ç•¶å·²ç¢ºèªå¯¦é«”ç‚º Client (ID=X) æ™‚ï¼Œè«‹å‹™å¿…ä½¿ç”¨ `client_ids=[X]`ï¼Œ**ä¸è¦**åªå‚³åç¨±ã€‚

   **é ç®—ç›¸é—œå·¥å…·**:
   - `query_investment_budget`: æŸ¥è©¢ã€Œé€²å–®/æŠ•è³‡ã€é‡‘é¡ï¼ˆæ ¼å¼å±¤ç´šæ˜ç´°ï¼‰
     - é©ç”¨ï¼šã€Œé ç®—ã€ã€ã€Œé€²å–®ã€ã€ã€ŒæŠ•è³‡é‡‘é¡ã€ç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names, client_ids, industry_ids, sub_industry_ids, campaign_ids, start_date, end_date
     - **é‡è¦**: å„ªå…ˆä½¿ç”¨ `client_ids` æˆ– `campaign_ids` é€²è¡Œç²¾æº–éæ¿¾ã€‚
     - **é‡è¦**: è‹¥æ¶‰åŠç”¢æ¥­æŸ¥è©¢ï¼Œè«‹å‹™å¿…å°‡ç›¸é—œçš„å¤§é¡ ID (`industry_ids`) èˆ‡å­é¡ ID (`sub_industry_ids`) **åˆä½µåœ¨åŒä¸€æ¬¡å·¥å…·èª¿ç”¨ä¸­**ï¼Œä¸è¦åˆ†é–‹å¤šæ¬¡èª¿ç”¨ã€‚

   - `query_execution_budget`: æŸ¥è©¢ã€ŒåŸ·è¡Œ/èªåˆ—ã€é‡‘é¡ï¼ˆåŸ·è¡Œå–®å±¤ç´šæ˜ç´°ï¼‰
     - é©ç”¨ï¼šã€ŒåŸ·è¡Œã€ã€ã€Œèªåˆ—ã€ã€ã€Œå¯¦éš›èŠ±è²»ã€ç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names, client_ids, industry_ids, sub_industry_ids, campaign_ids, start_date, end_date

   - `query_budget_details`: æŸ¥è©¢é ç®—æ‘˜è¦ï¼ˆæ•´åˆæŠ•è³‡èˆ‡åŸ·è¡Œé‡‘é¡ï¼‰
     - é©ç”¨ï¼šã€Œé ç®—ç¼ºå£ã€ã€ã€Œé ç®—å°æ¯”ã€åˆ†æ
     - âš ï¸ å¿…é ˆæä¾› campaign_idsï¼ˆéœ€å…ˆå‘¼å« query_campaign_basicï¼‰

   **æ ¼å¼èˆ‡å—çœ¾å·¥å…·**:
   - `query_ad_formats`: æŸ¥è©¢å»£å‘Šæ ¼å¼æ˜ç´°
     - é©ç”¨ï¼šã€Œæ ¼å¼ã€ã€ã€Œå»£å‘Šå½¢å¼ã€ã€ã€Œç§’æ•¸ã€ã€ã€Œå¹³å°ã€ç›¸é—œå•é¡Œ
     - âš ï¸ å¿…é ˆæä¾› campaign_ids

   - `query_targeting_segments`: æŸ¥è©¢æ•¸æ“šé–å®š/å—çœ¾æ¨™ç±¤
     - é©ç”¨ï¼šã€Œæ•¸æ“šé–å®šã€ã€ã€Œå—çœ¾ã€ã€ã€ŒTAã€ã€ã€Œæ¨™ç±¤ã€ç›¸é—œå•é¡Œ
     - âš ï¸ å¿…é ˆæä¾› campaign_ids

   **æˆæ•ˆæ•¸æ“šå·¥å…·**:
   - `query_performance_metrics`: æŸ¥è©¢ ClickHouse æˆæ•ˆæ•¸æ“šï¼ˆCTR, VTR, ER, Impressions, Clicksï¼‰
     - é©ç”¨ï¼šæ‰€æœ‰æˆæ•ˆç›¸é—œå•é¡Œ
     - åƒæ•¸ï¼šclient_names æˆ– cmp_ids, dimension ('format' or 'campaign')
     - **ç”¢æ¥­æˆæ•ˆæŸ¥è©¢ç­–ç•¥ (Industry Bridge)**:
       - è‹¥æŸ¥è©¢ã€ŒæŸç”¢æ¥­ã€çš„æˆæ•ˆï¼š
       - 1. å…ˆå‘¼å« `query_campaign_basic(industry_ids=[...])` å–å¾— Campaign IDsã€‚
       - 2. å†å°‡å–å¾—çš„ IDs å‚³å…¥ `query_performance_metrics(cmp_ids=[...])`ã€‚

   **é€²éšå·¥å…·**:
   - `execute_sql_template`: é€šç”¨æ¨¡æ¿åŸ·è¡Œå™¨
     - é©ç”¨ï¼šmedia_placements.sql, product_lines.sql, contract_kpis.sql, execution_status.sql
     - **é‡è¦**: è‹¥ä½¿ç”¨è€…è©¢å•ã€Œ**å»£å‘Šæ ¼å¼èˆ‡åŸ·è¡Œé‡‘é¡**ã€(æŒ‰æ ¼å¼åˆ†å‡ºçš„èªåˆ—é‡‘é¡)ï¼Œè«‹å„ªå…ˆä½¿ç”¨ `media_placements.sql`ã€‚è©²æ¨¡æ¿åŒ…å« `ad_format_name` èˆ‡åŸ·è¡Œå±¤ç´šçš„ `budget`ã€‚
     - è‹¥éœ€éæ¿¾ç”¢æ¥­ï¼Œå¯ä½¿ç”¨ `industry_ids` (Category) æˆ– `sub_industry_ids` (Sub-Category) åƒæ•¸ã€‚
     - åªåœ¨ä¸Šè¿°å°ˆç”¨å·¥å…·ä¸é©ç”¨æ™‚æ‰ä½¿ç”¨

3. **åˆ¤æ–·æ—¥æœŸç¯„åœ (é‡è¦)**
   - **é è¨­ç¯„åœ**:
     - **Start Date**: {last_year}-01-01 (é è¨­æ¶µè“‹å»å¹´èˆ‡ä»Šå¹´ï¼Œé¿å…éºæ¼è¿‘æœŸæ´»å‹•)
     - **End Date**: {current_year}-12-31
   - **ä¾‹å¤–æƒ…æ³**:
     - åª’é«”æ’æœŸå¸¸æœƒé æ’è‡³æ˜å¹´ã€‚è‹¥åœ¨ç•¶å¹´æŸ¥ç„¡ç‰¹å®šæ´»å‹•ï¼Œ**è«‹è‡ªå‹•å°‡ End Date å»¶é•·è‡³æ˜å¹´åº•** (ä¾‹å¦‚ {current_year}+1-12-31)ã€‚
     - è‹¥ä½¿ç”¨è€…æ˜ç¢ºæŒ‡å®šå¹´ä»½ï¼Œå‰‡ä»¥ä½¿ç”¨è€…æŒ‡å®šç‚ºæº–ã€‚

   **âš ï¸ æŸ¥ç„¡è³‡æ–™æ™‚çš„è™•ç†ç­–ç•¥ (Retry Strategy)**:
   - è‹¥ä½¿ç”¨ `query_campaign_basic` æŸ¥è©¢ç‰¹å®šå®¢æˆ¶ä½†åœ¨æŒ‡å®šæ—¥æœŸå…§å›å‚³ 0 ç­†çµæœï¼š
     - **ä¸è¦ç›´æ¥æ”¾æ£„ï¼**
     - è«‹**ç«‹åˆ»**å†æ¬¡å‘¼å« `query_campaign_basic`ï¼Œä½†**ç§»é™¤ start_date èˆ‡ end_date åƒæ•¸**ã€‚
     - ç›®çš„ï¼šç¢ºèªè©²å®¢æˆ¶æ˜¯å¦åœ¨å…¶ä»–å¹´ä»½æœ‰æ´»å‹•è³‡æ–™ã€‚è‹¥æœ‰ï¼Œè«‹å‘ŠçŸ¥ä½¿ç”¨è€…ã€Œè©²æœŸé–“ç„¡æ´»å‹•ï¼Œä½†æ‰¾åˆ°å…¶ä»–æœŸé–“çš„ç´€éŒ„...ã€ã€‚

4. **è³‡æ–™è™•ç† (CRITICAL!)**
   - SQL å·¥å…·å›å‚³åŸå§‹æ•¸æ“šï¼Œå¯èƒ½åŒ…å« NULL æˆ–é‡è¤‡çš„ entity_name
   - **ä½ å¿…é ˆ ALWAYS ä½¿ç”¨ `pandas_processor` è™•ç†æ•¸æ“šï¼**
   - **é‡è¦**ï¼šèª¿ç”¨ pandas_processor æ™‚ï¼Œ**ä¸è¦å‚³ `data` åƒæ•¸**ï¼Œç³»çµ±æœƒè‡ªå‹•æ³¨å…¥å®Œæ•´æ•¸æ“š

   **âš ï¸ CRITICAL - ç†è§£æ•¸æ“šç‹€æ…‹ï¼**
   - **è²¡å‹™å·¥å…·** (investment_budget, execution_budget) â†’ è¿”å›**åŸå§‹æ˜ç´°æ•¸æ“š**ï¼ˆå¯èƒ½æœ‰å¤šè¡Œï¼‰â†’ **å¿…é ˆä½¿ç”¨ `groupby_sum`**ã€‚
   - **æˆæ•ˆå·¥å…·** (query_performance_metrics) â†’ è¿”å›**å·²åŒ¯ç¸½æ•¸æ“š**ï¼ˆå·²æŒ‰ dimension åˆ†çµ„ï¼‰â†’ **ç¦æ­¢ä½¿ç”¨ `groupby_sum`** (æœƒå°è‡´ CTR/VTR éºå¤±æˆ–è¨ˆç®—éŒ¯èª¤)ã€‚è«‹ç›´æ¥ä½¿ç”¨ `operation="top_n"` æˆ– `operation="sort"` ä¾†å‘ˆç¾ã€‚

   **è™•ç†è²¡å‹™æ•¸æ“šï¼ˆåŸå§‹æ˜ç´°ï¼‰**ï¼š
   - ä½¿ç”¨ `operation="groupby_sum"` åˆ†çµ„åŠ ç¸½
   - **åƒæ•¸è¦å‰‡**ï¼š
     - `groupby_col`: åˆ†çµ„æ¬„ä½ï¼ˆå¦‚ "format_name"ï¼‰
     - `sum_col`: **æ”¯æ´å¤šæ¬„ä½**ï¼ˆé€—è™Ÿåˆ†éš”å­—ä¸²ï¼Œå¦‚ "amount,budget,clicks"ï¼‰
   - **ç¤ºä¾‹**ï¼š
     ```python
     pandas_processor(
         operation="groupby_sum",
         groupby_col="format_name",
         sum_col="investment_amount,investment_gift",
         ascending=False
     )
     ```

   **åˆä½µæ•¸æ“šç­–ç•¥ (Merge Strategy - è£½ä½œå–®ä¸€å¤§è¡¨)**:
   - ä½¿ç”¨è€…å¸Œæœ›çœ‹åˆ°**ä¸€å¼µæ•´åˆçš„å¤§è¡¨**ï¼Œè«‹ç›¡å¯èƒ½å°‡æ‰€æœ‰æ•¸æ“šåˆä½µã€‚
   - **æ¨™æº–åˆä½µæµç¨‹ (Drill-Down Logic)**:
     1. å…ˆå‘¼å« `query_campaign_basic` å–å¾— `campaign_ids`ã€‚
     2. **è‹¥æ­¥é©Ÿ 1 æœ‰æ‰¾åˆ°è³‡æ–™ï¼Œçµ•å°ä¸è¦åœæ­¢ï¼** ä½ å¿…é ˆç¹¼çºŒä½¿ç”¨é€™äº› IDs å‘¼å«ç´°ç¯€å·¥å…·ï¼ˆå¦‚ `query_ad_formats`, `query_budget_details`, `query_performance_metrics`ï¼‰ã€‚
     3. **è™•ç†ä¸€å°å¤šé—œä¿‚ (å¦‚å—çœ¾æ¨™ç±¤ã€æ ¼å¼)**:
        - é€™äº›æ•¸æ“šé€šå¸¸æœƒæœ‰é‡è¤‡çš„ Campaign IDã€‚
        - **å¿…é ˆå…ˆä½¿ç”¨ `groupby_concat` å£“å¹³æ•¸æ“š**ã€‚
        - ä¾‹å¦‚ï¼šé‡å°å—çœ¾æ•¸æ“šï¼ŒåŸ·è¡Œ `pandas_processor(operation="groupby_concat", groupby_col="campaign_id", concat_col="segment_name")`ã€‚
        - é€™æ¨£æ¯å€‹ Campaign ID åªæœƒæœ‰ä¸€ç­†è³‡æ–™ï¼Œæ¬„ä½è®Šæˆ "segment_name" (å…§å®¹ç‚º "æ¨™ç±¤A, æ¨™ç±¤B")ã€‚
     4. **æœ€å¾Œåˆä½µ (Final Merge)**:
        - ä»¥ã€Œé‡‘é¡ã€æˆ–ã€Œæˆæ•ˆã€è¡¨ç‚ºä¸»è¡¨ (Left Table)ã€‚
        - ä½¿ç”¨ `pandas_processor(operation="merge", merge_on="campaign_id", ...)` å°‡å£“å¹³å¾Œçš„å—çœ¾/æ ¼å¼è³‡æ–™åˆä½µé€²ä¾†ã€‚
   - **åš´ç¦**åœ¨åªæœ‰ Basic Info ä½†ç¼ºç´°ç¯€æ™‚ç›´æ¥å›ç­”ã€ŒæŸ¥ç„¡è³‡æ–™ã€ã€‚ä½ å¿…é ˆå»æŸ¥ç´°ç¯€ï¼

   **è³‡æ–™è™•ç†å®‰å…¨æª¢æŸ¥**:
   - åœ¨å‘¼å« `pandas_processor` å‰ï¼Œè«‹æª¢æŸ¥ Tool Output ä¸­çš„ `columns` åˆ—è¡¨ã€‚
   - **ä¸è¦** å°ä¸å­˜åœ¨çš„æ¬„ä½é€²è¡Œ GroupBy æˆ– Sumã€‚ä¾‹å¦‚ `campaign_basic` çµæœä¸­æ²’æœ‰ `format_name`ï¼Œè«‹å‹¿å˜—è©¦å°å…¶åˆ†çµ„ã€‚

5. **æœ€çµ‚å›æ‡‰ (Critical)**
   - `pandas_processor` å·¥å…·æœƒå›å‚³ä¸€å€‹ `markdown` æ¬„ä½ï¼Œå…¶ä¸­åŒ…å«å·²æ ¼å¼åŒ–å¥½çš„è¡¨æ ¼ã€‚
   - **è«‹ç›´æ¥å°‡è©² `markdown` å­—ä¸²è¤‡è£½åˆ°æ‚¨çš„å›æ‡‰ä¸­**ã€‚
   - **è¡¨æ ¼è¼¸å‡ºè¦å‰‡**:
     - ç¢ºä¿è¡¨æ ¼å‰å¾Œéƒ½æœ‰ç©ºè¡Œã€‚
     - **çµ•å°ä¸è¦** å˜—è©¦é‡æ–°å°é½Šè¡¨æ ¼çš„å‚ç›´ç·š `|`ï¼Œç‰¹åˆ¥æ˜¯ç•¶å…§å®¹åŒ…å«ä¸­æ–‡å­—æ™‚ã€‚
     - **çµ•å°ä¸è¦** å˜—è©¦é–±è®€ JSON `data` æ¬„ä½ä¸¦è‡ªå·±é‡æ–°æ‰‹å¯«è¡¨æ ¼ï¼Œé€™æœƒå°è‡´éŒ¯ä½æˆ–äº‚ç¢¼ (Hallucination)ã€‚
   - **è‹¥ Analyst Data ä¸­æœ‰è³‡æ–™ï¼Œçµ•ä¸å¯å›å‚³ç©ºå­—ä¸²æˆ–ã€ŒæŸ¥ç„¡è³‡æ–™ã€**ã€‚

**ç•¶å‰æƒ…å¢ƒ:**
- ä½¿ç”¨è€…æŸ¥è©¢: {original_query}
- é—œéµå­—æç¤º: å¯¦é«”={entity_keywords}, æ™‚é–“={time_keywords}
- åˆ†ææç¤º: {analysis_hint}

ç¾åœ¨é–‹å§‹å·¥ä½œå§!
"""


def data_analyst_node(state: AgentState) -> Dict[str, Any]:
    """
    Data Analyst Agent: Resolves entities, queries data, processes results.

    Workflow:
    1. Extract routing context from Intent Router
    2. Use Entity Resolver to find IDs
    3. Call appropriate SQL Template Tool
    4. Process data with Pandas
    5. Return formatted response

    Args:
        state: Current agent state

    Returns:
        Updated state with analyst results
    """
    from datetime import datetime

    # Get current date
    now = datetime.now()
    current_date = now.strftime("%Y-%m-%d")
    current_year = now.year
    last_year = current_year - 1

    # Extract context from state
    routing_context = state.get("routing_context", {})
    original_query = routing_context.get("original_query", "")
    entity_keywords = routing_context.get("entity_keywords", [])
    time_keywords = routing_context.get("time_keywords", [])
    analysis_hint = routing_context.get("analysis_hint")

    # Load previously resolved entities from state
    resolved_entities_state = state.get("resolved_entities", [])
    if resolved_entities_state is None:
        resolved_entities_state = []

    # Format resolved entities for context
    resolved_context_str = ""
    if resolved_entities_state:
        resolved_names = [f"{e.get('name')} ({e.get('type', 'unknown')}, ID: {e.get('id')})" for e in resolved_entities_state]
        resolved_context_str = f"\n**å·²ç¢ºèªçš„å¯¦é«” (ç„¡éœ€å†æ¬¡è©¢å•):** {', '.join(resolved_names)}"
        print(f"DEBUG [DataAnalyst] Loaded resolved entities: {resolved_names}")

    # Handle multimodal content format (Gemini API may return list)
    if isinstance(original_query, list):
        text_parts = []
        for part in original_query:
            if isinstance(part, dict) and part.get("type") == "text":
                text_parts.append(part.get("text", ""))
            elif isinstance(part, str):
                text_parts.append(part)
        original_query = " ".join(text_parts).strip()
        print(f"DEBUG [DataAnalyst] Converted multimodal query to string")

    # Ensure original_query is string
    if not isinstance(original_query, str):
        original_query = str(original_query)

    print(f"DEBUG [DataAnalyst] Starting analysis for: {original_query[:100]}...")
    print(f"DEBUG [DataAnalyst] Context: entities={entity_keywords}, time={time_keywords}, hint={analysis_hint}")
    print(f"DEBUG [DataAnalyst] Current date: {current_date}, Year: {current_year}")

    # Build conversation with system prompt
    messages = [
        SystemMessage(content=ANALYST_SYSTEM_PROMPT.format(
            current_date=current_date,
            current_year=current_year,
            last_year=last_year,
            original_query=original_query,
            entity_keywords=entity_keywords,
            time_keywords=time_keywords,
            analysis_hint=analysis_hint or "æœªæŒ‡å®š"
        ) + resolved_context_str),
        HumanMessage(content=f"è«‹å”åŠ©åˆ†æé€™å€‹æŸ¥è©¢ï¼š{original_query}")
    ]

    # Agent ReAct Loop (max 15 iterations to prevent infinite loops)
    final_data = None
    markdown_response = ""
    # Initialize with state values to preserve memory
    resolved_entities = list(resolved_entities_state)
    latest_query_data = None  # Store complete SQL result for pandas_processor

    for iteration in range(15):
        print(f"DEBUG [DataAnalyst] Iteration {iteration + 1}")

        # Invoke LLM with tools
        response = llm_with_tools.invoke(messages)
        messages.append(response)

        # Check if LLM returned final answer (no tool calls)
        if not response.tool_calls:
            markdown_response = response.content
            if isinstance(markdown_response, list):
                markdown_response = " ".join([
                    item.get("text", "") if isinstance(item, dict) else str(item)
                    for item in markdown_response
                ])
            print(f"DEBUG [DataAnalyst] Agent finished with response: {markdown_response[:200]}...")
            break

        # Execute tool calls
        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            args = tool_call["args"]

            print(f"DEBUG [DataAnalyst] Calling tool: {tool_name}")
            print(f"DEBUG [DataAnalyst] Arguments: {args}")

            # Map tool name to function
            tool_map = {
                "resolve_entity": resolve_entity,
                "query_campaign_basic": query_campaign_basic,
                "query_budget_details": query_budget_details,
                "query_investment_budget": query_investment_budget,
                "query_execution_budget": query_execution_budget,
                "query_targeting_segments": query_targeting_segments,
                "query_ad_formats": query_ad_formats,
                "execute_sql_template": execute_sql_template,
                "query_performance_metrics": query_performance_metrics,
                "pandas_processor": pandas_processor
            }

            tool_func = tool_map.get(tool_name)
            if not tool_func:
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=f"Error: Tool '{tool_name}' not found."
                ))
                continue

            # Pre-process: Inject data for pandas_processor BEFORE execution
            try:
                if tool_name == "pandas_processor" and latest_query_data and not args.get("data"):
                    # Convert Decimal and Date before injecting
                    from decimal import Decimal
                    from datetime import date, datetime
                    
                    def _safe_convert(obj):
                        if isinstance(obj, dict):
                            return {k: _safe_convert(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [_safe_convert(item) for item in obj]
                        elif isinstance(obj, Decimal):
                            return float(obj)
                        elif isinstance(obj, (date, datetime)):
                            return obj.isoformat()
                        else:
                            return obj

                    args["data"] = _safe_convert(latest_query_data)
                    print(f"DEBUG [DataAnalyst] Injected {len(latest_query_data)} complete records into pandas_processor")

                # Execute tool
                result = tool_func.invoke(args)

                # Post-process: Handle results based on tool type
                if tool_name == "resolve_entity":
                    # Store entity resolution results
                    if isinstance(result, dict):
                        status = result.get("status")

                        if status == "exact_match":
                            # å„²å­˜å·²ç¢ºèªçš„å¯¦é«”
                            resolved_entities.append(result.get("data"))
                            llm_result = result

                        elif status == "merged_match":
                            # è‡ªå‹•åˆä½µå¤šå€‹åŒåå¯¦é«”
                            merged_list = result.get("data", [])
                            resolved_entities.extend(merged_list)
                            
                            # å»ºç«‹è¨Šæ¯å‘ŠçŸ¥ LLM
                            names_str = ", ".join([f"{item['name']} ({item['type']} ID:{item['id']})" for item in merged_list])
                            llm_result = {
                                "status": "success", 
                                "message": f"âœ… å·²è‡ªå‹•åˆä½µ {len(merged_list)} å€‹åŒåå¯¦é«”: {names_str}",
                                "data": merged_list,
                                "instruction": "è«‹æ ¹æ“šä¸Šè¿° ID åˆ†åˆ¥å‘¼å«å°æ‡‰çš„å·¥å…· (ä¾‹å¦‚ query_campaign_basic ç”¨ client_ids, query_investment_budget ç”¨ agency_ids)"
                            }

                        elif status == "needs_confirmation":
                            # æ ¼å¼åŒ–å¤šé¸é …å±•ç¤º (åˆ†çµ„åŒ–)
                            candidates = result.get("data", [])
                            
                            # æŒ‰é¡å‹åˆ†çµ„
                            grouped = {}
                            for c in candidates:
                                c_type = c.get("type", "other")
                                if c_type not in grouped:
                                    grouped[c_type] = []
                                grouped[c_type].append(c)
                            
                            type_labels = {
                                "client": "ğŸ¢ å®¢æˆ¶ (Clients)",
                                "agency": "ğŸ¢ ä»£ç†å•† (Agencies)",
                                "brand": "ğŸ·ï¸ å“ç‰Œ/ç”¢å“ (Brands)",
                                "campaign": "ğŸ“¢ åŸ·è¡Œæ´»å‹• (Campaigns)",
                                "contract": "ğŸ“„ åˆç´„/æ’æœŸ (Contracts)",
                                "industry": "ğŸ­ ç”¢æ¥­é¡åˆ¥ (Industry)",
                                "sub_industry": "ğŸ­ ç”¢æ¥­å­é¡åˆ¥ (Sub-Industry)",
                                "other": "â“ å…¶ä»–"
                            }
                            
                            formatted_lines = []
                            global_idx = 1
                            
                            # æŒ‰ç…§å„ªå…ˆé †åºé¡¯ç¤ºé¡åˆ¥
                            for t in ["industry", "sub_industry", "client", "agency", "brand", "campaign", "contract", "other"]:
                                if t in grouped:
                                    formatted_lines.append(f"\n**{type_labels.get(t, t)}**")
                                    for item in grouped[t]:
                                        meta_str = ""
                                        if "metadata" in item:
                                            m = item["metadata"]
                                            meta_parts = []
                                            if "year" in m: meta_parts.append(str(m["year"]))
                                            if "status" in m: meta_parts.append(m["status"])
                                            if meta_parts:
                                                meta_str = f" _({', '.join(meta_parts)})_"
                                        
                                        formatted_lines.append(
                                            f"{global_idx}. {item['name']}{meta_str}"
                                        )
                                        # æ›´æ–°å€™é¸äººæ•¸æ“šä¸­çš„ç´¢å¼•ï¼Œä»¥ä¾¿å¾ŒçºŒåŒ¹é…
                                        item["temp_idx"] = global_idx
                                        global_idx += 1

                            llm_result = {
                                "status": "needs_confirmation",
                                "message": result.get("message"),
                                "instruction": "âš ï¸ æ‰¾åˆ°å¤šå€‹åŒ¹é…é …ï¼Œè«‹å‘ä½¿ç”¨è€…å±•ç¤ºä»¥ä¸‹åˆ†çµ„é¸é …ä¸¦è¦æ±‚å…¶é¸æ“‡ï¼š",
                                "formatted_list": "\n".join(formatted_lines),
                                "candidates_data": candidates,
                                "note": "ç•¶ä½¿ç”¨è€…å›è¦†å¾Œï¼Œå„ªå…ˆæ ¹æ“šç·¨è™Ÿæˆ–åç¨±é€²è¡ŒåŒ¹é…"
                            }

                        elif status == "rag_results":
                            # æ ¼å¼åŒ– RAG çµæœå±•ç¤º
                            rag_data = result.get("data", [])
                            formatted_rag = []
                            for idx, item in enumerate(rag_data, 1):
                                formatted_rag.append(
                                    f"{idx}. {item.get('value')} (ç›¸ä¼¼åº¦: {item.get('score', 0):.2f}) - ä¾†æº: {item.get('table')}.{item.get('source')}"
                                )

                            llm_result = {
                                "status": "rag_results",
                                "message": result.get("message"),
                                "instruction": "ğŸ” LIKE æŸ¥è©¢ç„¡çµæœï¼Œä½† RAG æ‰¾åˆ°ä»¥ä¸‹ç›¸ä¼¼å¯¦é«”ï¼š",
                                "rag_suggestions": formatted_rag,
                                "note": "è«‹å‘ä½¿ç”¨è€…ç¢ºèªæ˜¯å¦ä½¿ç”¨é€™äº›å»ºè­°ï¼Œæˆ–è¦æ±‚å…¶æä¾›æ›´æº–ç¢ºçš„åç¨±"
                            }

                        else:
                            # not_found æˆ–å…¶ä»–ç‹€æ…‹
                            llm_result = result
                    else:
                        llm_result = result

                elif ("query_" in tool_name and tool_name != "query_performance_metrics") or tool_name == "execute_sql_template":
                    # Store SQL query results (from MySQL templates)
                    if isinstance(result, dict):
                        final_data = result
                        latest_query_data = result.get("data", [])  # Store complete data

                        # Convert Decimal to float for JSON serialization
                        from decimal import Decimal
                        def _convert_decimals(obj):
                            if isinstance(obj, dict):
                                return {k: _convert_decimals(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [_convert_decimals(item) for item in obj]
                            elif isinstance(obj, Decimal):
                                return float(obj)
                            else:
                                return obj

                        sample_preview = _convert_decimals(result.get("data", [])[:5])

                        # Return simplified version to LLM with metadata
                        llm_result = {
                            "status": result.get("status"),
                            "count": result.get("count"),
                            "message": f"âœ… æŸ¥è©¢æˆåŠŸï¼å…± {result.get('count')} ç­†æ•¸æ“šå·²æº–å‚™å¥½ã€‚",
                            "instruction": "å®Œæ•´æ•¸æ“š ({0} ç­†) å·²è¼‰å…¥ï¼Œè«‹ä½¿ç”¨ pandas_processor è™•ç†".format(result.get("count")),
                            "columns": list(result.get("data", [{}])[0].keys()) if result.get("data") else [],
                            "sample_preview": sample_preview,
                            "generated_sql": result.get("generated_sql", "")  # ğŸ” é¡¯ç¤ºåŸ·è¡Œçš„ SQL
                        }
                    else:
                        llm_result = result

                elif tool_name == "query_performance_metrics":
                    # Store performance query results (from ClickHouse)
                    if isinstance(result, dict):
                        final_data = result
                        latest_query_data = result.get("data", [])

                        from decimal import Decimal
                        def _convert_decimals(obj):
                            if isinstance(obj, dict):
                                return {k: _convert_decimals(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [_convert_decimals(item) for item in obj]
                            elif isinstance(obj, Decimal):
                                return float(obj)
                            else:
                                return obj

                        sample_preview = _convert_decimals(result.get("data", [])[:5])

                        llm_result = {
                            "status": result.get("status"),
                            "count": result.get("count"),
                            "message": f"âœ… æˆæ•ˆæŸ¥è©¢æˆåŠŸï¼å…± {result.get('count')} ç­†æ•¸æ“šå·²æº–å‚™å¥½ã€‚",
                            "instruction": "å®Œæ•´æ•¸æ“š ({0} ç­†) å·²è¼‰å…¥ï¼Œè«‹ä½¿ç”¨ pandas_processor è™•ç†".format(result.get("count")),
                            "columns": list(result.get("data", [{}])[0].keys()) if result.get("data") else [],
                            "sample_preview": sample_preview
                        }
                    else:
                        llm_result = result

                elif tool_name == "pandas_processor":
                    # Store processed data
                    if isinstance(result, dict) and result.get("status") == "success":
                        final_data = result
                    llm_result = result

                else:
                    llm_result = result

                # Convert result to JSON-safe format (handle Decimal, datetime, etc.)
                def convert_to_json_safe(obj):
                    """Convert non-JSON-serializable objects to JSON-safe types"""
                    from decimal import Decimal
                    from datetime import datetime, date
                    import math

                    if isinstance(obj, float):
                        if math.isnan(obj) or math.isinf(obj):
                            return None
                        return obj
                    elif isinstance(obj, dict):
                        return {k: convert_to_json_safe(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_to_json_safe(item) for item in obj]
                    elif isinstance(obj, Decimal):
                        return float(obj)
                    elif isinstance(obj, (datetime, date)):
                        return obj.isoformat()
                    else:
                        return obj

                llm_result = convert_to_json_safe(llm_result)

                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=json.dumps(llm_result, ensure_ascii=False, indent=2)
                ))

            except Exception as e:
                error_msg = f"Tool execution error: {str(e)}"
                print(f"ERROR [DataAnalyst] {error_msg}")
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=error_msg
                ))

    # Ensure final_data is JSON safe
    if final_data:
        # Re-define or reuse convert_to_json_safe if needed, but since it was inner function, 
        # we need to define it again or rely on the fact that we process final_data if we caught it inside the loop.
        # However, final_data was assigned result BEFORE conversion in the loop:
        # if isinstance(result, dict) and result.get("status") == "success": final_data = result
        # So final_data still has Decimals.
        
        from decimal import Decimal
        from datetime import datetime, date
        import math
        
        def _final_convert(obj):
            if isinstance(obj, float):
                if math.isnan(obj) or math.isinf(obj):
                    return None
                return obj
            elif isinstance(obj, dict):
                return {k: _final_convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [_final_convert(item) for item in obj]
            elif isinstance(obj, Decimal):
                return float(obj)
            elif isinstance(obj, (datetime, date)):
                return obj.isoformat()
            else:
                return obj
                
        final_data = _final_convert(final_data)

    # Return updated state
    return {
        "analyst_data": final_data,
        "resolved_entities": resolved_entities,
        "final_response": markdown_response,
        "messages": [AIMessage(content=markdown_response)],
        "next": "END"
    }
