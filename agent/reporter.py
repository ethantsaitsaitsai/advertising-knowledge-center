"""
Data Reporter Node for AKC Framework 3.0

Responsibilities:
1. Receive raw `data_store` from Retriever.
2. Use `pandas_processor` to Merge, Aggregage, and Format data.
3. Generate the final Markdown response.
"""
from typing import Dict, Any, List
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from config.llm import llm
from agent.state import AgentState
from tools.data_processing_tool import pandas_processor
import json
import pandas as pd
import re

# Tools for Reporter (Pandas Only)
REPORTER_TOOLS = [pandas_processor]
llm_with_tools = llm.bind_tools(REPORTER_TOOLS)

REPORTER_SYSTEM_PROMPT = """ä½ æ˜¯ AKC æ™ºèƒ½åŠ©æ‰‹çš„è³‡æ–™å ±å‘Šå°ˆå®¶ (Data Reporter)ã€‚

**ä½ çš„ä»»å‹™**:
ä½ å¾æª¢ç´¢è€… (Retriever) é‚£è£¡æ¥æ”¶åˆ°äº†åŸå§‹æ•¸æ“š (`data_store`)ã€‚ä½ çš„å·¥ä½œæ˜¯å°‡é€™äº›é›¶æ•£çš„æ•¸æ“šæ•´åˆæˆä¸€å¼µæœ‰æ„ç¾©çš„å ±è¡¨ã€‚

**åŸå§‹æ•¸æ“šæ¦‚æ³**:
{data_summary}

**æ“ä½œæŒ‡å—**:
1. **åˆ†ææ•¸æ“šæº**: æŸ¥çœ‹æœ‰å“ªäº›æ•¸æ“šå¯ç”¨ (ä¾‹å¦‚ `query_investment_budget` æœ‰é‡‘é¡, `query_unified_performance` æœ‰æˆæ•ˆ)ã€‚
2. **æ±ºå®šä¸»è¡¨ (Anchor)**: é¸æ“‡æ¶µè“‹é¢æœ€å»£çš„è¡¨ä½œç‚ºä¸»è¡¨ (é€šå¸¸æ˜¯ Investment, Execution æˆ– Format Benchmark è¡¨)ã€‚
3. **åŸ·è¡Œåˆä½µ (Merge)**:
   - ä½¿ç”¨ `pandas_processor(operation="merge", ...)`ã€‚
   - **é€™æ˜¯å¿…é ˆçš„**ã€‚ä½ ä¸èƒ½åˆ†é–‹é¡¯ç¤ºå…©å¼µè¡¨ã€‚ä½ å¿…é ˆå°‡æŠ•è³‡é‡‘é¡ã€æˆæ•ˆã€å—çœ¾æ¨™ç±¤åˆä½µåœ¨ä¸€èµ·ã€‚
   - å¦‚æœæœ‰å—çœ¾æ¨™ç±¤ (`query_targeting_segments`)ï¼Œè«‹å…ˆç”¨ `groupby_concat` æŠŠå®ƒå£“æ‰æˆä¸€è¡Œä¸€ç­†ï¼Œå† Mergeã€‚
   - **é‡è¦**: è‹¥è¦åˆä½µã€Œæ˜ç´°è¡¨ã€(å¦‚æˆæ•ˆ) åˆ°ã€Œç¸½è¡¨ã€(å¦‚é ç®—)ï¼Œè«‹å…ˆå°‡æ˜ç´°è¡¨ **èšåˆ (Aggregate)** åˆ°ç›¸åŒé¡†ç²’åº¦ (å¦‚ Campaign+Format)ï¼Œé¿å…é‡‘é¡é‡è¤‡è¨ˆç®—ã€‚
4. **è¼¸å‡º (Select Columns)**:
   - ä½¿ç”¨ `select_columns` æŒ‡å®šä½¿ç”¨è€…é—œå¿ƒçš„æ¬„ä½ (ä¾‹å¦‚ `['å»£å‘Šæ ¼å¼', 'æŠ•è³‡é‡‘é¡', 'æˆæ•ˆ']`)ã€‚
   - å·¥å…·æœƒè‡ªå‹•è™•ç†æˆæ•ˆæŒ‡æ¨™çš„é‡ç®— (CTR/VTR)ã€‚

**ç¦æ­¢äº‹é …**:
- ç¦æ­¢ä½¿ç”¨ SQL å·¥å…· (ä½ æ²’æœ‰æ¬Šé™)ã€‚
- ç¦æ­¢åœ¨æ–‡å­—å›æ‡‰ä¸­è‡ªå·±ç•« Markdown è¡¨æ ¼ (å·¥å…·æœƒè‡ªå‹•ç”¢ç”Ÿ)ã€‚
- ç¦æ­¢åˆ†é–‹è¼¸å‡ºå¤šå¼µå°è¡¨ã€‚

**ç›®æ¨™**: ç”¢å‡ºä¸€å¼µåŒ…å«ã€Œ{user_query_intent}ã€ç›¸é—œæ‰€æœ‰ç¶­åº¦çš„å¯¬è¡¨ã€‚
"""

def data_reporter_node(state: AgentState) -> Dict[str, Any]:
    """
    Auto-Drive Reporter: Programmatically merges data and lets LLM summarize.
    """
    data_store = state.get("data_store", {})
    
    # --- Reconstruct data_store from messages if empty ---
    if not data_store:
        print("DEBUG [Reporter] data_store is empty. Reconstructing from ToolMessages...")
        from langchain_core.messages import ToolMessage
        
        tool_call_map = {}
        for msg in state.get("messages", []):
            if hasattr(msg, "tool_calls") and msg.tool_calls:
                for tc in msg.tool_calls:
                    tool_call_map[tc["id"]] = tc["name"]
        
        for msg in state.get("messages", []):
            if isinstance(msg, ToolMessage):
                tool_name = tool_call_map.get(msg.tool_call_id)
                if not tool_name: continue
                
                try:
                    content = msg.content
                    if "\n\nâœ…" in content:
                        content = content.split("\n\nâœ…")[0]
                    
                    result = None
                    try:
                        result = json.loads(content)
                    except json.JSONDecodeError:
                        cleaned_content = content.replace("Decimal('", "").replace("')", "")
                        try:
                            import ast
                            result = ast.literal_eval(cleaned_content)
                        except:
                            continue

                    if isinstance(result, dict) and "data" in result:
                        data = result.get("data")
                        if isinstance(data, list):
                            if tool_name not in data_store:
                                data_store[tool_name] = []
                            
                            if data:
                                existing_data_str = {json.dumps(row, sort_keys=True, default=str) for row in data_store[tool_name]}
                                for row in data:
                                    row_str = json.dumps(row, sort_keys=True, default=str)
                                    if row_str not in existing_data_str:
                                        data_store[tool_name].append(row)
                                        existing_data_str.add(row_str)
                except Exception as e:
                    print(f"DEBUG [Reporter] Error processing {tool_name}: {e}")

    original_query = state.get("routing_context", {}).get("original_query", "")
    execution_logs = state.get("debug_logs", [])

    has_actual_data = any(len(rows) > 0 for rows in data_store.values() if isinstance(rows, list))
    
    if not data_store or not has_actual_data:
        msg = "æŠ±æ­‰ï¼Œæˆ‘åœ¨è³‡æ–™åº«ä¸­æ²’æœ‰æ‰¾åˆ°èˆ‡ã€Œæ‚ éŠå¡ã€ç›¸é—œçš„æˆæ•ˆæˆ–é ç®—æ•¸æ“šã€‚" if "æ‚ éŠå¡" in original_query else "æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ•¸æ“šã€‚"
        return {
            "final_response": msg,
            "messages": [AIMessage(content=msg)],
            "debug_logs": execution_logs
        }

    print(f"DEBUG [Reporter] Auto-Drive Mode Activated. Processing {len(data_store)} datasets...")
    print(f"DEBUG [Reporter] Data Store Keys: {list(data_store.keys())}")

    # --- Pre-processing: Aggregate Investment Budget ---
    if "query_investment_budget" in data_store:
        print("DEBUG [Reporter] Pre-aggregating Investment Budget...")
        inv_data = data_store["query_investment_budget"]
        
        # èšåˆè‡³ Campaign + Format å±¤ç´šï¼Œé¿å…å¤šå€‹ CueList å°è‡´é‡è¤‡
        groupby_keys = ["campaign_id", "format_name", "format_type_id", "client_name", "agency_name"]
        if inv_data:
            available_keys = [k for k in groupby_keys if k in inv_data[0]]
            
            res = pandas_processor.invoke({
                "data": inv_data,
                "operation": "groupby_sum",
                "groupby_col": ",".join(available_keys),
                "sum_col": "investment_amount",
                "sort_col": "campaign_id"
            })
            
            if res.get("status") == "success":
                data_store["query_investment_budget"] = res.get("data")
                print(f"DEBUG [Reporter] Aggregated Investment Budget to {len(res.get('data'))} rows.")

    # --- Auto-Drive Pipeline ---
    current_data = None

    # 1. Determine Anchor Table
    if "query_execution_budget" in data_store:
        current_data = data_store["query_execution_budget"]
        print("DEBUG [Reporter] Anchor: Execution Budget")
    elif "query_investment_budget" in data_store:
        current_data = data_store["query_investment_budget"]
        print("DEBUG [Reporter] Anchor: Investment Budget")
    elif "query_industry_format_budget" in data_store:
        current_data = data_store["query_industry_format_budget"]
        print("DEBUG [Reporter] Anchor: Industry Format Budget")
    elif "query_unified_performance" in data_store:
        current_data = data_store["query_unified_performance"]
        print("DEBUG [Reporter] Anchor: Unified Performance")
    elif "query_campaign_basic" in data_store:
        current_data = data_store["query_campaign_basic"]
        print("DEBUG [Reporter] Anchor: Campaign Basic")
    else:
        valid_keys = [k for k in data_store.keys() if k != "resolve_entity" and k != "id_finder"]
        if valid_keys:
            key = valid_keys[0]
            current_data = data_store[key]
            print(f"DEBUG [Reporter] Anchor: Fallback to {key}")
    
    if current_data:
        print(f"DEBUG [Reporter] Anchor Cols: {list(current_data[0].keys())[:10]}")

        # Filter out Direct Client for Agency queries
        agency_keywords = ['ä»£ç†å•†', 'ä»£ç†', 'å»£å‘Šä»£ç†', 'agency']
        is_agency_query = any(kw in original_query.lower() for kw in agency_keywords)

        if is_agency_query and 'agency_name' in current_data[0]:
            current_data = [row for row in current_data if row.get('agency_name') != 'Direct Client']

    # 2. Process Segments (Flatten)
    if "query_targeting_segments" in data_store:
        # Check merge keys. Targeting usually has 'plaid' or 'campaign_id'
        has_plaid = current_data and ("plaid" in current_data[0] or "placement_id" in current_data[0])
        has_campaign = current_data and "campaign_id" in current_data[0]
        
        segments_data = data_store["query_targeting_segments"]
        
        # Priority: Merge by Plaid (More accurate) -> Campaign
        if has_plaid:
            print("DEBUG [Reporter] Merging Segments by Plaid...")
            # Normalize key
            seg_key = "plaid" if "plaid" in segments_data[0] else "placement_id"
            anchor_key = "plaid" if "plaid" in current_data[0] else "placement_id"
            
            # Rename segment key to match anchor if needed
            if seg_key != anchor_key:
                for row in segments_data:
                    if seg_key in row: row[anchor_key] = row.pop(seg_key)
            
            res = pandas_processor.invoke({
                "data": segments_data,
                "operation": "groupby_concat",
                "groupby_col": anchor_key,
                "concat_col": "segment_name",
                "new_col": "targeting_segments"
            })
            
            if res.get("status") == "success":
                current_data = pandas_processor.invoke({
                    "data": current_data,
                    "merge_data": res.get("data"),
                    "merge_on": anchor_key,
                    "operation": "merge",
                    "merge_how": "left"
                }).get("data")
                
        elif has_campaign:
            print("DEBUG [Reporter] Merging Segments by Campaign ID...")
            # Segments data might have campaign_id
            if "campaign_id" in segments_data[0]:
                res = pandas_processor.invoke({
                    "data": segments_data,
                    "operation": "groupby_concat",
                    "groupby_col": "campaign_id",
                    "concat_col": "segment_name",
                    "new_col": "targeting_segments"
                })
                if res.get("status") == "success":
                    current_data = pandas_processor.invoke({
                        "data": current_data,
                        "merge_data": res.get("data"),
                        "merge_on": "campaign_id",
                        "operation": "merge",
                        "merge_how": "left"
                    }).get("data")

    # 3. Merge Performance (The Inflation Fix)
    if "query_unified_performance" in data_store and current_data != data_store["query_unified_performance"]:
        print("DEBUG [Reporter] Merging Unified Performance...")
        perf_data = data_store["query_unified_performance"]
        
        # Normalize Keys
        for row in perf_data:
            if "cmpid" in row: row["campaign_id"] = row["cmpid"]
            
        # Determine Join Key and Granularity
        # If Anchor is Investment Budget, it is aggregated (Campaign + Format).
        # If Anchor is Execution Budget, it is Plaid level.
        
        has_plaid = "plaid" in current_data[0]
        has_format = "format_name" in current_data[0]
        
        if has_plaid:
            # Join by Plaid (1:1 usually) - Ideal
            print("DEBUG [Reporter] Performance Merge Strategy: Plaid Level")
            join_key = "plaid"
            # Ensure perf has plaid (it should)
        else:
            # Join by Campaign (+ Format if possible)
            print("DEBUG [Reporter] Performance Merge Strategy: Campaign Level (Pre-aggregation required)")
            
            join_key = "campaign_id"
            # Try to add Format to key
            if has_format and "ad_format_type" in perf_data[0]:
                # Normalize format names logic could go here, but let's stick to campaign_id for safety first
                # Or simplistic name matching
                pass

            # Pre-aggregate Performance to prevent inflation
            print(f"DEBUG [Reporter] Pre-aggregating performance by {join_key}")
            agg_res = pandas_processor.invoke({
                "data": perf_data,
                "operation": "groupby_sum",
                "groupby_col": join_key,
                "sum_col": "effective_impressions, clicks, total_q100_views, total_engagements",
                "top_n": 0
            })
            if agg_res.get("status") == "success":
                perf_data = agg_res.get("data")
            
        # Execute Merge
        res = pandas_processor.invoke({
            "data": current_data,
            "merge_data": perf_data,
            "merge_on": join_key,
            "operation": "merge",
            "merge_how": "left"
        })
        if res.get("status") == "success":
            current_data = res.get("data")

    # 4. Merge Campaign Basic (Enrichment)
    if "query_campaign_basic" in data_store and current_data != data_store["query_campaign_basic"]:
        if current_data and "campaign_id" in current_data[0]:
            print("DEBUG [Reporter] Enriching with Campaign Basic info...")
            res = pandas_processor.invoke({
                "data": current_data,
                "merge_data": data_store["query_campaign_basic"],
                "merge_on": "campaign_id",
                "operation": "merge",
                "merge_how": "left"
            })
            if res.get("status") == "success":
                current_data = res.get("data")

    # 5. Schema Planning & Output
    if current_data:
        available_cols = list(current_data[0].keys())
        print(f"DEBUG [Reporter] Planning Schema with cols: {available_cols}")
        
        # Load Mapping Config
        import os
        config_path = os.path.join(os.getcwd(), "config", "column_mapping.json")
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                mapping_config = json.load(f)
                std_dict_str = json.dumps(mapping_config.get("standard_dictionary", {}), ensure_ascii=False, indent=2)
                expansion_str = json.dumps(mapping_config.get("concept_expansion", {}), ensure_ascii=False, indent=2)
        except Exception:
            std_dict_str = "{}"
            expansion_str = "{}"

        SCHEMA_PROMPT = f"""
        ä½ æ˜¯è³‡æ–™å ±è¡¨æ¶æ§‹å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡é›œäº‚çš„ SQL åŸå§‹æ¬„ä½è½‰æ›ç‚ºä½¿ç”¨è€…æ˜“è®€çš„å•†æ¥­å ±è¡¨ã€‚
        
        ä½¿ç”¨è€…æŸ¥è©¢: "{original_query}"
        
        ç›®å‰è³‡æ–™è¡¨æœ‰ä»¥ä¸‹æ¬„ä½ (Raw Columns):
        {available_cols}
        
        è«‹æ ¹æ“šä½¿ç”¨è€…æŸ¥è©¢ï¼Œè¨­è¨ˆæœ€å¾Œçš„è¡¨æ ¼æ¶æ§‹ã€‚è«‹åš´æ ¼éµå®ˆä»¥ä¸‹æ¨™æº–ï¼š

        **1. AKC æ¨™æº–æ¬„ä½å­—å…¸ (Standard Dictionary)**:
        è«‹å‹™å¿…åƒè€ƒæ­¤å­—å…¸é€²è¡Œ Renameï¼š
        {std_dict_str}

        **2. æ¦‚å¿µè‡ªå‹•å±•é–‹ (Concept Expansion) - æœ€é«˜å„ªå…ˆç´š**:
        å¦‚æœä½¿ç”¨è€…æŸ¥è©¢äº†ä»¥ä¸‹æ¦‚å¿µï¼Œ**å¿…é ˆ**å°‡å°æ‡‰çš„**æ‰€æœ‰**æŒ‡æ¨™åŠ å…¥ `display_columns`ï¼š
        {expansion_str}

        âš ï¸ **é‡è¦è¦å‰‡**:
        - æ¦‚å¿µå±•é–‹çš„æ¬„ä½ç­‰åŒæ–¼ä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ï¼Œå¿…é ˆå…¨éƒ¨é¡¯ç¤ºã€‚
        - **æ™ºèƒ½ç²¾ç°¡**: ç•¶ä½¿ç”¨è€…è¦æ±‚ã€Œæˆæ•ˆã€æ™‚ï¼š
          - å¦‚æœè³‡æ–™ä¸­åŒæ™‚æœ‰ã€Œé»æ“Šç‡ (CTR%)ã€ã€Œè§€çœ‹ç‡ (VTR%)ã€ã€Œäº’å‹•ç‡ (ER%)ã€**ä»¥åŠ**ã€Œæœ‰æ•ˆæ›å…‰ã€ã€Œç¸½é»æ“Šã€
          - å‰‡**åªé¡¯ç¤ºæ¯”ç‡æŒ‡æ¨™**ï¼ˆCTR/VTR/ERï¼‰ï¼Œ**éš±è—åŸå§‹æŒ‡æ¨™**ï¼ˆæœ‰æ•ˆæ›å…‰ã€ç¸½é»æ“Šã€å®Œæ•´è§€çœ‹æ•¸ã€ç¸½äº’å‹•ï¼‰
          - åŸå› ï¼šæ¯”ç‡æŒ‡æ¨™å·²ç¶“åŒ…å«äº†æˆæ•ˆè³‡è¨Šï¼Œé¡¯ç¤ºåŸå§‹æ•¸æ“šæœƒé€ æˆè¡¨æ ¼å†—é¤˜
        - ä½†å¦‚æœåªæœ‰åŸå§‹æŒ‡æ¨™ï¼ˆæœ‰æ•ˆæ›å…‰ã€ç¸½é»æ“Šï¼‰è€Œæ²’æœ‰æ¯”ç‡ï¼Œå‰‡å¿…é ˆé¡¯ç¤ºåŸå§‹æŒ‡æ¨™

        **3. ä½”æ¯”è¨ˆç®—è¦å‰‡ (Percentage Calculation)**:
        - å¦‚æœä½¿ç”¨è€…æŸ¥è©¢åŒ…å«ã€Œ**ä½”æ¯”**ã€ã€ã€Œ**æ¯”ä¾‹**ã€ã€ã€Œ**åˆ†ä½ˆ**ã€ã€ã€Œ**Share**ã€ï¼š
          - å¿…é ˆåœ¨ `percentage_config` ä¸­æŒ‡å®šè¦è¨ˆç®—ä½”æ¯”çš„æ¬„ä½ï¼ˆé€šå¸¸æ˜¯æŠ•è³‡é‡‘é¡ï¼‰ã€‚
          - æ ¼å¼: `{{"column": "investment_amount", "new_col": "æŠ•è³‡é‡‘é¡ä½”æ¯”%"}}`

        **4. æ¬„ä½æ’åºè¦å‰‡ (Column Ordering)**:
        è«‹æŒ‰ç…§ä»¥ä¸‹é †åºæ’åˆ— `display_columns`ï¼š
        1. **ä¸»éµ** (Primary Key): é€šå¸¸æ˜¯ã€Œå»£å‘Šæ ¼å¼ã€æˆ–ã€Œæ´»å‹•åç¨±ã€
        2. **æ–‡å­—å‹æ¬„ä½** (Text Fields): å¦‚ã€Œå—çœ¾æ¨™ç±¤ã€ã€Œæ´»å‹•åç¨±ã€ç­‰æè¿°æ€§æ¬„ä½
        3. **æ•¸å€¼å‹æ¬„ä½** (Numeric Fields): å¦‚ã€ŒæŠ•è³‡é‡‘é¡ã€ã€ŒæŠ•è³‡é‡‘é¡ä½”æ¯”%ã€ã€Œé»æ“Šç‡ (CTR%)ã€ç­‰æ•¸å­—æŒ‡æ¨™

        **5. è¼¸å‡ºè¦æ±‚**:
        - **rename_map**: åŸå§‹æ¬„ä½ -> æ¨™æº–ä¸­æ–‡åç¨±ï¼ˆç”¨æ–¼æœ€çµ‚é¡¯ç¤ºï¼‰ã€‚
        - **display_columns**: æœ€çµ‚è¦é¡¯ç¤ºçš„æ¬„ä½åˆ—è¡¨ï¼ˆä½¿ç”¨ä¸­æ–‡åç¨±ï¼‰ã€‚
          - **è¦å‰‡**: é¡¯ç¤ºä¸»éµ + ä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚çš„æ¬„ä½ + æ¦‚å¿µå±•é–‹çš„æ¬„ä½ + **ä½”æ¯”æ¬„ä½**(è‹¥æœ‰)ã€‚
          - **ç¦æ­¢**: åš´ç¦å‡ºç¾ã€Œå®¢æˆ¶åç¨±ã€ã€Œä»£ç†å•†ã€ã€Œæ´»å‹•ç·¨è™Ÿã€ã€Œcmpidã€ã€Œplaidã€ã€Œformat_type_idã€ã€Œcue_list_idã€ç­‰å…§éƒ¨æ¬„ä½ï¼Œé™¤éä½¿ç”¨è€…æ˜ç¢ºè©¢å•ã€‚
          - **åš´æ ¼éæ¿¾**: åªæœ‰åœ¨ä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ï¼ˆæˆ–æ¦‚å¿µå±•é–‹éœ€è¦ï¼‰æ™‚æ‰é¡¯ç¤ºã€Œå»£å‘Šæ ¼å¼ã€ã€‚
        - **groupby_cols**: ç”¨æ–¼å»é‡çš„ç¶­åº¦æ¬„ä½ (è‹±æ–‡å)ã€‚
        - **sum_cols**: ç”¨æ–¼åŠ ç¸½çš„æŒ‡æ¨™æ¬„ä½ (è‹±æ–‡å)ã€‚
        - **concat_col**: ç”¨æ–¼å­—ä¸²èšåˆçš„æ¬„ä½ (è‹±æ–‡å)ã€‚
        - **sort_col**: æ’åºæ¬„ä½ (å¦‚ "ctr DESC")ã€‚
        - **limit**: é™åˆ¶é¡¯ç¤ºç­†æ•¸ï¼ˆæ•´æ•¸ï¼‰ã€‚å¦‚æœä½¿ç”¨è€…è¦ã€Œå‰Xã€å‰‡è¨­ç‚º Xï¼Œå¦å‰‡è¨­ç‚º 0ã€‚
        - **time_aggregation**: æ™‚é–“èšåˆé…ç½®ã€‚
        - **percentage_config**: ä½”æ¯”è¨ˆç®—é…ç½® (ä¾‹å¦‚ `{{"column": "investment_amount", "new_col": "æŠ•è³‡é‡‘é¡ä½”æ¯”%"}}`)ã€‚

        è«‹ç›´æ¥å›å‚³ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½• Markdown æ¨™è¨˜æˆ–æ–‡å­—èªªæ˜ã€‚
        """
        
        try:
            # LLM Planning call
            plan_response = llm.invoke([
                SystemMessage(content="You are a JSON generator. Output only valid raw JSON without any markdown formatting."),
                HumanMessage(content=SCHEMA_PROMPT)
            ])
            content = plan_response.content
            if isinstance(content, list): content = " ".join([c.get("text", "") for c in content])
            
            content = content.replace("```json", "").replace("```", "").strip()
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                plan = json.loads(json_match.group(0))
                
                # ... (Schema Optimizer Logic - kept same as before) ...
                
                # Auto-inject Performance Raw Metrics into sum_cols
                rate_metrics = ["ctr", "vtr", "er"]
                raw_metrics_map = {
                    "ctr": ["effective_impressions", "total_impressions", "total_clicks", "clicks"],
                    "vtr": ["total_q100_views", "total_q100", "effective_impressions", "total_impressions"],
                    "er": ["total_engagements", "effective_impressions", "total_impressions"]
                }
                
                # Build reverse mapping
                reverse_map = {v: k for k, v in plan.get("rename_map", {}).items()}
                
                sum_cols_en = []
                for col in plan.get("sum_cols", []):
                    sum_cols_en.append(reverse_map.get(col, col))
                
                requested_cols_en = []
                for col_cn in plan.get("display_columns", []):
                    requested_cols_en.append(reverse_map.get(col_cn, col_cn))
                
                # --- Robust Sort Logic & Default Fallback ---
                sort_col_val = plan.get("sort_col")
                
                # If LLM didn't provide a sort, pick a smart default based on business rules
                if not sort_col_val:
                    # Priorities: Rate Metrics > Money > Volume
                    priority_metrics = ["ctr", "vtr", "er", "investment_amount", "execution_amount", "clicks", "effective_impressions"]
                    for metric in priority_metrics:
                        if metric in available_cols:
                            sort_col_val = f"{metric} DESC"
                            print(f"DEBUG [Reporter] Auto-assigned default sort: {sort_col_val}")
                            break
                
                # Safe split (fixes NoneType error)
                sort_col_raw = sort_col_val.split(" ")[0] if sort_col_val else ""
                
                # Ensure sort column is included in selection if valid
                if sort_col_raw and sort_col_raw not in requested_cols_en:
                     requested_cols_en.append(sort_col_raw)
                
                # Update plan for downstream use
                plan["sort_col"] = sort_col_val

                # --- State-Driven Column Inclusion ---
                # If specific tools were called and returned data, we force those columns to be shown.
                # This ensures that if the Analyst decided to fetch data, the Reporter MUST show it,
                # even if the LLM Planning step accidentally missed it.
                if "query_targeting_segments" in data_store and data_store["query_targeting_segments"]:
                    col_en = "targeting_segments"
                    col_cn = "å—çœ¾æ¨™ç±¤"
                    
                    if col_en in available_cols and col_cn not in plan.get("display_columns", []):
                        print(f"DEBUG [Reporter] State-driven: Forcing inclusion of '{col_cn}' because segments data exists.")
                        # Insert at second position (after Primary Key)
                        plan["display_columns"].insert(1, col_cn)
                        
                        # Sync requested_cols_en for metric injection logic below
                        if col_en not in requested_cols_en:
                            requested_cols_en.append(col_en)
                        
                        # Ensure it's in concat_col for the pandas aggregation
                        if not concat_col_en:
                            concat_col_en = col_en
                        elif col_en not in concat_col_en:
                            concat_col_en += f",{col_en}"

                for rate in rate_metrics:
                    if any(rate in col.lower() for col in requested_cols_en):
                        raws = raw_metrics_map.get(rate, [])
                        for raw in raws:
                            if raw not in sum_cols_en and raw in available_cols:
                                sum_cols_en.append(raw)

                # Prepare processor args
                groupby_cols_en = [reverse_map.get(col, col) for col in plan.get("groupby_cols", [])]
                concat_col_en = plan.get("concat_col", "")
                if concat_col_en in reverse_map: concat_col_en = reverse_map[concat_col_en]
                
                # Ensure percentage base column is in sum_cols
                perc_config = plan.get("percentage_config")
                if perc_config and perc_config.get("column"):
                    base_col = perc_config.get("column")
                    if base_col not in sum_cols_en and base_col in available_cols:
                        sum_cols_en.append(base_col)

                # Execute Final Aggregation
                final_result = pandas_processor.invoke({
                    "data": current_data,
                    "operation": "groupby_sum",
                    "rename_map": plan.get("rename_map", {}),
                    "groupby_col": ",".join(groupby_cols_en),
                    "sum_col": ",".join(sum_cols_en),
                    "concat_col": concat_col_en,
                    "select_columns": plan.get("display_columns", []),
                    "sort_col": plan.get("sort_col"),
                    "percentage_config": perc_config, # [NEW] Pass percentage config
                    "ascending": False,
                    "top_n": plan.get("limit", 0)
                })

            else:
                raise ValueError("No valid JSON found in LLM response")

        except Exception as e:
            print(f"DEBUG [Reporter] Planning failed ({e}). Fallback to simple top_n.")
            fallback_select = [c for c in available_cols if not c.lower().endswith('id')]
            final_result = pandas_processor.invoke({
                "data": current_data,
                "operation": "top_n", 
                "top_n": 100,
                "select_columns": fallback_select[:7], 
                "sort_col": available_cols[0]
            })
    else:
        final_result = {"markdown": ""}

    final_table = final_result.get("markdown", "")
    
    # --- LLM Summary Generation ---
    start_date = state.get("routing_context", {}).get("start_date", "æŒ‡å®šæœŸé–“")
    end_date = state.get("routing_context", {}).get("end_date", "æŒ‡å®šæœŸé–“")
    
    SUMMARY_PROMPT = """
    ä½ æ˜¯æ•¸æ“šå ±å‘Šå‘ˆç¾è€…ã€‚è«‹é‡å°ä½¿ç”¨è€…æŸ¥è©¢ã€Œ{query}ã€èˆ‡ç”Ÿæˆçš„æ•¸æ“šè¡¨ç”¢å‡ºå›æ‡‰ã€‚
    
    è«‹å›å‚³ JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
    1. "suggestions": æ ¹æ“šæ•¸æ“šçµæœï¼Œæä¾› 3 å€‹å…·é«”ä¸”é«˜åº¦ç›¸é—œçš„å¾ŒçºŒæŸ¥è©¢å»ºè­°ï¼ˆå¸¶æœ‰ ğŸ’¡ ç¬¦è™Ÿèˆ‡æ¨™é¡Œï¼‰ã€‚
    
    **è¦å‰‡**:
    - **åš´ç¦åˆ†æ**: ä¸è¦åœ¨è¼¸å‡ºä¸­åŒ…å«ä»»ä½•æ•¸æ“šè§£è®€æˆ–ç¸½çµã€‚
    - **JSON æ ¼å¼**: åªå›å‚³åŸå§‹ JSONï¼Œä¸è¦åŒ…å« Markdown æ¨™è¨˜ã€‚
    """
    
    opening_text = f"é€™æ˜¯ **{start_date}** è‡³ **{end_date}** æœŸé–“ï¼Œé—œæ–¼ã€{original_query}ã€çš„æ•¸æ“šè³‡æ–™ã€‚"
    suggestions_text = ""

    if final_table:
        try:
            messages = [
                SystemMessage(content="You are a JSON generator. Output only valid raw JSON."),
                HumanMessage(content=SUMMARY_PROMPT.format(query=original_query))
            ]
            response = llm.invoke(messages)
            content = response.content
            if isinstance(content, list): content = " ".join([c.get("text", "") for c in content])
            
            content = content.replace("```json", "").replace("```", "").strip()
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                res_json = json.loads(json_match.group(0))
                suggestions_data = res_json.get("suggestions", "")
                if isinstance(suggestions_data, list):
                    suggestions_text = "\n".join(suggestions_data)
                else:
                    suggestions_text = str(suggestions_data)
        except Exception as e:
            print(f"DEBUG [Reporter] Summary JSON parsing failed: {e}")
    else:
        opening_text = "æŠ±æ­‰ï¼Œç„¡æ³•å¾æ•¸æ“šä¸­ç”Ÿæˆå ±è¡¨ã€‚"

    final_response = opening_text + "\n\n" + final_table
    if suggestions_text:
        final_response += "\n\n" + suggestions_text

    # Sanitization
    final_response = final_response.strip()
    if final_response.startswith("```"):
        final_response = re.sub(r"^```[a-zA-Z]*\n?", "", final_response)
        final_response = re.sub(r"\n?```$", "", final_response)
    final_response = final_response.strip()

    return {
        "final_response": final_response,
        "messages": [AIMessage(content=final_response)],
        "debug_logs": execution_logs
    }