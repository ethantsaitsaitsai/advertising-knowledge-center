"""
Data Reporter Node for AKC Framework 3.0

Responsibilities:
1. Receive raw `data_store` from Retriever.
2. Use `pandas_processor` to Merge, Aggregage, and Format data.
3. Generate the final Markdown response.
"""
from typing import Dict, Any, List
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from config.llm import llm
from agent.state import AgentState
from tools.data_processing_tool import pandas_processor
import json
import pandas as pd

# Tools for Reporter (Pandas Only)
REPORTER_TOOLS = [pandas_processor]
llm_with_tools = llm.bind_tools(REPORTER_TOOLS)

REPORTER_SYSTEM_PROMPT = """ä½ æ˜¯ AKC æ™ºèƒ½åŠ©æ‰‹çš„è³‡æ–™å ±å‘Šå°ˆå®¶ (Data Reporter)ã€‚

**ä½ çš„ä»»å‹™**:
ä½ å¾žæª¢ç´¢è€… (Retriever) é‚£è£¡æŽ¥æ”¶åˆ°äº†åŽŸå§‹æ•¸æ“š (`data_store`)ã€‚ä½ çš„å·¥ä½œæ˜¯å°‡é€™äº›é›¶æ•£çš„æ•¸æ“šæ•´åˆæˆä¸€å¼µæœ‰æ„ç¾©çš„å ±è¡¨ã€‚

**åŽŸå§‹æ•¸æ“šæ¦‚æ³**:
{data_summary}

**æ“ä½œæŒ‡å—**:
1. **åˆ†æžæ•¸æ“šæº**: æŸ¥çœ‹æœ‰å“ªäº›æ•¸æ“šå¯ç”¨ (ä¾‹å¦‚ `query_investment_budget` æœ‰é‡‘é¡, `query_performance_metrics` æœ‰æˆæ•ˆ)ã€‚
2. **æ±ºå®šä¸»è¡¨ (Anchor)**: é¸æ“‡æ¶µè“‹é¢æœ€å»£çš„è¡¨ä½œç‚ºä¸»è¡¨ (é€šå¸¸æ˜¯ Investment æˆ– Format è¡¨)ã€‚
3. **åŸ·è¡Œåˆä½µ (Merge)**:
   - ä½¿ç”¨ `pandas_processor(operation="merge", ...)`ã€‚
   - **é€™æ˜¯å¿…é ˆçš„**ã€‚ä½ ä¸èƒ½åˆ†é–‹é¡¯ç¤ºå…©å¼µè¡¨ã€‚ä½ å¿…é ˆå°‡æŠ•è³‡é‡‘é¡ã€æˆæ•ˆã€å—çœ¾æ¨™ç±¤åˆä½µåœ¨ä¸€èµ·ã€‚
   - å¦‚æžœæœ‰å—çœ¾æ¨™ç±¤ (`query_targeting_segments`)ï¼Œè«‹å…ˆç”¨ `groupby_concat` æŠŠå®ƒå£“æ‰æˆä¸€è¡Œä¸€ç­†ï¼Œå† Mergeã€‚
4. **è¼¸å‡º (Select Columns)**:
   - ä½¿ç”¨ `select_columns` æŒ‡å®šä½¿ç”¨è€…é—œå¿ƒçš„æ¬„ä½ (ä¾‹å¦‚ `['å»£å‘Šæ ¼å¼', 'æŠ•è³‡é‡‘é¡', 'æˆæ•ˆ']`)ã€‚
   - å·¥å…·æœƒè‡ªå‹•è™•ç†æˆæ•ˆæŒ‡æ¨™çš„é‡ç®— (CTR/VTR)ã€‚

**ç¦æ­¢äº‹é …**:
- ç¦æ­¢ä½¿ç”¨ SQL å·¥å…· (ä½ æ²’æœ‰æ¬Šé™)ã€‚
- ç¦æ­¢åœ¨æ–‡å­—å›žæ‡‰ä¸­è‡ªå·±ç•« Markdown è¡¨æ ¼ (å·¥å…·æœƒè‡ªå‹•ç”¢ç”Ÿ)ã€‚
- ç¦æ­¢åˆ†é–‹è¼¸å‡ºå¤šå¼µå°è¡¨ã€‚

**ç›®æ¨™**: ç”¢å‡ºä¸€å¼µåŒ…å«ã€Œ{user_query_intent}ã€ç›¸é—œæ‰€æœ‰ç¶­åº¦çš„å¯¬è¡¨ã€‚
"""

def data_reporter_node(state: AgentState) -> Dict[str, Any]:
    """
    Auto-Drive Reporter: Programmatically merges data and lets LLM summarize.
    """
    data_store = state.get("data_store", {})
    original_query = state.get("routing_context", {}).get("original_query", "")
    execution_logs = state.get("debug_logs", [])

    if not data_store:
        return {
            "final_response": "æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ•¸æ“šã€‚",
            "messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ•¸æ“šã€‚")]
        }

    print(f"DEBUG [Reporter] Auto-Drive Mode Activated. Processing {len(data_store)} datasets...")

    # --- Pre-processing: Aggregate Investment Budget ---
    # Fix for Many-to-Many explosion: Group investment by Campaign + Format before merging
    if "query_investment_budget" in data_store:
        print("DEBUG [Reporter] Pre-aggregating Investment Budget...")
        inv_data = data_store["query_investment_budget"]
        
        # We want to sum 'investment_amount' but keep other descriptor columns.
        # Strategy: Group by ID/Name keys, sum Amount, keep others as is (first).
        
        # 1. Identify GroupBy Keys (Unique Identifier for a row in the final table)
        # Usually Campaign + Format is the granularity users want.
        groupby_keys = ["campaign_id", "format_name", "format_type_id", "client_name", "agency_name"]
        
        # Filter keys that actually exist in the data
        if inv_data:
            available_keys = [k for k in groupby_keys if k in inv_data[0]]
            
            res = pandas_processor.invoke({
                "data": inv_data,
                "operation": "groupby_sum",
                "groupby_col": ",".join(available_keys),
                "sum_col": "investment_amount",
                "sort_col": "campaign_id"
            })
            
            if res.get("status") == "success":
                data_store["query_investment_budget"] = res.get("data")
                print(f"DEBUG [Reporter] Aggregated Investment Budget to {len(res.get('data'))} rows.")

    # --- Auto-Drive Pipeline ---
    current_data = None

    # 1. Determine Anchor Table (ä¸»è¡¨)
    # Priority: Execution > Investment > Performance > Campaign > Others (but NEVER resolve_entity)
    # Note: resolve_entity should NEVER be used as anchor - it's only for ID lookup

    if "query_execution_budget" in data_store:
        current_data = data_store["query_execution_budget"]
        print("DEBUG [Reporter] Anchor: Execution Budget")
    elif "query_investment_budget" in data_store:
        current_data = data_store["query_investment_budget"]
        print("DEBUG [Reporter] Anchor: Investment Budget")
    elif "query_performance_metrics" in data_store:
        current_data = data_store["query_performance_metrics"]
        print("DEBUG [Reporter] Anchor: Performance Metrics")
    elif "query_campaign_basic" in data_store:
        current_data = data_store["query_campaign_basic"]
        print("DEBUG [Reporter] Anchor: Campaign Basic")
    else:
        # Fallback: take the first non-resolve_entity table
        valid_keys = [k for k in data_store.keys() if k != "resolve_entity"]
        if valid_keys:
            key = valid_keys[0]
            current_data = data_store[key]
            print(f"DEBUG [Reporter] Anchor: Fallback to {key}")
        else:
            # Last resort: even resolve_entity, but this should rarely happen
            key = list(data_store.keys())[0]
            current_data = data_store[key]
            print(f"DEBUG [Reporter] Anchor: Last resort fallback to {key}")
    
    if current_data:
        print(f"DEBUG [Reporter] Anchor Cols: {list(current_data[0].keys())[:10]}")

        # --- [NEW] Smart Filtering: Remove "Direct Client" for Agency Queries ---
        # When user asks about agencies, filter out "Direct Client" (non-agency records)
        agency_keywords = ['ä»£ç†å•†', 'ä»£ç†', 'å»£å‘Šä»£ç†', 'agency']
        is_agency_query = any(kw in original_query.lower() for kw in agency_keywords)

        if is_agency_query and current_data:
            # Check if data has agency_name column
            if 'agency_name' in current_data[0]:
                original_count = len(current_data)
                current_data = [row for row in current_data if row.get('agency_name') != 'Direct Client']
                filtered_count = len(current_data)
                print(f"DEBUG [Reporter] Filtered out Direct Client: {original_count} â†’ {filtered_count} rows")

        if not current_data:
            return {
                "final_response": "æŠ±æ­‰ï¼ŒéŽæ¿¾å¾Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œæ•¸æ“šã€‚",
                "messages": [AIMessage(content="æŠ±æ­‰ï¼ŒéŽæ¿¾å¾Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œæ•¸æ“šã€‚")]
            }

    # 2. Process Segments (Flatten)
    if "query_targeting_segments" in data_store:
        print("DEBUG [Reporter] Processing Segments (Groupby Concat)...")
        res = pandas_processor.invoke({
            "data": data_store["query_targeting_segments"],
            "operation": "groupby_concat",
            "groupby_col": "campaign_id",
            "concat_col": "segment_name",
            "new_col": "targeting_segments"
        })
        if res.get("status") == "success":
            # Merge flattened segments into current_data
            current_data = pandas_processor.invoke({
                "data": current_data,
                "merge_data": res.get("data"),
                "merge_on": "campaign_id",
                "operation": "merge",
                "merge_how": "left"
            }).get("data")
            if current_data:
                print(f"DEBUG [Reporter] After Segments Merge Cols: {list(current_data[0].keys())[:10]}")

    # 3. Merge Performance (if not anchor)
    if "query_performance_metrics" in data_store and current_data != data_store["query_performance_metrics"]:
        print("DEBUG [Reporter] Merging Performance Metrics...")
        perf_data = data_store["query_performance_metrics"]
        
        # --- Dual-Path ID Strategy ---
        # 1. Rename ClickHouse ID to align with Execution ID
        if perf_data:
            # Create a copy or modify in place? Modifying list of dicts is safe here.
            for row in perf_data:
                if "format_type_id" in row:
                    row["format_type_id_exec"] = row.pop("format_type_id")
        
        # 2. Try Match with Execution ID
        primary_key = "campaign_id, format_type_id_exec"
        
        # Check if anchor has this key (it should, after Ad Format merge)
        # Wait! Ad Formats merge happens AFTER Performance merge in current code flow?
        # CRITICAL: We must merge Ad Formats FIRST to get the 'format_type_id_exec'
        pass # Placeholder to indicate logic shift needed

    # [REORDER] Merge Ad Formats FIRST (Step 3 -> Step 4)
    if "query_ad_formats" in data_store:
        print("DEBUG [Reporter] Merging Ad Formats (Priority)...")
        # Use simple Campaign ID merge first to enrich the table with IDs
        res = pandas_processor.invoke({
            "data": current_data,
            "merge_data": data_store["query_ad_formats"],
            "merge_on": "campaign_id", 
            "operation": "merge",
            "merge_how": "left"
        })
        if res.get("status") == "success":
            current_data = res.get("data")
            print("DEBUG [Reporter] Ad Formats merged. Now we have dual IDs.")

    # [REORDER] Merge Performance SECOND (Step 4 -> Step 3)
    if "query_performance_metrics" in data_store and current_data != data_store["query_performance_metrics"]:
        print("DEBUG [Reporter] Merging Performance Metrics (Dual-Path)...")
        
        # Check match rate with Execution ID
        def get_match_rate(left_data, right_data, on_keys):
            try:
                l_df = pd.DataFrame(left_data)
                r_df = pd.DataFrame(right_data)
                merged = pd.merge(l_df, r_df, on=on_keys, how="inner")
                return len(merged) / len(l_df) if len(l_df) > 0 else 0
            except:
                return 0

        # Try ID Match (Exec ID)
        match_rate = get_match_rate(current_data, perf_data, ["campaign_id", "format_type_id_exec"])
        print(f"DEBUG [Reporter] Strategy A (Exec ID Match) Rate: {match_rate:.2%}")
        
        final_merge_key = "campaign_id" # Default fallback
        
        if match_rate > 0.3:
            final_merge_key = "campaign_id, format_type_id_exec"
        else:
            # Fallback: Name Match (as backup)
            print("DEBUG [Reporter] Exec ID Match failed. Trying Strategy B (Name Match)...")
            def normalize_format(name):
                if not isinstance(name, str): return str(name)
                return name.replace("ï¼ˆå·²é€€å½¹ï¼‰", "").replace("(å·²é€€å½¹)", "").replace("å·²é€€å½¹ - ", "").strip().lower()
            
            for row in current_data:
                row["_norm_fmt"] = normalize_format(row.get("format_name", ""))
            for row in perf_data:
                row["_norm_fmt"] = normalize_format(row.get("format_name", "")) # ClickHouse usually has name
                
            name_match_rate = get_match_rate(current_data, perf_data, ["campaign_id", "_norm_fmt"])
            print(f"DEBUG [Reporter] Strategy B (Name Match) Rate: {name_match_rate:.2%}")
            
            if name_match_rate > 0:
                final_merge_key = "campaign_id, _norm_fmt"
            else:
                 # Final Fallback: Aggregation
                 print("DEBUG [Reporter] Fallback to Campaign Aggregation")
                 agg_res = pandas_processor.invoke({
                     "data": perf_data,
                     "operation": "groupby_sum",
                     "groupby_col": "campaign_id",
                     "sum_col": "effective_impressions, total_clicks, total_q100_views, total_engagements",
                     "sort_col": "campaign_id"
                 })
                 if agg_res.get("status") == "success":
                     perf_data = agg_res.get("data")
                     final_merge_key = "campaign_id"

        # Merge
        res = pandas_processor.invoke({
            "data": current_data,
            "merge_data": perf_data,
            "merge_on": final_merge_key,
            "operation": "merge",
            "merge_how": "left"
        })
        if res.get("status") == "success":
            current_data = res.get("data")
            # Cleanup
            if current_data and "_norm_fmt" in current_data[0]:
                for row in current_data:
                    row.pop("_norm_fmt", None)

    # 5. LLM Schema Planning (The Brain)
    # Instead of guessing, we ask the LLM to map columns based on user intent.
    if current_data:
        available_cols = list(current_data[0].keys())
        print(f"DEBUG [Reporter] Planning Schema with cols: {available_cols}")
        
        # Load Mapping Config
        import os
        config_path = os.path.join(os.getcwd(), "config", "column_mapping.json")
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                mapping_config = json.load(f)
                std_dict_str = json.dumps(mapping_config.get("standard_dictionary", {}), ensure_ascii=False, indent=2)
                expansion_str = json.dumps(mapping_config.get("concept_expansion", {}), ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"WARN [Reporter] Failed to load column_mapping.json: {e}")
            std_dict_str = "(Configuration Load Failed)"
            expansion_str = "(Configuration Load Failed)"

        SCHEMA_PROMPT = f"""
        ä½ æ˜¯è³‡æ–™å ±è¡¨æž¶æ§‹å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡é›œäº‚çš„ SQL åŽŸå§‹æ¬„ä½è½‰æ›ç‚ºä½¿ç”¨è€…æ˜“è®€çš„å•†æ¥­å ±è¡¨ã€‚
        
        ä½¿ç”¨è€…æŸ¥è©¢: "{original_query}"
        
        ç›®å‰è³‡æ–™è¡¨æœ‰ä»¥ä¸‹æ¬„ä½ (Raw Columns):
        {available_cols}
        
        è«‹æ ¹æ“šä½¿ç”¨è€…æŸ¥è©¢ï¼Œè¨­è¨ˆæœ€å¾Œçš„è¡¨æ ¼æž¶æ§‹ã€‚è«‹åš´æ ¼éµå®ˆä»¥ä¸‹æ¨™æº–ï¼š

        **1. AKC æ¨™æº–æ¬„ä½å­—å…¸ (Standard Dictionary)**:
        è«‹å‹™å¿…åƒè€ƒæ­¤å­—å…¸é€²è¡Œ Renameï¼š
        {std_dict_str}

        **2. æ¦‚å¿µè‡ªå‹•å±•é–‹ (Concept Expansion) - æœ€é«˜å„ªå…ˆç´š**:
        å¦‚æžœä½¿ç”¨è€…æŸ¥è©¢äº†ä»¥ä¸‹æ¦‚å¿µï¼Œ**å¿…é ˆ**å°‡å°æ‡‰çš„**æ‰€æœ‰**æŒ‡æ¨™åŠ å…¥ `display_columns`ï¼š
        {expansion_str}

        âš ï¸ **é‡è¦è¦å‰‡**:
        - æ¦‚å¿µå±•é–‹çš„æ¬„ä½ç­‰åŒæ–¼ä½¿ç”¨è€…æ˜Žç¢ºè¦æ±‚ï¼Œå¿…é ˆå…¨éƒ¨é¡¯ç¤ºã€‚
        - **æ™ºèƒ½ç²¾ç°¡**: ç•¶ä½¿ç”¨è€…è¦æ±‚ã€Œæˆæ•ˆã€æ™‚ï¼š
          - å¦‚æžœè³‡æ–™ä¸­åŒæ™‚æœ‰ã€Œé»žæ“ŠçŽ‡ (CTR%)ã€ã€Œè§€çœ‹çŽ‡ (VTR%)ã€ã€Œäº’å‹•çŽ‡ (ER%)ã€**ä»¥åŠ**ã€Œæœ‰æ•ˆæ›å…‰ã€ã€Œç¸½é»žæ“Šã€
          - å‰‡**åªé¡¯ç¤ºæ¯”çŽ‡æŒ‡æ¨™**ï¼ˆCTR/VTR/ERï¼‰ï¼Œ**éš±è—åŽŸå§‹æŒ‡æ¨™**ï¼ˆæœ‰æ•ˆæ›å…‰ã€ç¸½é»žæ“Šã€å®Œæ•´è§€çœ‹æ•¸ã€ç¸½äº’å‹•ï¼‰
          - åŽŸå› ï¼šæ¯”çŽ‡æŒ‡æ¨™å·²ç¶“åŒ…å«äº†æˆæ•ˆè³‡è¨Šï¼Œé¡¯ç¤ºåŽŸå§‹æ•¸æ“šæœƒé€ æˆè¡¨æ ¼å†—é¤˜
        - ä½†å¦‚æžœåªæœ‰åŽŸå§‹æŒ‡æ¨™ï¼ˆæœ‰æ•ˆæ›å…‰ã€ç¸½é»žæ“Šï¼‰è€Œæ²’æœ‰æ¯”çŽ‡ï¼Œå‰‡å¿…é ˆé¡¯ç¤ºåŽŸå§‹æŒ‡æ¨™

        **3. æ¬„ä½æŽ’åºè¦å‰‡ (Column Ordering)**:
        è«‹æŒ‰ç…§ä»¥ä¸‹é †åºæŽ’åˆ— `display_columns`ï¼š
        1. **ä¸»éµ** (Primary Key): é€šå¸¸æ˜¯ã€Œå»£å‘Šæ ¼å¼ã€æˆ–ã€Œæ´»å‹•åç¨±ã€
        2. **æ–‡å­—åž‹æ¬„ä½** (Text Fields): å¦‚ã€Œå—çœ¾æ¨™ç±¤ã€ã€Œæ´»å‹•åç¨±ã€ç­‰æè¿°æ€§æ¬„ä½
        3. **æ•¸å€¼åž‹æ¬„ä½** (Numeric Fields): å¦‚ã€ŒæŠ•è³‡é‡‘é¡ã€ã€Œé»žæ“ŠçŽ‡ (CTR%)ã€ç­‰æ•¸å­—æŒ‡æ¨™

        ç¯„ä¾‹é †åº: ["å»£å‘Šæ ¼å¼", "å—çœ¾æ¨™ç±¤", "æŠ•è³‡é‡‘é¡", "é»žæ“ŠçŽ‡ (CTR%)", "è§€çœ‹çŽ‡ (VTR%)", "äº’å‹•çŽ‡ (ER%)"]

        **4. Top-N æŽ’åè­˜åˆ¥ (Ranking & Limit)**:
        âš ï¸ **é‡è¦**: åªæœ‰åœ¨ä½¿ç”¨è€…**æ˜Žç¢ºè¦æ±‚æŽ’å**æ™‚æ‰è¨­å®š `limit`ï¼Œå¦å‰‡è«‹è¨­ç‚º 0ï¼ˆé¡¯ç¤ºå…¨éƒ¨ï¼‰ã€‚

        - **éœ€è¦è¨­å®š limit çš„æƒ…æ³** (ä½¿ç”¨è€…æ˜Žç¢ºè¦æ±‚æŽ’å):
          - **é—œéµå­—**: ã€Œå‰ Xã€ã€Œå‰Xåã€ã€Œå‰Xå¤§ã€ã€ŒTop Xã€ã€ŒæŽ’åå‰Xã€ã€Œæœ€é«˜Xå€‹ã€ã€Œæœ€ä½³Xå€‹ã€
            - ä¾‹å¦‚ï¼šã€Œå‰ä¸‰å¤§æ ¼å¼ã€â†’ limit=3
            - ä¾‹å¦‚ï¼šã€ŒTop 5 å®¢æˆ¶ã€â†’ limit=5
            - ä¾‹å¦‚ï¼šã€ŒæŽ’åå‰åçš„æ´»å‹•ã€â†’ limit=10

        - **ä¸éœ€è¦è¨­å®š limit çš„æƒ…æ³** (è¨­ç‚º 0ï¼Œé¡¯ç¤ºå…¨éƒ¨):
          - ä½¿ç”¨è€…åªæ˜¯è¦ã€ŒåŒ¯ç¸½ã€ã€Œçµ±è¨ˆã€ã€Œåˆ†æžã€ã€Œæ‰€æœ‰ã€ã€Œå„å€‹ã€ç­‰ï¼Œæ²’æœ‰æ˜Žç¢ºè¦æ±‚æŽ’å
            - ä¾‹å¦‚ï¼šã€Œä»£ç†å•† YTD èªåˆ—é‡‘é¡ã€â†’ limit=0ï¼ˆé¡¯ç¤ºæ‰€æœ‰ä»£ç†å•†ï¼‰
            - ä¾‹å¦‚ï¼šã€Œå„ç”¢æ¥­çš„æŠ•è³‡é‡‘é¡ã€â†’ limit=0ï¼ˆé¡¯ç¤ºæ‰€æœ‰ç”¢æ¥­ï¼‰
            - ä¾‹å¦‚ï¼šã€Œå®¢æˆ¶çš„æˆæ•ˆåˆ†æžã€â†’ limit=0ï¼ˆé¡¯ç¤ºæ‰€æœ‰å®¢æˆ¶ï¼‰

        **æŽ’åºæ¬„ä½é¸æ“‡é‚è¼¯**ï¼š
        - å¦‚æžœæŸ¥è©¢æåˆ°ã€Œæˆæ•ˆã€ä¸”è³‡æ–™ä¸­æœ‰ CTR/VTR/ERï¼Œå„ªå…ˆä½¿ç”¨ `ctr`ï¼ˆé»žæ“ŠçŽ‡ï¼‰é™åºæŽ’åˆ—
        - å¦‚æžœæŸ¥è©¢æåˆ°ã€ŒæŠ•è³‡ã€ã€Œèªåˆ—ã€ã€Œé ç®—ã€ã€Œé‡‘é¡ã€ï¼Œä½¿ç”¨ `execution_amount` æˆ– `investment_amount` é™åºæŽ’åˆ—
        - å¦‚æžœæŸ¥è©¢æåˆ°ã€Œæ›å…‰ã€ï¼Œä½¿ç”¨ `effective_impressions` é™åºæŽ’åˆ—
        - å¦‚æžœç„¡æ³•åˆ¤æ–·ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹æ•¸å€¼åž‹æ¬„ä½é™åºæŽ’åˆ—

        âš ï¸ **æ³¨æ„**: `sort_col` å¿…é ˆä½¿ç”¨**åŽŸå§‹è‹±æ–‡æ¬„ä½å**ï¼ˆå¦‚ "ctr"ã€"execution_amount"ï¼‰ï¼Œä¸¦åœ¨æ¬„ä½åå¾ŒåŠ ä¸Š " DESC" è¡¨ç¤ºé™åºï¼ˆä¾‹å¦‚ "execution_amount DESC"ï¼‰

        **5. æ™‚é–“ç¶­åº¦èšåˆ (Time Period Aggregation)**:
        å¦‚æžœä½¿ç”¨è€…æŸ¥è©¢åŒ…å«ä»¥ä¸‹æ™‚é–“èšåˆé—œéµå­—ï¼Œè«‹è¨­å®š `time_aggregation`ï¼š
        - **é—œéµå­—**: ã€Œæ¯æœˆã€ã€ŒæŒ‰æœˆã€ã€Œæœˆå ±ã€ã€Œæ¯å­£ã€ã€ŒæŒ‰å­£ã€ã€Œå­£å ±ã€ã€Œæ¯å¹´ã€ã€ŒæŒ‰å¹´ã€ã€Œå¹´å ±ã€
          - ä¾‹å¦‚ï¼šã€Œæ¯æœˆé ç®—ã€â†’ time_aggregation={{"enabled": true, "period": "month"}}
          - ä¾‹å¦‚ï¼šã€ŒæŒ‰å­£åº¦æˆæ•ˆã€â†’ time_aggregation={{"enabled": true, "period": "quarter"}}
          - ä¾‹å¦‚ï¼šã€Œå¹´åº¦æŠ•è³‡ã€â†’ time_aggregation={{"enabled": true, "period": "year"}}

        **æ™‚é–“èšåˆè¦å‰‡**ï¼š
        - å¦‚æžœå•Ÿç”¨æ™‚é–“èšåˆï¼Œç³»çµ±æœƒè‡ªå‹•ï¼š
          1. å¾žå¯ç”¨çš„æ—¥æœŸæ¬„ä½ (è‡ªå‹•åµæ¸¬ "investment_start_date", "start_date", "end_date" ç­‰) ç”Ÿæˆæ™‚é–“ç¶­åº¦æ¬„ä½ï¼ˆå¦‚ "year_month"ï¼‰
          2. å°‡æ™‚é–“ç¶­åº¦æ¬„ä½æ·»åŠ åˆ° `groupby_cols` çš„**ç¬¬ä¸€ä½**
          3. å°‡æ™‚é–“ç¶­åº¦ä¸­æ–‡åç¨±ï¼ˆå¦‚ã€Œå¹´/æœˆã€ï¼‰æ·»åŠ åˆ° `display_columns` çš„**ç¬¬ä¸€ä½**
        - `period` å¯é¸å€¼: "month" (æœˆ), "quarter" (å­£), "year" (å¹´)
        - ç³»çµ±æœƒè‡ªå‹•é¸æ“‡é©ç•¶çš„æ—¥æœŸæ¬„ä½ï¼Œä½ ä¸éœ€è¦æŒ‡å®š `source_col`

        âš ï¸ **æ³¨æ„**: æ™‚é–“ç¶­åº¦æ¬„ä½æœƒè‡ªå‹•ç”Ÿæˆï¼Œä½ ä¸éœ€è¦åœ¨ `rename_map` ä¸­æ‰‹å‹•æ·»åŠ ã€‚

        **6. ä½”æ¯”è¨ˆç®— (Percentage Calculation)**:
        å¦‚æžœä½¿ç”¨è€…æŸ¥è©¢åŒ…å«ä»¥ä¸‹ä½”æ¯”é—œéµå­—ï¼Œè«‹è¨­å®š `percentage_config`ï¼š
        - **é—œéµå­—**: ã€Œä½”æ¯”ã€ã€Œç™¾åˆ†æ¯”ã€ã€Œæ¯”ä¾‹ã€ã€Œpercentageã€ã€Œproportionã€ã€Œshareã€ã€Œå æ¯”ã€
          - ä¾‹å¦‚ï¼šã€Œå„æ ¼å¼çš„é ç®—ä½”æ¯”ã€â†’ percentage_config={{"enabled": true, "value_col": "investment_amount", "percentage_col": "ä½”æ¯” (%)"}}
          - ä¾‹å¦‚ï¼šã€Œå®¢æˆ¶æŠ•è³‡é‡‘é¡æ¯”ä¾‹ã€â†’ percentage_config={{"enabled": true, "value_col": "investment_amount", "percentage_col": "æ¯”ä¾‹ (%)"}}
          - ä¾‹å¦‚ï¼šã€Œæˆæ•ˆä½”æ¯”ã€â†’ percentage_config={{"enabled": true, "value_col": "effective_impressions", "percentage_col": "ä½”æ¯” (%)"}}

        **ä½”æ¯”è¨ˆç®—è¦å‰‡**ï¼š
        - **value_col**: è¦è¨ˆç®—ä½”æ¯”çš„æ¬„ä½ï¼ˆåŽŸå§‹è‹±æ–‡æ¬„ä½åï¼‰ï¼Œé€šå¸¸æ˜¯ä½¿ç”¨è€…æŸ¥è©¢ä¸­æåˆ°çš„é‡‘é¡æˆ–æ•¸é‡æ¬„ä½
          - å¦‚æžœæŸ¥è©¢æåˆ°ã€Œé ç®—ã€ã€Œé‡‘é¡ã€ã€ŒæŠ•è³‡ã€ï¼Œä½¿ç”¨ "investment_amount" æˆ– "execution_amount"
          - å¦‚æžœæŸ¥è©¢æåˆ°ã€Œæ›å…‰ã€ï¼Œä½¿ç”¨ "effective_impressions"
          - å¦‚æžœæŸ¥è©¢æåˆ°ã€Œé»žæ“Šã€ï¼Œä½¿ç”¨ "total_clicks"
        - **percentage_col**: ä½”æ¯”æ¬„ä½çš„ä¸­æ–‡åç¨±ï¼ˆé¡¯ç¤ºç”¨ï¼‰ï¼Œå»ºè­°ä½¿ç”¨ã€Œä½”æ¯” (%)ã€
        - ç³»çµ±æœƒè‡ªå‹•ï¼š
          1. è¨ˆç®—æ¯è¡Œ value_col ä½”ç¸½å’Œçš„ç™¾åˆ†æ¯”
          2. å°‡ç™¾åˆ†æ¯”æ¬„ä½æ·»åŠ åˆ° `display_columns` çš„**æœ€å¾Œä¸€ä½**
          3. å°‡ç™¾åˆ†æ¯”æ¬„ä½æ·»åŠ åˆ° `rename_map` (å¦‚æžœéœ€è¦)

        âš ï¸ **é‡è¦**: ä½”æ¯”è¨ˆç®—æœƒåœ¨ groupby_sum ä¹‹å¾ŒåŸ·è¡Œï¼Œç¢ºä¿å…ˆå®Œæˆæ•¸æ“šèšåˆå†è¨ˆç®—ä½”æ¯”ã€‚

        **7. èšåˆé‚è¼¯ (Aggregation Logic)**:
        - **`groupby_cols`**: ç”¨æ–¼åŽ»é‡çš„ç¶­åº¦æ¬„ä½ (âš ï¸ **å¿…é ˆä½¿ç”¨åŽŸå§‹è‹±æ–‡æ¬„ä½å**)ã€‚
          - ä¾‹å¦‚ï¼Œå¦‚æžœä½¿ç”¨è€…æŸ¥è©¢ã€Œå„æ ¼å¼çš„ç”¢æ¥­åˆ†ä½ˆã€ï¼Œä½ æ‡‰è©²è¨­å®š `groupby_cols=["format_name"]`ã€‚
        - **`sum_cols`**: ç”¨æ–¼åŠ ç¸½çš„æŒ‡æ¨™æ¬„ä½ (âš ï¸ **å¿…é ˆä½¿ç”¨åŽŸå§‹è‹±æ–‡æ¬„ä½å**)ã€‚
        - **`concat_col`**: **(æ–°åŠŸèƒ½)** ç”¨æ–¼å­—ä¸²èšåˆçš„æ¬„ä½ã€‚
          - ç•¶ä½ å°‡ `groupby_cols` è¨­ç‚º `["format_name"]` æ™‚ï¼Œä½ æ‡‰è©²åŒæ™‚è¨­å®š `concat_col="dimension_name"`ï¼Œé€™æ¨£ç³»çµ±å°±æœƒè‡ªå‹•å°‡æ‰€æœ‰ç”¢æ¥­åç¨±åˆä½µæˆä¸€å€‹æ¬„ä½ã€‚

        **8. è¼¸å‡ºè¦æ±‚**:
        - **rename_map**: åŽŸå§‹æ¬„ä½ -> æ¨™æº–ä¸­æ–‡åç¨±ï¼ˆç”¨æ–¼æœ€çµ‚é¡¯ç¤ºï¼‰ã€‚
        - **display_columns**: æœ€çµ‚è¦é¡¯ç¤ºçš„æ¬„ä½åˆ—è¡¨ï¼ˆä½¿ç”¨ä¸­æ–‡åç¨±ï¼‰ã€‚
          - **è¦å‰‡**: é¡¯ç¤ºä¸»éµ + ä½¿ç”¨è€…æ˜Žç¢ºè¦æ±‚çš„æ¬„ä½ + æ¦‚å¿µå±•é–‹çš„æ¬„ä½ã€‚
          - **ç¦æ­¢**: åš´ç¦å‡ºç¾ã€Œå®¢æˆ¶åç¨±ã€ã€Œä»£ç†å•†ã€ã€Œæ´»å‹•ç·¨è™Ÿã€ç­‰å…§éƒ¨æ¬„ä½ï¼Œé™¤éžä½¿ç”¨è€…æ˜Žç¢ºè©¢å•ã€‚
        - **groupby_cols**: æ ¹æ“šä¸Šè¿°èšåˆé‚è¼¯è¨­å®š (è‹±æ–‡å)ã€‚
        - **concat_col**: æ ¹æ“šä¸Šè¿°èšåˆé‚è¼¯è¨­å®š (è‹±æ–‡å)ã€‚
        - **sum_cols**: ç”¨æ–¼åŠ ç¸½çš„æŒ‡æ¨™æ¬„ä½ (è‹±æ–‡å)ã€‚
        - **sort_col**: æŽ’åºæ¬„ä½ï¼ˆåŽŸå§‹è‹±æ–‡æ¬„ä½å + " DESC" æˆ– " ASC"ï¼‰ã€‚å¦‚æžœä¸éœ€è¦æŽ’åºå‰‡ç•™ç©º ""ã€‚
        - **limit**: é™åˆ¶é¡¯ç¤ºç­†æ•¸ï¼ˆæ•´æ•¸ï¼‰ã€‚å¦‚æžœä½¿ç”¨è€…è¦ã€Œå‰Xã€å‰‡è¨­ç‚º Xï¼Œå¦å‰‡è¨­ç‚º 0ã€‚
        - **time_aggregation**: æ™‚é–“èšåˆé…ç½®ã€‚
        - **percentage_config**: ä½”æ¯”è¨ˆç®—é…ç½®ã€‚

        âš ï¸ **é‡è¦**: `groupby_cols`, `sum_cols`, `concat_col`, `sort_col` ç­‰éƒ½ä½¿ç”¨åŽŸå§‹è‹±æ–‡æ¬„ä½å, `display_columns` ä½¿ç”¨ä¸­æ–‡åã€‚

        è«‹ç›´æŽ¥å›žå‚³ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½• Markdown æ¨™è¨˜æˆ–æ–‡å­—èªªæ˜Žã€‚
        ç¯„ä¾‹æ ¼å¼: {{"rename_map": {{}}, "display_columns": [], "sort_col": "", "groupby_cols": [], "sum_cols": [], "concat_col": "", "limit": 0, ...}}
        """
        
        import re
        try:
            # Use a simpler LLM call for JSON planning
            plan_response = llm.invoke([
                SystemMessage(content="You are a JSON generator. Output only valid raw JSON without any markdown formatting."),
                HumanMessage(content=SCHEMA_PROMPT)
            ])
            content = plan_response.content
            if isinstance(content, list): content = " ".join([c.get("text", "") for c in content])
            
            # Extract JSON and clean up
            content = content.replace("```json", "").replace("```", "").strip()
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                plan_json = json_match.group(0)
                plan = json.loads(plan_json)
                
                # --- [NEW] Schema Plan Optimizer (Rule-Based Overrides) ---
                # Rule 1: Auto-Aggregate Industry by Format
                # If the planner tries to group by BOTH format and industry, it creates granular rows.
                # We want to group by Format and CONCAT Industry for a cleaner report.
                raw_groupby = plan.get("groupby_cols", [])
                
                # Check for Format + Industry combination
                has_format = "format_name" in raw_groupby
                has_dimension = "dimension_name" in raw_groupby
                
                if has_format and has_dimension:
                    print(f"DEBUG [Reporter] Plan Optimizer: Detected granular Format+Industry grouping. Switching to Aggregation.")
                    
                    # Remove dimension_name from groupby
                    plan["groupby_cols"] = [col for col in raw_groupby if col != "dimension_name"]
                    
                    # Set concat_col to dimension_name
                    plan["concat_col"] = "dimension_name"
                    
                    # Ensure dimension_name is NOT in sum_cols (just in case)
                    if "sum_cols" in plan:
                        plan["sum_cols"] = [col for col in plan["sum_cols"] if col != "dimension_name"]

                print(f"DEBUG [Reporter] Schema Plan Success: {plan}")

                # [NEW] Auto-convert Chinese column names to English for groupby/sum operations
                # Build reverse mapping: Chinese -> English
                reverse_map = {v: k for k, v in plan.get("rename_map", {}).items()}

                # Convert groupby_cols
                groupby_cols_en = []
                for col in plan.get("groupby_cols", []):
                    # If it's Chinese, convert to English; otherwise keep as is
                    groupby_cols_en.append(reverse_map.get(col, col))
                
                # Convert concat_col
                concat_col_en = plan.get("concat_col", "")
                if concat_col_en in reverse_map:
                    concat_col_en = reverse_map[concat_col_en]

                # Convert sum_cols
                sum_cols_en = []
                for col in plan.get("sum_cols", []):
                    sum_cols_en.append(reverse_map.get(col, col))

                print(f"DEBUG [Reporter] Converted groupby_cols: {plan.get('groupby_cols', [])} -> {groupby_cols_en}")
                print(f"DEBUG [Reporter] Converted concat_col: {plan.get('concat_col', '')} -> {concat_col_en}")
                print(f"DEBUG [Reporter] Converted sum_cols: {plan.get('sum_cols', [])} -> {sum_cols_en}")

                # [NEW] Handle Time Aggregation
                time_agg = plan.get("time_aggregation", {"enabled": False})
                if time_agg.get("enabled"):
                    period = time_agg.get("period", "month")

                    # Auto-detect available date column
                    date_candidates = ["investment_start_date", "start_date", "campaign_start_date", "end_date"]
                    source_col = None
                    for col in date_candidates:
                        if col in available_cols:
                            source_col = col
                            break

                    if not source_col:
                        print(f"WARNING [Reporter] Time aggregation enabled but no date column found. Skipping.")
                        time_agg["enabled"] = False
                    else:
                        # Determine period column name
                        if period == "month":
                            period_col_en = "year_month"
                            period_col_cn = "å¹´/æœˆ"
                        elif period == "quarter":
                            period_col_en = "year_quarter"
                            period_col_cn = "å¹´/å­£"
                        elif period == "year":
                            period_col_en = "year"
                            period_col_cn = "å¹´ä»½"
                        else:
                            period_col_en = "year_month"
                            period_col_cn = "å¹´/æœˆ"

                        print(f"DEBUG [Reporter] Time aggregation enabled: period={period}, source={source_col}")

                        # Execute add_time_period operation
                        current_data = pandas_processor.invoke({
                            "data": current_data,
                            "operation": "add_time_period",
                            "date_col": source_col,
                            "new_col": period_col_en,
                            "period": period
                        }).get("data", [])

                        # Add time column to groupby_cols (first position)
                        groupby_cols_en.insert(0, period_col_en)

                        # Add time column to display_columns (first position)
                        display_columns = plan.get("display_columns", [])
                        if period_col_cn not in display_columns:
                            display_columns.insert(0, period_col_cn)
                            plan["display_columns"] = display_columns

                        # Add time column to rename_map
                        rename_map = plan.get("rename_map", {})
                        rename_map[period_col_en] = period_col_cn
                        plan["rename_map"] = rename_map

                        print(f"DEBUG [Reporter] Added time column: {period_col_en} -> {period_col_cn}")

                # Execute Plan
                # Determine top_n from Schema Plan's limit parameter
                limit = plan.get("limit", 0)

                # Smart top_n logic:
                # 1. If user explicitly wants top N (limit > 0), use it
                # 2. If dataset is small (<= 50 rows), show all
                # 3. Otherwise, show 100 rows
                data_size = len(current_data)
                if limit > 0:
                    top_n = limit
                elif data_size <= 50:
                    top_n = data_size  # Show all for small datasets
                else:
                    top_n = 100  # Default cap for large datasets

                print(f"DEBUG [Reporter] Display logic: data_size={data_size}, limit={limit}, top_n={top_n}")

                # [NEW] Handle Percentage Calculation - Check BEFORE groupby_sum
                percentage_config = plan.get("percentage_config", {"enabled": False})
                needs_percentage = percentage_config.get("enabled", False)

                if needs_percentage:
                    # Step 1: Aggregate without rename (keep English column names)
                    print(f"DEBUG [Reporter] Percentage mode: Aggregating first (keeping English column names)")
                    aggregated_result = pandas_processor.invoke({
                        "data": current_data,
                        "operation": "groupby_sum",
                        "groupby_col": ",".join(groupby_cols_en),
                        "sum_col": ",".join(sum_cols_en),
                        "concat_col": concat_col_en,
                        "sort_col": plan.get("sort_col"),
                        "ascending": False,
                        "top_n": top_n
                    })

                    if aggregated_result.get("status") == "success":
                        # Step 2: Calculate percentage
                        value_col = percentage_config.get("value_col")
                        percentage_col_cn = percentage_config.get("percentage_col", "ä½”æ¯” (%)")

                        print(f"DEBUG [Reporter] Adding percentage column based on '{value_col}'")
                        percentage_result = pandas_processor.invoke({
                            "data": aggregated_result.get("data", []),
                            "operation": "add_percentage_column",
                            "sum_col": value_col,
                            "new_col": "percentage"
                        })

                        if percentage_result.get("status") == "success":
                            # Step 3: Apply rename_map and select_columns
                            rename_map = plan.get("rename_map", {})
                            rename_map["percentage"] = percentage_col_cn

                            display_columns = plan.get("display_columns", [])
                            if percentage_col_cn not in display_columns:
                                display_columns.append(percentage_col_cn)

                            print(f"DEBUG [Reporter] Applying rename and select with percentage column")
                            final_result = pandas_processor.invoke({
                                "data": percentage_result.get("data", []),
                                "operation": "groupby_sum",
                                "rename_map": rename_map,
                                "groupby_col": ",".join(groupby_cols_en),
                                "sum_col": ",".join(sum_cols_en) + ",percentage",
                                "concat_col": concat_col_en,
                                "select_columns": display_columns,
                                "sort_col": plan.get("sort_col"),
                                "ascending": False,
                                "top_n": top_n
                            })
                        else:
                            print(f"WARN [Reporter] Percentage calculation failed: {percentage_result.get('markdown', 'Unknown error')}")
                            # Fallback: use aggregated result without percentage
                            final_result = aggregated_result
                    else:
                        # Aggregation failed
                        final_result = aggregated_result
                else:
                    # No percentage needed - standard flow with rename
                    final_result = pandas_processor.invoke({
                        "data": current_data,
                        "operation": "groupby_sum",
                        "rename_map": plan.get("rename_map", {}),
                        "groupby_col": ",".join(groupby_cols_en),
                        "sum_col": ",".join(sum_cols_en),
                        "concat_col": concat_col_en,
                        "select_columns": plan.get("display_columns", []),
                        "sort_col": plan.get("sort_col"),
                        "ascending": False,
                        "top_n": top_n
                    })

            else:
                raise ValueError("No valid JSON found in LLM response")

        except Exception as e:
            print(f"DEBUG [Reporter] Planning failed ({e}). Attempting best-effort fallback.")
            # Fallback: åªé¡¯ç¤ºé—œéµæ¬„ä½ï¼Œé¿é–‹ ID
            fallback_select = [c for c in available_cols if not c.lower().endswith('id')]
            final_result = pandas_processor.invoke({
                "data": current_data,
                "operation": "top_n", 
                "top_n": 100,
                "select_columns": fallback_select[:7], # é™åˆ¶æ•¸é‡
                "sort_col": available_cols[0]
            })
    else:
        final_result = {"markdown": ""}

    final_table = final_result.get("markdown", "")

    final_table = final_result.get("markdown", "")
    
    # --- LLM Summary Generation ---
    # Now we ask LLM to summarize based on the table we generated
    
    # [NEW] Extract dates for the prompt
    routing_context = state.get("routing_context", {})
    start_date = routing_context.get("start_date", "æŒ‡å®šæœŸé–“")
    end_date = routing_context.get("end_date", "æŒ‡å®šæœŸé–“")
    
    SUMMARY_PROMPT = """
    ä½ æ˜¯æ•¸æ“šå ±å‘Šå‘ˆç¾è€…ã€‚è«‹é‡å°ä½¿ç”¨è€…æŸ¥è©¢ã€Œ{query}ã€èˆ‡ç”Ÿæˆçš„æ•¸æ“šè¡¨ç”¢å‡ºå›žæ‡‰ã€‚
    
    è«‹å›žå‚³ JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
    1. "suggestions": æ ¹æ“šæ•¸æ“šçµæžœï¼Œæä¾› 3 å€‹å…·é«”ä¸”é«˜åº¦ç›¸é—œçš„å¾ŒçºŒæŸ¥è©¢å»ºè­°ï¼ˆå¸¶æœ‰ ðŸ’¡ ç¬¦è™Ÿèˆ‡æ¨™é¡Œï¼Œä¾‹å¦‚ï¼šðŸ’¡ **æ‚¨é‚„å¯ä»¥å˜—è©¦æŸ¥è©¢ï¼š** ...ï¼‰ã€‚
    
    **è¦å‰‡**:
    - **åš´ç¦åˆ†æž**: ä¸è¦åœ¨è¼¸å‡ºä¸­åŒ…å«ä»»ä½•æ•¸æ“šè§£è®€æˆ–ç¸½çµã€‚
    - **JSON æ ¼å¼**: åªå›žå‚³åŽŸå§‹ JSONï¼Œä¸è¦åŒ…å« Markdown æ¨™è¨˜ã€‚
    """
    
    # [FIX] Programmatically generate opening to ensure date accuracy
    opening_text = f"é€™æ˜¯ **{start_date}** è‡³ **{end_date}** æœŸé–“ï¼Œé—œæ–¼ã€Ž{original_query}ã€çš„æ•¸æ“šè³‡æ–™ã€‚"
    suggestions_text = ""

    if final_table:
        try:
            messages = [
                SystemMessage(content="You are a JSON generator. Output only valid raw JSON."),
                HumanMessage(content=SUMMARY_PROMPT.format(query=original_query))
            ]
            response = llm.invoke(messages)
            content = response.content
            if isinstance(content, list): content = " ".join([c.get("text", "") for c in content])
            
            import re
            content = content.replace("```json", "").replace("```", "").strip()
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                res_json = json.loads(json_match.group(0))
                # opening_text is already set programmatically
                suggestions_data = res_json.get("suggestions", "")
                
                if isinstance(suggestions_data, list):
                    suggestions_text = "\n".join(suggestions_data)
                else:
                    suggestions_text = str(suggestions_data)
            else:
                print(f"DEBUG [Reporter] JSON not found in summary response.")
        except Exception as e:
            print(f"DEBUG [Reporter] Summary JSON parsing failed: {e}")
    else:
        opening_text = "æŠ±æ­‰ï¼Œç„¡æ³•å¾žæ•¸æ“šä¸­ç”Ÿæˆå ±è¡¨ã€‚"

    # Final Assembly (Correct Order: Opening -> Table -> Suggestions)
    final_response = opening_text + "\n\n" + final_table
    if suggestions_text:
        final_response += "\n\n" + suggestions_text

    return {
        "final_response": final_response,
        "messages": [AIMessage(content=final_response)],
        "debug_logs": execution_logs
    }
